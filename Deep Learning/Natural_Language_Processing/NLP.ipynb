{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\DQ815GM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words() # Returns a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents() # Returns a list of list of strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents(fileids='ca01') # You can access a specific file with 'fileids' argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual brown corpus data is packaged as raw text files. And you can find their IDs with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.fileids()) # 500 sources, each file is a source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09', 'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', 'ca18', 'ca19', 'ca20', 'ca21', 'ca22', 'ca23', 'ca24', 'ca25', 'ca26', 'ca27', 'ca28', 'ca29', 'ca30', 'ca31', 'ca32', 'ca33', 'ca34', 'ca35', 'ca36', 'ca37', 'ca38', 'ca39', 'ca40', 'ca41', 'ca42', 'ca43', 'ca44', 'cb01', 'cb02', 'cb03', 'cb04', 'cb05', 'cb06', 'cb07', 'cb08', 'cb09', 'cb10', 'cb11', 'cb12', 'cb13', 'cb14', 'cb15', 'cb16', 'cb17', 'cb18', 'cb19', 'cb20', 'cb21', 'cb22', 'cb23', 'cb24', 'cb25', 'cb26', 'cb27', 'cc01', 'cc02', 'cc03', 'cc04', 'cc05', 'cc06', 'cc07', 'cc08', 'cc09', 'cc10', 'cc11', 'cc12', 'cc13', 'cc14', 'cc15', 'cc16', 'cc17', 'cd01', 'cd02', 'cd03', 'cd04', 'cd05', 'cd06', 'cd07', 'cd08', 'cd09', 'cd10', 'cd11', 'cd12']\n"
     ]
    }
   ],
   "source": [
    "print(brown.fileids()[:100]) # First 100 sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembly/nn-hl session/nn-hl brought/vbd-hl much/ap-hl good/nn-hl \n",
      "The/at General/jj-tl Assembly/nn-tl ,/, which/wdt adjourns/vbz today/nr ,/, has/hvz performed/vbn in/in an/at atmosphere/nn of/in crisis/nn and/cc struggle/nn from/in the/at day/nn it/pps convened/vbd ./.\n",
      "It/pps was/bedz faced/vbn immediately/rb with/in a/at showdown/nn on/in the/at schools/nns ,/, an/at issue/nn which/wdt was/bedz met/vbn squarely/rb in/in conjunction/nn with/in the/at governor/nn with/in a/at decision/nn not/* to/to risk/vb abandoning/vbg public/nn education/nn ./.\n",
      "\n",
      "\n",
      "\tThere/ex followed/vbd the/at historic/jj appropriations/nns and/cc budget/nn fight/nn ,/, in/in which/wdt the/at General/jj-tl Assembly/nn-tl decided/vbd to/to tackle/vb executive/nn powers/nns ./.\n",
      "The/at final/jj decision/nn went/vbd to/in the/at executive/nn but/cc a/at way/nn has/hvz been/ben opened/vbn for/in strengthening/vbg budgeting/vbg procedures/nns and/cc to/to provide/vb legislators/nns information/nn they/ppss need/vb ./.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(brown.raw('cb01').strip()[:1000]) # First 1000 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Sentence tokenization is the process of splitting up strings into “sentences”\n",
    "\n",
    "Word tokenization is the process of splitting up “sentences” into “words”\n",
    "\n",
    "Lets play around with some interesting texts, the singles.txt from webtext corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\DQ815GM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('webtext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['firefox.txt',\n",
       " 'grail.txt',\n",
       " 'overheard.txt',\n",
       " 'pirates.txt',\n",
       " 'singles.txt',\n",
       " 'wine.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import webtext\n",
    "webtext.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t25 SEXY MALE, seeks attrac older single lady, for discreet encounters.\n",
      "1:\t35YO Security Guard, seeking lady in uniform for fun times.\n",
      "2:\t40 yo SINGLE DAD, sincere friendly DTE seeks r/ship with fem age open S/E\n",
      "3:\t44yo tall seeks working single mum or lady below 45 fship rship. Nat Open\n",
      "4:\t6.2 35 yr old OUTGOING M seeks fem 28-35 for o/door sports - w/e away\n",
      "5:\tA professional business male, late 40s, 6 feet tall, slim build, well groomed, great personality, home owner, interests include the arts travel and all things good, Ringwood area, is seeking a genuine female of similar age or older, in same area or surrounds, for a meaningful long term rship. Looking forward to hearing from you all.\n",
      "6:\tABLE young man seeks, sexy older women. Phone for fun ready to play\n",
      "7:\tAFFECTIONATE LADY Sought by generous guy, 40s, mutual fulfillment\n",
      "8:\tARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed.\n",
      "9:\tAMIABLE 43 y.o. gentleman with European background, 170 cm, medium build, employed, never married, no children. Enjoys sports, music, cafes, beach &c. Seeks an honest, attractive lady with a European background, without children, who would like to get married and have chil dren in the future. 29-39 y.o. Prefer non-smoker and living in Adelaide.\n",
      "10:\tARE YOU A COPPER REDHEAD? I am 36 y.o. and looking for companionship/friendship. I enjoy the AFL, animals and dining out.\n"
     ]
    }
   ],
   "source": [
    "# Each line is one advertisment\n",
    "for i, line in enumerate(webtext.raw('singles.txt').split('\\n')):\n",
    "    if i > 10: #Lets take a look at the first 10 ads\n",
    "        break\n",
    "    print(str(i) + ':\\t' + line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets zoom in on candidate no.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed.\n"
     ]
    }
   ],
   "source": [
    "single_no8 = webtext.raw('singles.txt').split('\\n')[8]\n",
    "print(single_no8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization \n",
    "\n",
    "In NLTK, sent_tokenize() the default tokenizer function that you can use to split strings into \"sentences\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DQ815GM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARE YOU ALONE or lost in a r/ship too, with no hope in sight?',\n",
       " 'Maybe we could explore new beginnings together?',\n",
       " 'Im 45 Slim/Med build, GSOH, high needs and looking for someone similar.',\n",
       " 'You WONT be disappointed.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(single_no8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARE', 'YOU', 'ALONE', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?']\n",
      "['Maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?']\n",
      "['Im', '45', 'Slim/Med', 'build', ',', 'GSOH', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.']\n",
      "['You', 'WONT', 'be', 'disappointed', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokenize(single_no8):\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARE YOU ALONE or lost in a r/ship too, with no hope in sight?',\n",
       " 'Maybe we could explore new beginnings together?',\n",
       " 'Im 45 Slim/Med build, GSOH, high needs and looking for someone similar.',\n",
       " 'You WONT be disappointed.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(single_no8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are', 'you', 'alone', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?']\n",
      "['maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?']\n",
      "['im', '45', 'slim/med', 'build', ',', 'gsoh', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.']\n",
      "['you', 'wont', 'be', 'disappointed', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokenize(single_no8):\n",
    "    print([word.lower() for word in word_tokenize(sent)])\n",
    "\n",
    "    #Alternatively:  print(list(map(str.lower, word_tokenize(sent))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ARE', 'YOU', 'ALONE', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?', 'Maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?', 'Im', '45', 'Slim/Med', 'build', ',', 'GSOH', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.', 'You', 'WONT', 'be', 'disappointed', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(single_no8))  # Treats the whole line as one document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DQ815GM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords_en = stopwords.words('english')\n",
    "print(stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are', 'you', 'alone', 'or', 'lost', 'in', 'a', 'r/ship', 'too', ',', 'with', 'no', 'hope', 'in', 'sight', '?', 'maybe', 'we', 'could', 'explore', 'new', 'beginnings', 'together', '?', 'im', '45', 'slim/med', 'build', ',', 'gsoh', ',', 'high', 'needs', 'and', 'looking', 'for', 'someone', 'similar', '.', 'you', 'wont', 'be', 'disappointed', '.']\n"
     ]
    }
   ],
   "source": [
    "#Treat the multiple sentences as one document (no need to sent_tokenize)\n",
    "# Tokenize and lowercase\n",
    "single_no8_tokenized_lowered = list(map(str.lower, word_tokenize(single_no8)))\n",
    "print(single_no8_tokenized_lowered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alone', 'lost', 'r/ship', ',', 'hope', 'sight', '?', 'maybe', 'could', 'explore', 'new', 'beginnings', 'together', '?', 'im', '45', 'slim/med', 'build', ',', 'gsoh', ',', 'high', 'needs', 'looking', 'someone', 'similar', '.', 'wont', 'disappointed', '.']\n"
     ]
    }
   ],
   "source": [
    "stopwords_en = set(stopwords.words('english')) # Set checking is faster in Python than list.\n",
    "\n",
    "# List comprehension.\n",
    "print([word for word in single_no8_tokenized_lowered if word not in stopwords_en])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuatons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From string.punctuation: <class 'str'> !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "# It's a string so we have to them into a set type\n",
    "print('From string.punctuation:', type(punctuation), punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Stopwords with punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'whom', \"it's\", 'doesn', 'yourselves', 'does', 'y', 'this', 'your', 'than', 'mightn', '`', 'from', 'yourself', 'about', 'shouldn', 'our', 'no', 'themselves', '|', 'be', 'shan', 'but', 'how', 'can', 'those', \"you'll\", 'that', 'then', 'down', 'd', 'at', 'who', \"weren't\", 'o', 'too', 'for', 'where', \"didn't\", '\"', 'their', \"won't\", ']', 'isn', 'if', 'them', 'do', '~', 'what', 'by', 'after', \"mustn't\", 'has', 'being', \"isn't\", 'm', 'nor', '?', \"shan't\", 'aren', 'some', \"wouldn't\", \"should've\", 'he', 'll', 'i', 'himself', \"mightn't\", 'over', \"aren't\", ';', 'before', 'against', 'same', '_', \"that'll\", 'through', \"couldn't\", 'both', 'you', 'hers', '/', '=', 'her', 'herself', 'didn', \"wasn't\", 'while', '<', '\\\\', '{', '>', '%', ':', 'few', 'such', 'of', 'so', '-', 'to', 'these', 'am', '^', \"'\", 'doing', 'and', 'there', 'are', 'wouldn', 'above', 'hadn', 'each', 'up', 'my', 'she', ',', 'most', '+', 'him', 'not', \"hadn't\", 'its', 'into', 't', \"doesn't\", 'is', 'below', 'any', 'why', 'had', 'in', 'ours', 'during', 'ain', 'more', 'now', '#', 'they', 'have', 'other', 'been', \"haven't\", \"shouldn't\", 'which', '.', 'myself', 'under', 'until', 'when', 're', 'yours', \"you're\", \"she's\", 'very', \"don't\", 'or', 'as', 'once', 'it', 'don', 'will', 'own', 'mustn', '}', 'ma', 'off', 'out', 'further', 'his', \"needn't\", 'the', '*', 'should', 'were', 'again', '(', \"you've\", 've', 'hasn', \"hasn't\", 's', 'between', 'with', '$', 'couldn', 'was', 'weren', 'an', 'did', '[', 'here', 'won', 'all', 'haven', 'just', '!', '@', 'ourselves', 'only', 'itself', \"you'd\", ')', 'theirs', 'because', 'a', 'wasn', 'having', 'needn', '&', 'we', 'me', 'on'}\n"
     ]
    }
   ],
   "source": [
    "stopwords_en_withpunct = stopwords_en.union(set(punctuation))\n",
    "print(stopwords_en_withpunct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stopwords with punctuations from Single no. 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alone', 'lost', 'r/ship', 'hope', 'sight', 'maybe', 'could', 'explore', 'new', 'beginnings', 'together', 'im', '45', 'slim/med', 'build', 'gsoh', 'high', 'needs', 'looking', 'someone', 'similar', 'wont', 'disappointed']\n"
     ]
    }
   ],
   "source": [
    "print([word for word in single_no8_tokenized_lowered if word not in stopwords_en_withpunct])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk\n",
      "walk\n",
      "walk\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for word in ['walking', 'walks', 'walked']:\n",
    "    print(porter.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DQ815GM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DQ815GM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['he', 'be', 'walk', 'to', 'school']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' \n",
    "    \n",
    "def lemmatize_sent(text): \n",
    "    # Text input is string, returns lowercased strings.\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
    "            for word, tag in pos_tag(word_tokenize(text))]\n",
    "\n",
    "lemmatize_sent('He is walking to school')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try the lemmatize_sent() and remove stopwords from Single no. 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Single no. 8:\n",
      "ARE YOU ALONE or lost in a r/ship too, with no hope in sight? Maybe we could explore new beginnings together? Im 45 Slim/Med build, GSOH, high needs and looking for someone similar. You WONT be disappointed. \n",
      "\n",
      "Lemmatized and removed stopwords:\n",
      "['alone', 'lose', 'r/ship', 'hope', 'sight', 'maybe', 'could', 'explore', 'new', 'beginning', 'together', 'im', 'slim/med', 'build', 'gsoh', 'high', 'need', 'look', 'someone', 'similar', 'wont', 'disappoint']\n"
     ]
    }
   ],
   "source": [
    "print('Original Single no. 8:')\n",
    "print(single_no8, '\\n')\n",
    "print('Lemmatized and removed stopwords:')\n",
    "print([word for word in lemmatize_sent(single_no8) \n",
    "       if word not in stopwords_en_withpunct\n",
    "       and not word.isdigit() ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining what we know about removing stopwords and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Input: str, i.e. document/sentence\n",
    "    # Output: list(str) , i.e. list of lemmas\n",
    "    return [word for word in lemmatize_sent(text) \n",
    "            if word not in stopwords_en_withpunct\n",
    "            and not word.isdigit()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From String to Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "sent1 = \"The quick brown fox jumps over the lazy brown dog.\"\n",
    "sent2 = \"Mr brown jumps over the lazy fox.\"\n",
    "\n",
    "# Lemmatize and remove stopwords\n",
    "processed_sent1 = preprocess_text(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed sentence:\n",
      "['quick', 'brown', 'fox', 'jump', 'lazy', 'brown', 'dog']\n",
      "\n",
      "Word counts:\n",
      "Counter({'brown': 2, 'quick': 1, 'fox': 1, 'jump': 1, 'lazy': 1, 'dog': 1})\n"
     ]
    }
   ],
   "source": [
    "print('Processed sentence:')\n",
    "print(processed_sent1)\n",
    "print()\n",
    "print('Word counts:')\n",
    "print(Counter(processed_sent1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quick': 6, 'brown': 0, 'fox': 2, 'jumps': 3, 'lazy': 4, 'dog': 1, 'mr': 5}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sent1 = \"The quick brown fox jumps over the lazy brown dog.\"\n",
    "sent2 = \"Mr brown jumps over the lazy fox.\"\n",
    "\n",
    "with StringIO('\\n'.join([sent1, sent2])) as fin:\n",
    "    # Override the analyzer totally with our preprocess text\n",
    "    count_vect = CountVectorizer(stop_words=stopwords_en_withpunct,\n",
    "                                 tokenizer=word_tokenize)\n",
    "    count_vect.fit_transform(fin)\n",
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To vectorize any new sentences, we use  CountVectorizer.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x7 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.transform([sent1, sent2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the matrix, you can output it to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jump', 'lazy', 'brown', 'dog']\n",
      "['mr', 'brown', 'jump', 'lazy', 'fox']\n",
      "\n",
      "Vocab: ('brown', 'dog', 'fox', 'jumps', 'lazy', 'mr', 'quick')\n",
      "\n",
      "Matrix/Vectors:\n",
      " [[2 1 1 1 1 0 1]\n",
      " [1 0 1 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# Print the words sorted by their index\n",
    "words_sorted_by_index, _ = zip(*sorted(count_vect.vocabulary_.items(), key=itemgetter(1)))\n",
    "\n",
    "print(preprocess_text(sent1))\n",
    "print(preprocess_text(sent2))\n",
    "print()\n",
    "print('Vocab:', words_sorted_by_index)\n",
    "print()\n",
    "print('Matrix/Vectors:\\n', count_vect.transform([sent1, sent2]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:/Users/DQ815GM/Desktop/GitHub/vorsatti/Machine Learning/Natural_Language_Processing/writers/train.csv')\n",
    "test = pd.read_csv('C:/Users/DQ815GM/Desktop/GitHub/vorsatti/Machine Learning/Natural_Language_Processing/writers/test.csv')\n",
    "sample = pd.read_csv('C:/Users/DQ815GM/Desktop/GitHub/vorsatti/Machine Learning/Natural_Language_Processing/writers/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
       "1  id24541  If a fire wanted fanning, it could readily be ...\n",
       "2  id00134  And when they had broken down the frail door t...\n",
       "3  id27757  While I was thinking how I should possibly man...\n",
       "4  id04081  I am not sure to what limit his knowledge may ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL       MWS\n",
       "0  id02310  0.403494  0.287808  0.308698\n",
       "1  id24541  0.403494  0.287808  0.308698\n",
       "2  id00134  0.403494  0.287808  0.308698\n",
       "3  id27757  0.403494  0.287808  0.308698\n",
       "4  id04081  0.403494  0.287808  0.308698"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem requires us to predict the author, i.e. EAP, HPL and MWS given the text. In simpler words, text classification with 3 different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular problem, Kaggle has specified multi-class log-loss as evaluation metric. This is implemented in the follow way (taken from: https://github.com/dnouri/nolearn/blob/master/nolearn/lasagne/util.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(train.author.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17621,)\n",
      "(1958,)\n"
     ]
    }
   ],
   "source": [
    "print (xtrain.shape)\n",
    "print (xvalid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our very first model is a simple TF-IDF (Term Frequency - Inverse Document Frequency) followed by a simple Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.626 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on TFIDF\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using TF-IDF, we can also use word counts as features. This can be done easily using CountVectorizer from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain) + list(xvalid))\n",
    "xtrain_ctv =  ctv.transform(xtrain) \n",
    "xvalid_ctv = ctv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.528 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on Counts\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We just improved our first model by 0.1!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.578 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.485 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on Counts\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "Since SVMs take a lot of time, we will reduce the number of features from the TF-IDF using Singular Value Decomposition before applying SVM.\n",
    "\n",
    "Also, note that before applying SVMs, we must standardize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\n",
    "svd = decomposition.TruncatedSVD(n_components=120)\n",
    "svd.fit(xtrain_tfv)\n",
    "xtrain_svd = svd.transform(xtrain_tfv)\n",
    "xvalid_svd = svd.transform(xvalid_tfv)\n",
    "\n",
    "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xvalid_svd_scl = scl.transform(xvalid_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.732 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple SVM\n",
    "clf = SVC(C=1.0, probability=True) # since we need probabilities\n",
    "clf.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_svd_scl)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like SVM doesn't perform well on this data...!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1917494it [06:17, 5080.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.11969    0.1602    -0.24917    0.12878   -0.19874    0.44684\n",
      "  0.43726   -0.1943     0.013091   0.043389  -0.14589   -0.35991\n",
      " -0.034873  -0.44344   -0.13404   -0.096952   0.20621   -0.1439\n",
      " -0.0033411  0.23557   -0.01134   -0.080737  -0.056891  -0.12781\n",
      "  0.43218    0.0090635  0.035958   0.070213   0.14113    0.11309\n",
      "  0.14787    0.22466   -0.082153   0.097319  -0.21206   -0.25644\n",
      "  0.04528    0.24203   -0.24572    0.16853   -0.2641     0.16425\n",
      "  0.18171   -0.050493  -0.20969   -0.043564  -0.043627  -0.038104\n",
      " -0.0090871 -0.39152    0.083001  -0.12174    0.22382   -0.17588\n",
      " -0.11067   -0.013021  -0.3337    -0.10227   -0.03196    0.28049\n",
      " -0.073983   0.22665    0.082655   0.16785   -0.24602   -0.040421\n",
      " -0.023705  -0.012314   0.16754    0.17192   -0.2625    -0.18317\n",
      " -0.26723    0.0673     0.064148   0.37196    0.049488  -0.18392\n",
      " -0.018638  -0.14565   -0.36355    0.47322    0.067461   0.1984\n",
      "  0.045388  -0.016077   0.20864    0.18339   -0.18341    0.0066028\n",
      "  0.15325    0.24125    0.12986   -0.0051293 -0.018682   0.22995\n",
      "  0.50634   -0.14559   -0.15253    0.11466   -0.10831   -0.059963\n",
      "  0.086245   0.026059   0.051511  -0.11837    0.13188   -0.036852\n",
      "  0.14781   -0.24811    0.079022   0.15       0.11668   -0.1711\n",
      " -0.055809  -0.34097   -0.13966   -0.014578   0.043289  -0.201\n",
      "  0.069384  -0.045758   0.22566   -0.40566   -0.030383  -0.13459\n",
      "  0.031772  -0.024153  -0.2906    -0.051809  -0.21847   -0.25159\n",
      " -0.094286   0.13731   -0.23848    0.0025889 -0.2455     0.23321\n",
      "  0.038385  -0.45567    0.06847    0.14476    0.10382    0.090762\n",
      "  0.2696    -0.098781  -0.34006   -0.046922   0.072477   0.073566\n",
      "  0.31334    0.11514    0.16833    0.1108     0.0022866  0.074586\n",
      "  0.032092  -0.14347   -0.25466    0.22317   -0.10978   -0.24201\n",
      "  0.33852   -0.052326   0.01098    0.0023796  0.094459   0.37816\n",
      "  0.020722   0.073459  -0.060166  -0.065434   0.21354    0.26727\n",
      "  0.22324    0.032404   0.0019641 -0.03462   -0.042585   0.071747\n",
      "  0.12494   -0.1202    -0.13114   -0.087415  -0.077388   0.011516\n",
      "  0.0013175 -0.07055    0.24607   -0.0056581 -0.14063    0.040226\n",
      " -0.26162    0.19947   -0.099834  -0.16748   -0.27873   -0.34644\n",
      " -0.14153   -0.014876   0.035212   0.0089596  0.023693   0.40365\n",
      " -0.42829   -0.16679    0.13675   -0.36421   -0.02693    0.061095\n",
      " -0.096783   0.3595     0.34226    0.15502    0.23777    0.10758\n",
      " -0.13897    0.071619   0.0327    -0.031255   0.17608   -0.019069\n",
      "  0.012348   0.26759    0.57357   -0.18854   -0.14058    0.02928\n",
      "  0.0044915 -0.14315   -0.10357   -0.043943  -0.4278     0.12715\n",
      "  0.23067    0.26667   -0.19591    0.31715    0.2132     0.09395\n",
      "  0.17197    0.32419    0.164     -0.28277   -0.067733   0.076818\n",
      " -0.21156    0.0048457  0.099345  -0.076769   0.067689  -0.16736\n",
      "  0.41827    0.07784    0.37858   -0.01736    0.20519    0.37793\n",
      " -0.018276  -0.18601    0.13168    0.024476  -0.2664    -0.29664\n",
      "  0.11946    0.34602   -0.058778  -0.068049  -0.067339  -0.071196\n",
      " -0.19211   -0.11563    0.15913    0.40352    0.09735   -0.22407\n",
      " -0.33777    0.0165    -0.059003   0.13559   -0.28736    0.084492\n",
      "  0.23031   -0.24484   -0.20284   -0.020147  -0.067043   0.044702\n",
      " -0.0058131 -0.17282   -0.30697   -0.27771    0.34654    0.23245\n",
      "  0.012919   0.12475   -0.081428  -0.090892   0.27827    0.11374  ]\n",
      "Found 1917494 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.42B.300d.txt', encoding=\"utf8\")\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 17621/17621 [00:10<00:00, 1634.27it/s]\n",
      "100%|████████████████████████████████████| 1958/1958 [00:01<00:00, 1514.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# create sentence vectors using the above function for training and validation set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the performance of xgboost on glove features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:53:06] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 254 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:07] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 242 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:08] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 238 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:09] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 246 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:09] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 250 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:10] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 238 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:10] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 244 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:11] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 252 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:12] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 248 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:13] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 240 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:13] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 252 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:14] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 244 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:15] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 246 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:15] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 242 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:16] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 242 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:17] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 236 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:18] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 240 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:19] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 242 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:20] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 234 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:20] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 240 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:21] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 252 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:22] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:23] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 238 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:26] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 240 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 232 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 246 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 236 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 244 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 246 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 242 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 250 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 238 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:42] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 252 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 240 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 248 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 246 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 244 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 224 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:47] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 242 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:48] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 246 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:48] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 242 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:49] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 240 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:50] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 236 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:51] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 226 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 242 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 236 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:53] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:54] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 220 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:55] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 238 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:55] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:56] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:57] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 238 extra nodes, 0 pruned nodes, max_depth=7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:53:58] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 246 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:59] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:53:59] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 242 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 242 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:01] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 230 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 238 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 240 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 224 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:04] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 240 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:05] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 230 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:08] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 226 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:09] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 242 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:10] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 216 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:11] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 232 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:12] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 244 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:13] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 226 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:14] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 226 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:14] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 240 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:15] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 242 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:16] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:16] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 248 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:17] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 232 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:18] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:18] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 238 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:19] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 204 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:20] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 252 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:20] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:21] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 234 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:22] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:22] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 230 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:23] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 238 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:27] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 240 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:28] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 242 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 236 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 234 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 246 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 224 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 224 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 238 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 236 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 212 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 232 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 232 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 226 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 232 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 230 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:42] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 230 extra nodes, 0 pruned nodes, max_depth=7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:54:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 238 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 240 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:47] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 232 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:48] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:48] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 226 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:49] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 208 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:50] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 202 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 182 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 244 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:53] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 232 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:54] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 244 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:54] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:55] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 210 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:56] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 192 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:57] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 246 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:57] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 224 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:54:58] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:01] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 238 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:01] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 212 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 214 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 232 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 230 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:04] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:05] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 242 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:06] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 180 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:07] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 200 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:07] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 212 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:08] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 196 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:09] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 216 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:10] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 250 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:10] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 198 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:12] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:12] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:13] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 210 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:14] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 226 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:15] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 240 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:16] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:17] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 244 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:17] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 210 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:18] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 214 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:19] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 204 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:20] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 202 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:20] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 216 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:21] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 216 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:22] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 220 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:23] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:24] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 230 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:24] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 234 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 210 extra nodes, 0 pruned nodes, max_depth=7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:55:26] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 250 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:27] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 212 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:27] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:28] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 190 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 192 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 236 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 198 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 208 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 178 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 202 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 216 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 224 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 236 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 230 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 224 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 248 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 224 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 212 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:42] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 194 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 166 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 238 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:47] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 248 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:47] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 208 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:48] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 220 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:49] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 220 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:49] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 142 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:50] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 210 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:51] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 242 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:51] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 214 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 242 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:53] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 220 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:54] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 202 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:55] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:55] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 216 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:56] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 240 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:57] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 230 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:57] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 216 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:58] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 232 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:59] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 214 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:55:59] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 204 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 212 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:01] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 242 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:01] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 230 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 140 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 238 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:04] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 240 extra nodes, 0 pruned nodes, max_depth=7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:56:04] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 240 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:05] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 186 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:06] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 204 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:06] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:07] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 188 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:08] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 212 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:08] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 210 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:09] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 184 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:10] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:10] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 232 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:11] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 252 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:12] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 214 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:12] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 248 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:13] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 214 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:14] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:14] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:15] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:16] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 220 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:17] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 230 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:18] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 234 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:21] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 220 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:22] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:22] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 198 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:23] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 204 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:24] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 204 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:24] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 226 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 202 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:26] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 200 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:26] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 180 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:27] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 214 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:28] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 238 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:28] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 170 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 214 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 190 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 230 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 214 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 216 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 232 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 238 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 220 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 212 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 226 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 208 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 126 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 210 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 224 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:42] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 214 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 198 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 178 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 234 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 230 extra nodes, 0 pruned nodes, max_depth=7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:56:48] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 206 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:49] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 202 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:50] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 104 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:51] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 238 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:51] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 200 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:53] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 234 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:53] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 234 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:54] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 156 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:55] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 166 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:55] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 224 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:56] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 212 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:57] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 202 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:57] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 220 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:56:58] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 126 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 220 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:01] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 202 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 226 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 214 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:04] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:04] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 152 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:05] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 220 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:06] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 232 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:06] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 108 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:07] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 206 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:08] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 248 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:09] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 200 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:10] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 208 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:10] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 200 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:13] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 174 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:14] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 210 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:17] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 212 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:18] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 180 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:21] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:22] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 224 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:23] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 194 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:24] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 136 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 204 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:26] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 172 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:27] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 178 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 240 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 224 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 208 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 172 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 204 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 204 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 234 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 154 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 180 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 224 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 158 extra nodes, 0 pruned nodes, max_depth=7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:57:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 196 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 164 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:42] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 198 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 194 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 202 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 198 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 210 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 216 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:47] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 188 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:48] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 196 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:49] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 234 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:50] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 226 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:51] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 226 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 166 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:53] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 216 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:54] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 196 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:54] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 154 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:55] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 226 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:56] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:57] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 196 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:57] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 232 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:58] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 248 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:57:59] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 206 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 150 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:01] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 142 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 214 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 134 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:04] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 220 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:04] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 232 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:06] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:06] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 182 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:07] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 142 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:08] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 174 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:09] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 240 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:11] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 172 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:12] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 200 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:13] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 226 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:13] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 192 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:17] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 200 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:19] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 238 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:20] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:21] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 180 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:22] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 244 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:22] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 176 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 240 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:27] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 220 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:28] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 242 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 188 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 236 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 234 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 206 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 178 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 174 extra nodes, 0 pruned nodes, max_depth=7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:58:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 232 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 188 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 204 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 128 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 202 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 156 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 144 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:49] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 204 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:50] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 206 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:51] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 166 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 216 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:53] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 230 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:54] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 154 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:56] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 202 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:57] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 230 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:58] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 198 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:58:59] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 146 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 206 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:01] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 188 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 240 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 214 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:04] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 210 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:05] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 226 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:06] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:06] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 170 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:07] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 132 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:08] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 212 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:09] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 150 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:10] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 226 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:11] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 204 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:11] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 202 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:12] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 188 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:14] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 212 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:14] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 190 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:15] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 104 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:16] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 226 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:16] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 196 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:17] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 172 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:18] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:18] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 174 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:19] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 206 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:20] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 226 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:20] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 232 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:21] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 174 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:22] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 148 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:22] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 226 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:23] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 204 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:24] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 204 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 208 extra nodes, 0 pruned nodes, max_depth=7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:59:26] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 182 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:27] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 202 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:28] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 182 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 162 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 162 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 220 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 242 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 174 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 242 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 202 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 204 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 202 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 180 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 162 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 188 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 194 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 146 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 158 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 184 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:42] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 178 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 170 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 214 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:45] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 202 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 170 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 214 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:47] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 216 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:48] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 200 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:48] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 234 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:49] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 194 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:50] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 162 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:50] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 210 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:51] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 168 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 178 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:53] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 162 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:53] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 178 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:54] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 156 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:55] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 226 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:56] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 238 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:56] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 210 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:57] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 186 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[08:59:59] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 168 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 102 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:01] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 234 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 136 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 206 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 210 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:04] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:04] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 186 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:05] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 202 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:06] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 212 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:07] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:00:07] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 182 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:08] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 214 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:09] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 164 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:09] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:10] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 156 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:11] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 214 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:11] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:12] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 126 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:13] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 216 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:14] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 170 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:15] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 184 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:16] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 196 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:17] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 212 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:19] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 196 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:19] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 188 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:20] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 212 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:21] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 240 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:22] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 158 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:23] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 192 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:24] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 196 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:24] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 170 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:26] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 220 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:27] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 158 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:27] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 158 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:28] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 162 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 202 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:29] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 180 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 136 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:31] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 158 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 206 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:33] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 192 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 184 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 212 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 160 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 208 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 202 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 208 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 176 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 112 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:47] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:50] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 222 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 212 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:53] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 156 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:54] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 140 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:55] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:56] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 128 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:56] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 138 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:57] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 180 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:00:58] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 184 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 238 extra nodes, 0 pruned nodes, max_depth=7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:01:01] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 184 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 188 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 160 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:04] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 196 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:05] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 220 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:06] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 210 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:07] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 182 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:08] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 192 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:09] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 104 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:10] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 228 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:10] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 196 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:11] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 210 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:12] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 172 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:13] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 186 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:13] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 190 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:14] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 200 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:15] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 192 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:15] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 170 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:16] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 190 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:17] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 204 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:17] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 184 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:18] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 172 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:19] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 230 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:20] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 202 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:20] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 154 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:21] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 206 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:22] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 148 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:23] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 176 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:25] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 250 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:26] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 194 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:26] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 176 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:28] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 168 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:30] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 136 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 172 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:32] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 218 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:34] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 214 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:35] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 206 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:36] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 200 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 230 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:37] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 158 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:38] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 168 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 182 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:39] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 160 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:40] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 174 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 132 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:41] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 140 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:42] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 232 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:43] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 210 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:44] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 150 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:46] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 212 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:47] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 176 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:48] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 178 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:50] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 166 extra nodes, 0 pruned nodes, max_depth=7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:01:51] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 208 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:52] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 224 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:53] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 214 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:54] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 204 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:56] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 214 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:56] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 134 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:57] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 194 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:01:58] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 204 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:02:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 166 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:02:00] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 134 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:02:01] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 180 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:02:02] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 198 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:02:03] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 186 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:02:04] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 134 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:02:05] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 148 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:02:05] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 122 extra nodes, 0 pruned nodes, max_depth=7\n",
      "[09:02:07] C:\\Users\\Administrator\\Desktop\\xgboost\\src\\tree\\updater_prune.cc:74: tree pruning end, 1 roots, 186 extra nodes, 0 pruned nodes, max_depth=7\n",
      "logloss: 0.713 \n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_glove)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data before any neural net:\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "17621/17621 [==============================] - ETA: 4:50 - loss: 1.716 - ETA: 49s - loss: 1.400 - ETA: 34s - loss: 1.38 - ETA: 29s - loss: 1.33 - ETA: 23s - loss: 1.31 - ETA: 19s - loss: 1.25 - ETA: 15s - loss: 1.26 - ETA: 14s - loss: 1.24 - ETA: 13s - loss: 1.23 - ETA: 12s - loss: 1.21 - ETA: 10s - loss: 1.18 - ETA: 10s - loss: 1.15 - ETA: 9s - loss: 1.1360 - ETA: 8s - loss: 1.132 - ETA: 7s - loss: 1.118 - ETA: 7s - loss: 1.107 - ETA: 6s - loss: 1.099 - ETA: 6s - loss: 1.086 - ETA: 5s - loss: 1.080 - ETA: 5s - loss: 1.073 - ETA: 5s - loss: 1.071 - ETA: 5s - loss: 1.066 - ETA: 5s - loss: 1.059 - ETA: 4s - loss: 1.061 - ETA: 4s - loss: 1.061 - ETA: 4s - loss: 1.056 - ETA: 4s - loss: 1.053 - ETA: 4s - loss: 1.051 - ETA: 3s - loss: 1.047 - ETA: 3s - loss: 1.040 - ETA: 3s - loss: 1.034 - ETA: 3s - loss: 1.031 - ETA: 3s - loss: 1.026 - ETA: 3s - loss: 1.025 - ETA: 3s - loss: 1.023 - ETA: 3s - loss: 1.020 - ETA: 3s - loss: 1.019 - ETA: 2s - loss: 1.019 - ETA: 2s - loss: 1.016 - ETA: 2s - loss: 1.012 - ETA: 2s - loss: 1.008 - ETA: 2s - loss: 1.003 - ETA: 2s - loss: 1.001 - ETA: 2s - loss: 0.996 - ETA: 2s - loss: 0.994 - ETA: 2s - loss: 0.991 - ETA: 2s - loss: 0.989 - ETA: 2s - loss: 0.985 - ETA: 1s - loss: 0.984 - ETA: 1s - loss: 0.983 - ETA: 1s - loss: 0.979 - ETA: 1s - loss: 0.977 - ETA: 1s - loss: 0.974 - ETA: 1s - loss: 0.971 - ETA: 1s - loss: 0.968 - ETA: 1s - loss: 0.966 - ETA: 1s - loss: 0.966 - ETA: 1s - loss: 0.966 - ETA: 1s - loss: 0.964 - ETA: 1s - loss: 0.961 - ETA: 0s - loss: 0.958 - ETA: 0s - loss: 0.954 - ETA: 0s - loss: 0.952 - ETA: 0s - loss: 0.951 - ETA: 0s - loss: 0.950 - ETA: 0s - loss: 0.949 - ETA: 0s - loss: 0.948 - ETA: 0s - loss: 0.946 - ETA: 0s - loss: 0.944 - ETA: 0s - loss: 0.943 - ETA: 0s - loss: 0.941 - ETA: 0s - loss: 0.939 - ETA: 0s - loss: 0.936 - ETA: 0s - loss: 0.934 - ETA: 0s - loss: 0.932 - 5s 306us/step - loss: 0.9337 - val_loss: 0.7483\n",
      "Epoch 2/5\n",
      "17621/17621 [==============================] - ETA: 2s - loss: 0.646 - ETA: 3s - loss: 0.666 - ETA: 3s - loss: 0.666 - ETA: 3s - loss: 0.678 - ETA: 3s - loss: 0.683 - ETA: 3s - loss: 0.685 - ETA: 3s - loss: 0.689 - ETA: 3s - loss: 0.716 - ETA: 3s - loss: 0.721 - ETA: 3s - loss: 0.718 - ETA: 3s - loss: 0.724 - ETA: 2s - loss: 0.727 - ETA: 3s - loss: 0.719 - ETA: 2s - loss: 0.721 - ETA: 2s - loss: 0.722 - ETA: 2s - loss: 0.722 - ETA: 2s - loss: 0.726 - ETA: 2s - loss: 0.722 - ETA: 2s - loss: 0.719 - ETA: 2s - loss: 0.718 - ETA: 2s - loss: 0.718 - ETA: 2s - loss: 0.716 - ETA: 2s - loss: 0.715 - ETA: 2s - loss: 0.717 - ETA: 2s - loss: 0.720 - ETA: 2s - loss: 0.723 - ETA: 2s - loss: 0.721 - ETA: 2s - loss: 0.723 - ETA: 2s - loss: 0.719 - ETA: 2s - loss: 0.717 - ETA: 2s - loss: 0.716 - ETA: 2s - loss: 0.715 - ETA: 2s - loss: 0.712 - ETA: 2s - loss: 0.714 - ETA: 1s - loss: 0.715 - ETA: 1s - loss: 0.716 - ETA: 1s - loss: 0.717 - ETA: 1s - loss: 0.716 - ETA: 1s - loss: 0.716 - ETA: 1s - loss: 0.715 - ETA: 1s - loss: 0.716 - ETA: 1s - loss: 0.715 - ETA: 1s - loss: 0.714 - ETA: 1s - loss: 0.714 - ETA: 1s - loss: 0.714 - ETA: 1s - loss: 0.715 - ETA: 1s - loss: 0.716 - ETA: 1s - loss: 0.716 - ETA: 1s - loss: 0.715 - ETA: 1s - loss: 0.716 - ETA: 1s - loss: 0.715 - ETA: 1s - loss: 0.715 - ETA: 1s - loss: 0.715 - ETA: 0s - loss: 0.714 - ETA: 0s - loss: 0.714 - ETA: 0s - loss: 0.715 - ETA: 0s - loss: 0.715 - ETA: 0s - loss: 0.714 - ETA: 0s - loss: 0.716 - ETA: 0s - loss: 0.716 - ETA: 0s - loss: 0.714 - ETA: 0s - loss: 0.716 - ETA: 0s - loss: 0.716 - ETA: 0s - loss: 0.715 - ETA: 0s - loss: 0.715 - ETA: 0s - loss: 0.716 - ETA: 0s - loss: 0.717 - ETA: 0s - loss: 0.717 - ETA: 0s - loss: 0.716 - ETA: 0s - loss: 0.715 - ETA: 0s - loss: 0.716 - ETA: 0s - loss: 0.716 - 4s 231us/step - loss: 0.7155 - val_loss: 0.7087\n",
      "Epoch 3/5\n",
      "17621/17621 [==============================] - ETA: 5s - loss: 0.641 - ETA: 6s - loss: 0.604 - ETA: 4s - loss: 0.635 - ETA: 4s - loss: 0.632 - ETA: 4s - loss: 0.642 - ETA: 3s - loss: 0.629 - ETA: 3s - loss: 0.631 - ETA: 3s - loss: 0.631 - ETA: 3s - loss: 0.627 - ETA: 3s - loss: 0.630 - ETA: 3s - loss: 0.634 - ETA: 3s - loss: 0.633 - ETA: 3s - loss: 0.631 - ETA: 3s - loss: 0.626 - ETA: 3s - loss: 0.628 - ETA: 3s - loss: 0.627 - ETA: 3s - loss: 0.627 - ETA: 3s - loss: 0.625 - ETA: 3s - loss: 0.629 - ETA: 3s - loss: 0.629 - ETA: 3s - loss: 0.633 - ETA: 3s - loss: 0.631 - ETA: 2s - loss: 0.631 - ETA: 2s - loss: 0.630 - ETA: 2s - loss: 0.632 - ETA: 2s - loss: 0.633 - ETA: 2s - loss: 0.631 - ETA: 2s - loss: 0.633 - ETA: 2s - loss: 0.633 - ETA: 2s - loss: 0.634 - ETA: 2s - loss: 0.637 - ETA: 2s - loss: 0.637 - ETA: 2s - loss: 0.637 - ETA: 2s - loss: 0.637 - ETA: 2s - loss: 0.636 - ETA: 2s - loss: 0.636 - ETA: 2s - loss: 0.636 - ETA: 2s - loss: 0.636 - ETA: 2s - loss: 0.638 - ETA: 2s - loss: 0.638 - ETA: 2s - loss: 0.638 - ETA: 2s - loss: 0.640 - ETA: 2s - loss: 0.640 - ETA: 2s - loss: 0.640 - ETA: 2s - loss: 0.640 - ETA: 2s - loss: 0.641 - ETA: 2s - loss: 0.641 - ETA: 2s - loss: 0.641 - ETA: 2s - loss: 0.641 - ETA: 2s - loss: 0.641 - ETA: 2s - loss: 0.641 - ETA: 2s - loss: 0.641 - ETA: 2s - loss: 0.641 - ETA: 2s - loss: 0.640 - ETA: 2s - loss: 0.641 - ETA: 2s - loss: 0.641 - ETA: 2s - loss: 0.640 - ETA: 2s - loss: 0.639 - ETA: 2s - loss: 0.641 - ETA: 2s - loss: 0.641 - ETA: 2s - loss: 0.641 - ETA: 2s - loss: 0.641 - ETA: 2s - loss: 0.641 - ETA: 2s - loss: 0.641 - ETA: 1s - loss: 0.642 - ETA: 1s - loss: 0.643 - ETA: 1s - loss: 0.644 - ETA: 1s - loss: 0.644 - ETA: 1s - loss: 0.644 - ETA: 1s - loss: 0.644 - ETA: 1s - loss: 0.644 - ETA: 1s - loss: 0.644 - ETA: 1s - loss: 0.643 - ETA: 1s - loss: 0.644 - ETA: 1s - loss: 0.644 - ETA: 1s - loss: 0.644 - ETA: 1s - loss: 0.644 - ETA: 1s - loss: 0.644 - ETA: 1s - loss: 0.644 - ETA: 1s - loss: 0.645 - ETA: 1s - loss: 0.645 - ETA: 1s - loss: 0.645 - ETA: 1s - loss: 0.645 - ETA: 1s - loss: 0.646 - ETA: 1s - loss: 0.646 - ETA: 1s - loss: 0.646 - ETA: 1s - loss: 0.646 - ETA: 1s - loss: 0.646 - ETA: 1s - loss: 0.646 - ETA: 0s - loss: 0.648 - ETA: 0s - loss: 0.649 - ETA: 0s - loss: 0.648 - ETA: 0s - loss: 0.648 - ETA: 0s - loss: 0.649 - ETA: 0s - loss: 0.649 - ETA: 0s - loss: 0.648 - ETA: 0s - loss: 0.649 - ETA: 0s - loss: 0.649 - ETA: 0s - loss: 0.650 - ETA: 0s - loss: 0.650 - ETA: 0s - loss: 0.650 - ETA: 0s - loss: 0.650 - ETA: 0s - loss: 0.651 - ETA: 0s - loss: 0.651 - ETA: 0s - loss: 0.651 - ETA: 0s - loss: 0.651 - ETA: 0s - loss: 0.651 - 7s 369us/step - loss: 0.6516 - val_loss: 0.6934\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17621/17621 [==============================] - ETA: 8s - loss: 0.582 - ETA: 5s - loss: 0.565 - ETA: 4s - loss: 0.607 - ETA: 5s - loss: 0.597 - ETA: 4s - loss: 0.594 - ETA: 5s - loss: 0.584 - ETA: 5s - loss: 0.586 - ETA: 5s - loss: 0.576 - ETA: 5s - loss: 0.576 - ETA: 5s - loss: 0.580 - ETA: 5s - loss: 0.577 - ETA: 5s - loss: 0.572 - ETA: 5s - loss: 0.568 - ETA: 5s - loss: 0.572 - ETA: 5s - loss: 0.577 - ETA: 5s - loss: 0.577 - ETA: 5s - loss: 0.577 - ETA: 5s - loss: 0.577 - ETA: 5s - loss: 0.578 - ETA: 4s - loss: 0.579 - ETA: 4s - loss: 0.580 - ETA: 4s - loss: 0.581 - ETA: 4s - loss: 0.579 - ETA: 4s - loss: 0.580 - ETA: 4s - loss: 0.578 - ETA: 4s - loss: 0.574 - ETA: 4s - loss: 0.575 - ETA: 4s - loss: 0.574 - ETA: 4s - loss: 0.575 - ETA: 4s - loss: 0.578 - ETA: 4s - loss: 0.578 - ETA: 4s - loss: 0.575 - ETA: 4s - loss: 0.581 - ETA: 4s - loss: 0.582 - ETA: 4s - loss: 0.584 - ETA: 4s - loss: 0.585 - ETA: 3s - loss: 0.585 - ETA: 3s - loss: 0.584 - ETA: 3s - loss: 0.587 - ETA: 3s - loss: 0.587 - ETA: 3s - loss: 0.589 - ETA: 3s - loss: 0.591 - ETA: 3s - loss: 0.589 - ETA: 3s - loss: 0.590 - ETA: 3s - loss: 0.594 - ETA: 3s - loss: 0.595 - ETA: 3s - loss: 0.595 - ETA: 3s - loss: 0.594 - ETA: 3s - loss: 0.594 - ETA: 2s - loss: 0.595 - ETA: 2s - loss: 0.596 - ETA: 2s - loss: 0.594 - ETA: 2s - loss: 0.594 - ETA: 2s - loss: 0.597 - ETA: 2s - loss: 0.596 - ETA: 2s - loss: 0.596 - ETA: 2s - loss: 0.597 - ETA: 2s - loss: 0.599 - ETA: 2s - loss: 0.599 - ETA: 2s - loss: 0.598 - ETA: 2s - loss: 0.599 - ETA: 2s - loss: 0.598 - ETA: 2s - loss: 0.600 - ETA: 2s - loss: 0.600 - ETA: 1s - loss: 0.600 - ETA: 1s - loss: 0.599 - ETA: 1s - loss: 0.600 - ETA: 1s - loss: 0.600 - ETA: 1s - loss: 0.600 - ETA: 1s - loss: 0.602 - ETA: 1s - loss: 0.602 - ETA: 1s - loss: 0.602 - ETA: 1s - loss: 0.604 - ETA: 1s - loss: 0.604 - ETA: 1s - loss: 0.604 - ETA: 1s - loss: 0.605 - ETA: 1s - loss: 0.603 - ETA: 1s - loss: 0.603 - ETA: 1s - loss: 0.604 - ETA: 1s - loss: 0.604 - ETA: 1s - loss: 0.604 - ETA: 1s - loss: 0.604 - ETA: 1s - loss: 0.603 - ETA: 1s - loss: 0.604 - ETA: 0s - loss: 0.604 - ETA: 0s - loss: 0.603 - ETA: 0s - loss: 0.604 - ETA: 0s - loss: 0.604 - ETA: 0s - loss: 0.604 - ETA: 0s - loss: 0.606 - ETA: 0s - loss: 0.606 - ETA: 0s - loss: 0.606 - ETA: 0s - loss: 0.605 - ETA: 0s - loss: 0.606 - ETA: 0s - loss: 0.605 - ETA: 0s - loss: 0.605 - ETA: 0s - loss: 0.605 - ETA: 0s - loss: 0.605 - ETA: 0s - loss: 0.605 - ETA: 0s - loss: 0.605 - ETA: 0s - loss: 0.605 - ETA: 0s - loss: 0.605 - ETA: 0s - loss: 0.605 - ETA: 0s - loss: 0.606 - 6s 343us/step - loss: 0.6064 - val_loss: 0.6925\n",
      "Epoch 5/5\n",
      "17621/17621 [==============================] - ETA: 4s - loss: 0.509 - ETA: 5s - loss: 0.481 - ETA: 4s - loss: 0.535 - ETA: 4s - loss: 0.540 - ETA: 5s - loss: 0.526 - ETA: 5s - loss: 0.531 - ETA: 4s - loss: 0.546 - ETA: 4s - loss: 0.545 - ETA: 4s - loss: 0.550 - ETA: 4s - loss: 0.550 - ETA: 4s - loss: 0.546 - ETA: 4s - loss: 0.543 - ETA: 4s - loss: 0.541 - ETA: 4s - loss: 0.543 - ETA: 4s - loss: 0.546 - ETA: 4s - loss: 0.546 - ETA: 4s - loss: 0.549 - ETA: 4s - loss: 0.547 - ETA: 4s - loss: 0.545 - ETA: 4s - loss: 0.550 - ETA: 4s - loss: 0.547 - ETA: 4s - loss: 0.550 - ETA: 4s - loss: 0.551 - ETA: 3s - loss: 0.551 - ETA: 3s - loss: 0.552 - ETA: 3s - loss: 0.552 - ETA: 3s - loss: 0.554 - ETA: 3s - loss: 0.556 - ETA: 3s - loss: 0.558 - ETA: 3s - loss: 0.559 - ETA: 3s - loss: 0.559 - ETA: 3s - loss: 0.559 - ETA: 3s - loss: 0.559 - ETA: 3s - loss: 0.559 - ETA: 3s - loss: 0.559 - ETA: 3s - loss: 0.560 - ETA: 3s - loss: 0.557 - ETA: 3s - loss: 0.555 - ETA: 3s - loss: 0.555 - ETA: 3s - loss: 0.555 - ETA: 3s - loss: 0.555 - ETA: 3s - loss: 0.556 - ETA: 3s - loss: 0.556 - ETA: 3s - loss: 0.556 - ETA: 2s - loss: 0.557 - ETA: 2s - loss: 0.557 - ETA: 2s - loss: 0.558 - ETA: 2s - loss: 0.558 - ETA: 2s - loss: 0.556 - ETA: 2s - loss: 0.557 - ETA: 2s - loss: 0.556 - ETA: 2s - loss: 0.555 - ETA: 2s - loss: 0.555 - ETA: 2s - loss: 0.555 - ETA: 2s - loss: 0.555 - ETA: 2s - loss: 0.557 - ETA: 2s - loss: 0.557 - ETA: 2s - loss: 0.557 - ETA: 2s - loss: 0.558 - ETA: 2s - loss: 0.559 - ETA: 2s - loss: 0.558 - ETA: 2s - loss: 0.558 - ETA: 2s - loss: 0.556 - ETA: 2s - loss: 0.559 - ETA: 2s - loss: 0.559 - ETA: 1s - loss: 0.559 - ETA: 1s - loss: 0.560 - ETA: 1s - loss: 0.562 - ETA: 1s - loss: 0.560 - ETA: 1s - loss: 0.561 - ETA: 1s - loss: 0.562 - ETA: 1s - loss: 0.563 - ETA: 1s - loss: 0.562 - ETA: 1s - loss: 0.562 - ETA: 1s - loss: 0.562 - ETA: 1s - loss: 0.561 - ETA: 1s - loss: 0.560 - ETA: 1s - loss: 0.560 - ETA: 1s - loss: 0.560 - ETA: 1s - loss: 0.560 - ETA: 1s - loss: 0.560 - ETA: 1s - loss: 0.561 - ETA: 1s - loss: 0.561 - ETA: 1s - loss: 0.561 - ETA: 1s - loss: 0.561 - ETA: 0s - loss: 0.561 - ETA: 0s - loss: 0.562 - ETA: 0s - loss: 0.563 - ETA: 0s - loss: 0.563 - ETA: 0s - loss: 0.564 - ETA: 0s - loss: 0.564 - ETA: 0s - loss: 0.564 - ETA: 0s - loss: 0.564 - ETA: 0s - loss: 0.565 - ETA: 0s - loss: 0.565 - ETA: 0s - loss: 0.566 - ETA: 0s - loss: 0.566 - ETA: 0s - loss: 0.567 - ETA: 0s - loss: 0.567 - ETA: 0s - loss: 0.567 - ETA: 0s - loss: 0.567 - ETA: 0s - loss: 0.566 - 6s 336us/step - loss: 0.5661 - val_loss: 0.6939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xd6f8bda0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n",
    "          epochs=5, verbose=1, \n",
    "          validation_data=(xvalid_glove_scl, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To move further, i.e. with LSTMs we need to tokenize the text data\n",
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 25943/25943 [00:00<00:00, 94336.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/75\n",
      "17621/17621 [==============================] - ETA: 3:00 - loss: 1.161 - ETA: 2:05 - loss: 1.142 - ETA: 1:44 - loss: 1.153 - ETA: 1:33 - loss: 1.152 - ETA: 1:25 - loss: 1.146 - ETA: 1:19 - loss: 1.140 - ETA: 1:14 - loss: 1.137 - ETA: 1:09 - loss: 1.133 - ETA: 1:04 - loss: 1.129 - ETA: 1:00 - loss: 1.125 - ETA: 57s - loss: 1.122 - ETA: 55s - loss: 1.11 - ETA: 52s - loss: 1.11 - ETA: 48s - loss: 1.11 - ETA: 45s - loss: 1.11 - ETA: 42s - loss: 1.11 - ETA: 40s - loss: 1.11 - ETA: 37s - loss: 1.11 - ETA: 35s - loss: 1.10 - ETA: 32s - loss: 1.10 - ETA: 30s - loss: 1.10 - ETA: 27s - loss: 1.10 - ETA: 25s - loss: 1.10 - ETA: 22s - loss: 1.10 - ETA: 20s - loss: 1.10 - ETA: 18s - loss: 1.09 - ETA: 16s - loss: 1.09 - ETA: 13s - loss: 1.09 - ETA: 11s - loss: 1.09 - ETA: 9s - loss: 1.0945 - ETA: 7s - loss: 1.093 - ETA: 5s - loss: 1.091 - ETA: 3s - loss: 1.090 - ETA: 0s - loss: 1.089 - 78s 4ms/step - loss: 1.0888 - val_loss: 1.0172\n",
      "Epoch 2/75\n",
      "17621/17621 [==============================] - ETA: 1:09 - loss: 1.045 - ETA: 1:09 - loss: 1.036 - ETA: 1:07 - loss: 1.033 - ETA: 1:04 - loss: 1.030 - ETA: 1:02 - loss: 1.028 - ETA: 1:00 - loss: 1.031 - ETA: 59s - loss: 1.026 - ETA: 58s - loss: 1.02 - ETA: 56s - loss: 1.02 - ETA: 54s - loss: 1.02 - ETA: 51s - loss: 1.02 - ETA: 49s - loss: 1.02 - ETA: 47s - loss: 1.02 - ETA: 45s - loss: 1.02 - ETA: 42s - loss: 1.02 - ETA: 40s - loss: 1.01 - ETA: 38s - loss: 1.01 - ETA: 36s - loss: 1.01 - ETA: 34s - loss: 1.01 - ETA: 32s - loss: 1.01 - ETA: 30s - loss: 1.01 - ETA: 27s - loss: 1.01 - ETA: 25s - loss: 1.01 - ETA: 23s - loss: 1.01 - ETA: 20s - loss: 1.01 - ETA: 18s - loss: 1.01 - ETA: 16s - loss: 1.00 - ETA: 14s - loss: 1.00 - ETA: 12s - loss: 1.00 - ETA: 9s - loss: 1.0034 - ETA: 7s - loss: 1.000 - ETA: 5s - loss: 0.998 - ETA: 3s - loss: 0.996 - ETA: 0s - loss: 0.996 - 80s 5ms/step - loss: 0.9959 - val_loss: 0.8754\n",
      "Epoch 3/75\n",
      "17621/17621 [==============================] - ETA: 1:09 - loss: 0.930 - ETA: 1:04 - loss: 0.936 - ETA: 1:04 - loss: 0.951 - ETA: 1:05 - loss: 0.952 - ETA: 1:10 - loss: 0.947 - ETA: 1:12 - loss: 0.940 - ETA: 1:10 - loss: 0.937 - ETA: 1:10 - loss: 0.934 - ETA: 1:09 - loss: 0.931 - ETA: 1:04 - loss: 0.928 - ETA: 1:00 - loss: 0.925 - ETA: 56s - loss: 0.925 - ETA: 53s - loss: 0.92 - ETA: 50s - loss: 0.92 - ETA: 47s - loss: 0.92 - ETA: 44s - loss: 0.92 - ETA: 41s - loss: 0.92 - ETA: 38s - loss: 0.92 - ETA: 36s - loss: 0.92 - ETA: 34s - loss: 0.91 - ETA: 32s - loss: 0.91 - ETA: 30s - loss: 0.91 - ETA: 28s - loss: 0.92 - ETA: 26s - loss: 0.91 - ETA: 23s - loss: 0.91 - ETA: 21s - loss: 0.91 - ETA: 18s - loss: 0.91 - ETA: 16s - loss: 0.91 - ETA: 13s - loss: 0.91 - ETA: 11s - loss: 0.91 - ETA: 8s - loss: 0.9131 - ETA: 6s - loss: 0.912 - ETA: 3s - loss: 0.911 - ETA: 1s - loss: 0.911 - 97s 5ms/step - loss: 0.9106 - val_loss: 0.7993\n",
      "Epoch 4/75\n",
      "17621/17621 [==============================] - ETA: 1:21 - loss: 0.872 - ETA: 1:21 - loss: 0.851 - ETA: 1:17 - loss: 0.842 - ETA: 1:12 - loss: 0.842 - ETA: 1:11 - loss: 0.857 - ETA: 1:09 - loss: 0.854 - ETA: 1:06 - loss: 0.857 - ETA: 1:05 - loss: 0.862 - ETA: 1:03 - loss: 0.862 - ETA: 1:01 - loss: 0.869 - ETA: 59s - loss: 0.870 - ETA: 57s - loss: 0.87 - ETA: 55s - loss: 0.87 - ETA: 53s - loss: 0.87 - ETA: 51s - loss: 0.87 - ETA: 49s - loss: 0.87 - ETA: 46s - loss: 0.87 - ETA: 43s - loss: 0.87 - ETA: 41s - loss: 0.87 - ETA: 38s - loss: 0.87 - ETA: 36s - loss: 0.87 - ETA: 33s - loss: 0.87 - ETA: 31s - loss: 0.87 - ETA: 28s - loss: 0.87 - ETA: 26s - loss: 0.87 - ETA: 23s - loss: 0.87 - ETA: 20s - loss: 0.87 - ETA: 17s - loss: 0.87 - ETA: 15s - loss: 0.87 - ETA: 12s - loss: 0.87 - ETA: 9s - loss: 0.8727 - ETA: 6s - loss: 0.873 - ETA: 4s - loss: 0.871 - ETA: 1s - loss: 0.871 - 104s 6ms/step - loss: 0.8709 - val_loss: 0.7603\n",
      "Epoch 5/75\n",
      "17621/17621 [==============================] - ETA: 2:15 - loss: 0.862 - ETA: 1:51 - loss: 0.854 - ETA: 1:47 - loss: 0.859 - ETA: 1:57 - loss: 0.859 - ETA: 1:56 - loss: 0.858 - ETA: 1:54 - loss: 0.852 - ETA: 1:53 - loss: 0.850 - ETA: 1:52 - loss: 0.844 - ETA: 1:48 - loss: 0.845 - ETA: 1:45 - loss: 0.849 - ETA: 1:41 - loss: 0.850 - ETA: 1:36 - loss: 0.847 - ETA: 1:30 - loss: 0.844 - ETA: 1:24 - loss: 0.841 - ETA: 1:17 - loss: 0.840 - ETA: 1:12 - loss: 0.840 - ETA: 1:07 - loss: 0.840 - ETA: 1:03 - loss: 0.837 - ETA: 58s - loss: 0.838 - ETA: 53s - loss: 0.83 - ETA: 49s - loss: 0.83 - ETA: 45s - loss: 0.83 - ETA: 41s - loss: 0.83 - ETA: 37s - loss: 0.83 - ETA: 33s - loss: 0.82 - ETA: 29s - loss: 0.82 - ETA: 25s - loss: 0.82 - ETA: 22s - loss: 0.82 - ETA: 18s - loss: 0.82 - ETA: 15s - loss: 0.82 - ETA: 11s - loss: 0.82 - ETA: 8s - loss: 0.8299 - ETA: 4s - loss: 0.829 - ETA: 1s - loss: 0.828 - 118s 7ms/step - loss: 0.8278 - val_loss: 0.7413\n",
      "Epoch 6/75\n",
      "17621/17621 [==============================] - ETA: 1:21 - loss: 0.820 - ETA: 1:25 - loss: 0.801 - ETA: 1:21 - loss: 0.813 - ETA: 1:19 - loss: 0.808 - ETA: 1:15 - loss: 0.815 - ETA: 1:12 - loss: 0.812 - ETA: 1:09 - loss: 0.812 - ETA: 1:06 - loss: 0.811 - ETA: 1:04 - loss: 0.811 - ETA: 1:02 - loss: 0.816 - ETA: 59s - loss: 0.816 - ETA: 57s - loss: 0.81 - ETA: 54s - loss: 0.81 - ETA: 52s - loss: 0.81 - ETA: 49s - loss: 0.81 - ETA: 47s - loss: 0.81 - ETA: 46s - loss: 0.81 - ETA: 43s - loss: 0.81 - ETA: 40s - loss: 0.81 - ETA: 38s - loss: 0.81 - ETA: 35s - loss: 0.81 - ETA: 33s - loss: 0.81 - ETA: 30s - loss: 0.81 - ETA: 28s - loss: 0.81 - ETA: 25s - loss: 0.81 - ETA: 22s - loss: 0.81 - ETA: 20s - loss: 0.81 - ETA: 17s - loss: 0.81 - ETA: 14s - loss: 0.81 - ETA: 11s - loss: 0.81 - ETA: 9s - loss: 0.8117 - ETA: 6s - loss: 0.813 - ETA: 3s - loss: 0.813 - ETA: 1s - loss: 0.813 - 95s 5ms/step - loss: 0.8130 - val_loss: 0.7172\n",
      "Epoch 7/75\n",
      "17621/17621 [==============================] - ETA: 1:24 - loss: 0.829 - ETA: 1:23 - loss: 0.803 - ETA: 1:19 - loss: 0.807 - ETA: 1:16 - loss: 0.810 - ETA: 1:13 - loss: 0.795 - ETA: 1:11 - loss: 0.796 - ETA: 1:08 - loss: 0.795 - ETA: 1:08 - loss: 0.798 - ETA: 1:09 - loss: 0.795 - ETA: 1:11 - loss: 0.797 - ETA: 1:12 - loss: 0.795 - ETA: 1:11 - loss: 0.797 - ETA: 1:10 - loss: 0.798 - ETA: 1:08 - loss: 0.796 - ETA: 1:03 - loss: 0.798 - ETA: 1:00 - loss: 0.796 - ETA: 58s - loss: 0.796 - ETA: 56s - loss: 0.79 - ETA: 53s - loss: 0.79 - ETA: 49s - loss: 0.79 - ETA: 46s - loss: 0.79 - ETA: 43s - loss: 0.79 - ETA: 39s - loss: 0.79 - ETA: 36s - loss: 0.79 - ETA: 32s - loss: 0.78 - ETA: 29s - loss: 0.78 - ETA: 25s - loss: 0.78 - ETA: 22s - loss: 0.78 - ETA: 18s - loss: 0.78 - ETA: 15s - loss: 0.78 - ETA: 12s - loss: 0.78 - ETA: 8s - loss: 0.7894 - ETA: 4s - loss: 0.788 - ETA: 1s - loss: 0.787 - 124s 7ms/step - loss: 0.7871 - val_loss: 0.6979\n",
      "Epoch 8/75\n",
      "17621/17621 [==============================] - ETA: 1:57 - loss: 0.780 - ETA: 1:48 - loss: 0.778 - ETA: 1:39 - loss: 0.777 - ETA: 1:37 - loss: 0.756 - ETA: 1:34 - loss: 0.753 - ETA: 1:31 - loss: 0.755 - ETA: 1:28 - loss: 0.756 - ETA: 1:25 - loss: 0.749 - ETA: 1:21 - loss: 0.751 - ETA: 1:16 - loss: 0.752 - ETA: 1:12 - loss: 0.759 - ETA: 1:08 - loss: 0.758 - ETA: 1:04 - loss: 0.757 - ETA: 1:00 - loss: 0.764 - ETA: 56s - loss: 0.764 - ETA: 53s - loss: 0.76 - ETA: 49s - loss: 0.76 - ETA: 46s - loss: 0.76 - ETA: 43s - loss: 0.76 - ETA: 40s - loss: 0.76 - ETA: 37s - loss: 0.76 - ETA: 34s - loss: 0.76 - ETA: 31s - loss: 0.76 - ETA: 29s - loss: 0.76 - ETA: 26s - loss: 0.76 - ETA: 23s - loss: 0.76 - ETA: 20s - loss: 0.76 - ETA: 17s - loss: 0.76 - ETA: 14s - loss: 0.76 - ETA: 12s - loss: 0.76 - ETA: 9s - loss: 0.7616 - ETA: 6s - loss: 0.763 - ETA: 3s - loss: 0.762 - ETA: 1s - loss: 0.763 - 98s 6ms/step - loss: 0.7641 - val_loss: 0.6785\n",
      "Epoch 9/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17621/17621 [==============================] - ETA: 1:37 - loss: 0.791 - ETA: 1:35 - loss: 0.802 - ETA: 1:31 - loss: 0.790 - ETA: 1:29 - loss: 0.784 - ETA: 1:25 - loss: 0.782 - ETA: 1:22 - loss: 0.785 - ETA: 1:18 - loss: 0.784 - ETA: 1:14 - loss: 0.781 - ETA: 1:11 - loss: 0.784 - ETA: 1:08 - loss: 0.777 - ETA: 1:05 - loss: 0.773 - ETA: 1:02 - loss: 0.769 - ETA: 1:01 - loss: 0.767 - ETA: 1:00 - loss: 0.766 - ETA: 59s - loss: 0.765 - ETA: 57s - loss: 0.76 - ETA: 55s - loss: 0.76 - ETA: 52s - loss: 0.76 - ETA: 50s - loss: 0.76 - ETA: 48s - loss: 0.76 - ETA: 45s - loss: 0.76 - ETA: 42s - loss: 0.76 - ETA: 38s - loss: 0.76 - ETA: 35s - loss: 0.75 - ETA: 31s - loss: 0.75 - ETA: 28s - loss: 0.75 - ETA: 24s - loss: 0.75 - ETA: 21s - loss: 0.75 - ETA: 17s - loss: 0.75 - ETA: 14s - loss: 0.75 - ETA: 11s - loss: 0.75 - ETA: 7s - loss: 0.7545 - ETA: 4s - loss: 0.754 - ETA: 1s - loss: 0.751 - 111s 6ms/step - loss: 0.7515 - val_loss: 0.6683\n",
      "Epoch 10/75\n",
      "17621/17621 [==============================] - ETA: 1:03 - loss: 0.713 - ETA: 59s - loss: 0.723 - ETA: 56s - loss: 0.72 - ETA: 53s - loss: 0.71 - ETA: 51s - loss: 0.73 - ETA: 49s - loss: 0.73 - ETA: 48s - loss: 0.73 - ETA: 46s - loss: 0.73 - ETA: 44s - loss: 0.73 - ETA: 42s - loss: 0.73 - ETA: 41s - loss: 0.73 - ETA: 39s - loss: 0.73 - ETA: 37s - loss: 0.73 - ETA: 35s - loss: 0.74 - ETA: 34s - loss: 0.74 - ETA: 32s - loss: 0.74 - ETA: 30s - loss: 0.74 - ETA: 29s - loss: 0.74 - ETA: 27s - loss: 0.74 - ETA: 25s - loss: 0.74 - ETA: 23s - loss: 0.74 - ETA: 21s - loss: 0.74 - ETA: 19s - loss: 0.74 - ETA: 18s - loss: 0.74 - ETA: 16s - loss: 0.74 - ETA: 14s - loss: 0.74 - ETA: 12s - loss: 0.74 - ETA: 11s - loss: 0.74 - ETA: 9s - loss: 0.7416 - ETA: 7s - loss: 0.739 - ETA: 6s - loss: 0.737 - ETA: 4s - loss: 0.736 - ETA: 2s - loss: 0.737 - ETA: 0s - loss: 0.737 - 65s 4ms/step - loss: 0.7377 - val_loss: 0.6491\n",
      "Epoch 11/75\n",
      "17621/17621 [==============================] - ETA: 1:04 - loss: 0.714 - ETA: 58s - loss: 0.705 - ETA: 55s - loss: 0.70 - ETA: 53s - loss: 0.72 - ETA: 52s - loss: 0.71 - ETA: 50s - loss: 0.72 - ETA: 49s - loss: 0.72 - ETA: 47s - loss: 0.72 - ETA: 45s - loss: 0.72 - ETA: 43s - loss: 0.72 - ETA: 42s - loss: 0.72 - ETA: 40s - loss: 0.72 - ETA: 38s - loss: 0.71 - ETA: 37s - loss: 0.72 - ETA: 35s - loss: 0.72 - ETA: 33s - loss: 0.72 - ETA: 31s - loss: 0.72 - ETA: 30s - loss: 0.72 - ETA: 28s - loss: 0.72 - ETA: 26s - loss: 0.72 - ETA: 24s - loss: 0.72 - ETA: 22s - loss: 0.72 - ETA: 20s - loss: 0.72 - ETA: 19s - loss: 0.72 - ETA: 17s - loss: 0.72 - ETA: 15s - loss: 0.72 - ETA: 13s - loss: 0.72 - ETA: 11s - loss: 0.72 - ETA: 10s - loss: 0.72 - ETA: 8s - loss: 0.7225 - ETA: 6s - loss: 0.720 - ETA: 4s - loss: 0.720 - ETA: 2s - loss: 0.720 - ETA: 0s - loss: 0.719 - 67s 4ms/step - loss: 0.7194 - val_loss: 0.6509\n",
      "Epoch 12/75\n",
      "17621/17621 [==============================] - ETA: 1:20 - loss: 0.740 - ETA: 1:10 - loss: 0.746 - ETA: 1:04 - loss: 0.750 - ETA: 1:00 - loss: 0.721 - ETA: 56s - loss: 0.710 - ETA: 54s - loss: 0.70 - ETA: 51s - loss: 0.70 - ETA: 49s - loss: 0.70 - ETA: 47s - loss: 0.70 - ETA: 45s - loss: 0.69 - ETA: 44s - loss: 0.69 - ETA: 42s - loss: 0.68 - ETA: 40s - loss: 0.69 - ETA: 38s - loss: 0.68 - ETA: 36s - loss: 0.69 - ETA: 34s - loss: 0.69 - ETA: 32s - loss: 0.69 - ETA: 30s - loss: 0.69 - ETA: 28s - loss: 0.70 - ETA: 27s - loss: 0.70 - ETA: 25s - loss: 0.70 - ETA: 23s - loss: 0.70 - ETA: 21s - loss: 0.70 - ETA: 19s - loss: 0.70 - ETA: 17s - loss: 0.70 - ETA: 15s - loss: 0.70 - ETA: 14s - loss: 0.70 - ETA: 12s - loss: 0.70 - ETA: 10s - loss: 0.70 - ETA: 8s - loss: 0.7042 - ETA: 6s - loss: 0.703 - ETA: 4s - loss: 0.705 - ETA: 2s - loss: 0.705 - ETA: 0s - loss: 0.703 - 68s 4ms/step - loss: 0.7037 - val_loss: 0.6364\n",
      "Epoch 13/75\n",
      "17621/17621 [==============================] - ETA: 59s - loss: 0.70 - ETA: 56s - loss: 0.69 - ETA: 55s - loss: 0.67 - ETA: 53s - loss: 0.68 - ETA: 52s - loss: 0.68 - ETA: 51s - loss: 0.68 - ETA: 50s - loss: 0.69 - ETA: 48s - loss: 0.68 - ETA: 47s - loss: 0.69 - ETA: 45s - loss: 0.70 - ETA: 43s - loss: 0.69 - ETA: 42s - loss: 0.69 - ETA: 40s - loss: 0.69 - ETA: 38s - loss: 0.69 - ETA: 36s - loss: 0.69 - ETA: 34s - loss: 0.69 - ETA: 32s - loss: 0.69 - ETA: 30s - loss: 0.69 - ETA: 28s - loss: 0.69 - ETA: 27s - loss: 0.68 - ETA: 25s - loss: 0.68 - ETA: 23s - loss: 0.68 - ETA: 21s - loss: 0.68 - ETA: 19s - loss: 0.68 - ETA: 17s - loss: 0.68 - ETA: 15s - loss: 0.68 - ETA: 13s - loss: 0.68 - ETA: 11s - loss: 0.68 - ETA: 10s - loss: 0.68 - ETA: 8s - loss: 0.6876 - ETA: 6s - loss: 0.687 - ETA: 4s - loss: 0.686 - ETA: 2s - loss: 0.687 - ETA: 0s - loss: 0.686 - 66s 4ms/step - loss: 0.6872 - val_loss: 0.6234\n",
      "Epoch 14/75\n",
      "17621/17621 [==============================] - ETA: 58s - loss: 0.67 - ETA: 54s - loss: 0.64 - ETA: 52s - loss: 0.64 - ETA: 50s - loss: 0.67 - ETA: 49s - loss: 0.68 - ETA: 47s - loss: 0.67 - ETA: 46s - loss: 0.67 - ETA: 46s - loss: 0.67 - ETA: 44s - loss: 0.68 - ETA: 42s - loss: 0.68 - ETA: 40s - loss: 0.68 - ETA: 39s - loss: 0.68 - ETA: 37s - loss: 0.68 - ETA: 36s - loss: 0.68 - ETA: 34s - loss: 0.67 - ETA: 32s - loss: 0.67 - ETA: 30s - loss: 0.67 - ETA: 29s - loss: 0.67 - ETA: 27s - loss: 0.67 - ETA: 25s - loss: 0.67 - ETA: 23s - loss: 0.67 - ETA: 22s - loss: 0.67 - ETA: 20s - loss: 0.67 - ETA: 18s - loss: 0.67 - ETA: 16s - loss: 0.67 - ETA: 14s - loss: 0.67 - ETA: 13s - loss: 0.67 - ETA: 11s - loss: 0.67 - ETA: 9s - loss: 0.6749 - ETA: 7s - loss: 0.673 - ETA: 5s - loss: 0.672 - ETA: 4s - loss: 0.673 - ETA: 2s - loss: 0.671 - ETA: 0s - loss: 0.672 - 62s 4ms/step - loss: 0.6720 - val_loss: 0.6181\n",
      "Epoch 15/75\n",
      "17621/17621 [==============================] - ETA: 56s - loss: 0.64 - ETA: 54s - loss: 0.66 - ETA: 52s - loss: 0.64 - ETA: 51s - loss: 0.64 - ETA: 50s - loss: 0.65 - ETA: 48s - loss: 0.65 - ETA: 47s - loss: 0.66 - ETA: 47s - loss: 0.66 - ETA: 44s - loss: 0.65 - ETA: 42s - loss: 0.65 - ETA: 40s - loss: 0.64 - ETA: 39s - loss: 0.64 - ETA: 37s - loss: 0.64 - ETA: 35s - loss: 0.65 - ETA: 33s - loss: 0.65 - ETA: 31s - loss: 0.65 - ETA: 30s - loss: 0.65 - ETA: 28s - loss: 0.65 - ETA: 26s - loss: 0.65 - ETA: 24s - loss: 0.65 - ETA: 23s - loss: 0.65 - ETA: 21s - loss: 0.65 - ETA: 19s - loss: 0.65 - ETA: 17s - loss: 0.65 - ETA: 16s - loss: 0.65 - ETA: 14s - loss: 0.65 - ETA: 12s - loss: 0.65 - ETA: 10s - loss: 0.65 - ETA: 9s - loss: 0.6562 - ETA: 7s - loss: 0.656 - ETA: 5s - loss: 0.656 - ETA: 4s - loss: 0.657 - ETA: 2s - loss: 0.657 - ETA: 0s - loss: 0.657 - 61s 3ms/step - loss: 0.6567 - val_loss: 0.5996\n",
      "Epoch 16/75\n",
      "17621/17621 [==============================] - ETA: 55s - loss: 0.62 - ETA: 53s - loss: 0.63 - ETA: 51s - loss: 0.64 - ETA: 49s - loss: 0.64 - ETA: 48s - loss: 0.63 - ETA: 46s - loss: 0.64 - ETA: 45s - loss: 0.63 - ETA: 43s - loss: 0.64 - ETA: 43s - loss: 0.64 - ETA: 41s - loss: 0.63 - ETA: 39s - loss: 0.63 - ETA: 38s - loss: 0.63 - ETA: 36s - loss: 0.63 - ETA: 34s - loss: 0.63 - ETA: 32s - loss: 0.63 - ETA: 31s - loss: 0.63 - ETA: 29s - loss: 0.63 - ETA: 28s - loss: 0.63 - ETA: 26s - loss: 0.63 - ETA: 24s - loss: 0.63 - ETA: 22s - loss: 0.63 - ETA: 21s - loss: 0.63 - ETA: 19s - loss: 0.63 - ETA: 17s - loss: 0.63 - ETA: 16s - loss: 0.64 - ETA: 14s - loss: 0.64 - ETA: 12s - loss: 0.64 - ETA: 11s - loss: 0.64 - ETA: 9s - loss: 0.6439 - ETA: 7s - loss: 0.645 - ETA: 5s - loss: 0.645 - ETA: 4s - loss: 0.645 - ETA: 2s - loss: 0.645 - ETA: 0s - loss: 0.644 - 61s 3ms/step - loss: 0.6451 - val_loss: 0.6098\n",
      "Epoch 17/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17621/17621 [==============================] - ETA: 55s - loss: 0.60 - ETA: 53s - loss: 0.64 - ETA: 52s - loss: 0.63 - ETA: 50s - loss: 0.63 - ETA: 48s - loss: 0.62 - ETA: 46s - loss: 0.63 - ETA: 45s - loss: 0.62 - ETA: 43s - loss: 0.63 - ETA: 42s - loss: 0.62 - ETA: 41s - loss: 0.63 - ETA: 39s - loss: 0.63 - ETA: 38s - loss: 0.63 - ETA: 36s - loss: 0.63 - ETA: 34s - loss: 0.62 - ETA: 32s - loss: 0.62 - ETA: 31s - loss: 0.62 - ETA: 29s - loss: 0.62 - ETA: 27s - loss: 0.62 - ETA: 26s - loss: 0.62 - ETA: 24s - loss: 0.62 - ETA: 22s - loss: 0.63 - ETA: 20s - loss: 0.63 - ETA: 19s - loss: 0.63 - ETA: 17s - loss: 0.63 - ETA: 15s - loss: 0.63 - ETA: 14s - loss: 0.63 - ETA: 12s - loss: 0.62 - ETA: 10s - loss: 0.63 - ETA: 9s - loss: 0.6313 - ETA: 7s - loss: 0.631 - ETA: 5s - loss: 0.632 - ETA: 4s - loss: 0.632 - ETA: 2s - loss: 0.633 - ETA: 0s - loss: 0.632 - 61s 3ms/step - loss: 0.6333 - val_loss: 0.5858\n",
      "Epoch 18/75\n",
      "17621/17621 [==============================] - ETA: 56s - loss: 0.58 - ETA: 54s - loss: 0.58 - ETA: 52s - loss: 0.58 - ETA: 50s - loss: 0.60 - ETA: 48s - loss: 0.60 - ETA: 46s - loss: 0.60 - ETA: 45s - loss: 0.61 - ETA: 43s - loss: 0.61 - ETA: 41s - loss: 0.61 - ETA: 40s - loss: 0.61 - ETA: 39s - loss: 0.61 - ETA: 38s - loss: 0.61 - ETA: 36s - loss: 0.61 - ETA: 34s - loss: 0.61 - ETA: 32s - loss: 0.61 - ETA: 31s - loss: 0.61 - ETA: 29s - loss: 0.61 - ETA: 27s - loss: 0.61 - ETA: 26s - loss: 0.61 - ETA: 24s - loss: 0.61 - ETA: 22s - loss: 0.61 - ETA: 21s - loss: 0.61 - ETA: 19s - loss: 0.61 - ETA: 17s - loss: 0.61 - ETA: 15s - loss: 0.61 - ETA: 14s - loss: 0.61 - ETA: 12s - loss: 0.61 - ETA: 10s - loss: 0.61 - ETA: 9s - loss: 0.6134 - ETA: 7s - loss: 0.613 - ETA: 5s - loss: 0.615 - ETA: 4s - loss: 0.616 - ETA: 2s - loss: 0.616 - ETA: 0s - loss: 0.617 - 60s 3ms/step - loss: 0.6168 - val_loss: 0.5757\n",
      "Epoch 19/75\n",
      "17621/17621 [==============================] - ETA: 56s - loss: 0.58 - ETA: 53s - loss: 0.60 - ETA: 52s - loss: 0.61 - ETA: 50s - loss: 0.60 - ETA: 49s - loss: 0.60 - ETA: 47s - loss: 0.59 - ETA: 45s - loss: 0.59 - ETA: 43s - loss: 0.59 - ETA: 42s - loss: 0.59 - ETA: 40s - loss: 0.59 - ETA: 38s - loss: 0.59 - ETA: 38s - loss: 0.59 - ETA: 36s - loss: 0.59 - ETA: 34s - loss: 0.59 - ETA: 32s - loss: 0.58 - ETA: 31s - loss: 0.58 - ETA: 29s - loss: 0.58 - ETA: 27s - loss: 0.58 - ETA: 26s - loss: 0.59 - ETA: 24s - loss: 0.59 - ETA: 22s - loss: 0.59 - ETA: 21s - loss: 0.60 - ETA: 19s - loss: 0.60 - ETA: 17s - loss: 0.60 - ETA: 16s - loss: 0.60 - ETA: 14s - loss: 0.60 - ETA: 12s - loss: 0.60 - ETA: 10s - loss: 0.60 - ETA: 9s - loss: 0.6049 - ETA: 7s - loss: 0.606 - ETA: 5s - loss: 0.606 - ETA: 4s - loss: 0.606 - ETA: 2s - loss: 0.607 - ETA: 0s - loss: 0.606 - 61s 3ms/step - loss: 0.6071 - val_loss: 0.5716\n",
      "Epoch 20/75\n",
      "17621/17621 [==============================] - ETA: 57s - loss: 0.49 - ETA: 54s - loss: 0.54 - ETA: 53s - loss: 0.57 - ETA: 51s - loss: 0.57 - ETA: 49s - loss: 0.58 - ETA: 47s - loss: 0.57 - ETA: 45s - loss: 0.59 - ETA: 44s - loss: 0.59 - ETA: 42s - loss: 0.59 - ETA: 40s - loss: 0.59 - ETA: 39s - loss: 0.59 - ETA: 37s - loss: 0.60 - ETA: 36s - loss: 0.59 - ETA: 34s - loss: 0.59 - ETA: 33s - loss: 0.59 - ETA: 31s - loss: 0.59 - ETA: 29s - loss: 0.59 - ETA: 27s - loss: 0.59 - ETA: 26s - loss: 0.59 - ETA: 24s - loss: 0.59 - ETA: 23s - loss: 0.59 - ETA: 21s - loss: 0.59 - ETA: 19s - loss: 0.59 - ETA: 18s - loss: 0.59 - ETA: 16s - loss: 0.59 - ETA: 14s - loss: 0.59 - ETA: 13s - loss: 0.59 - ETA: 11s - loss: 0.59 - ETA: 9s - loss: 0.5972 - ETA: 7s - loss: 0.597 - ETA: 5s - loss: 0.597 - ETA: 4s - loss: 0.598 - ETA: 2s - loss: 0.597 - ETA: 0s - loss: 0.596 - 62s 4ms/step - loss: 0.5956 - val_loss: 0.5624\n",
      "Epoch 21/75\n",
      "17621/17621 [==============================] - ETA: 56s - loss: 0.59 - ETA: 54s - loss: 0.57 - ETA: 52s - loss: 0.59 - ETA: 50s - loss: 0.59 - ETA: 49s - loss: 0.59 - ETA: 47s - loss: 0.58 - ETA: 45s - loss: 0.59 - ETA: 43s - loss: 0.58 - ETA: 42s - loss: 0.59 - ETA: 40s - loss: 0.59 - ETA: 39s - loss: 0.59 - ETA: 37s - loss: 0.59 - ETA: 36s - loss: 0.59 - ETA: 35s - loss: 0.59 - ETA: 33s - loss: 0.59 - ETA: 31s - loss: 0.59 - ETA: 29s - loss: 0.58 - ETA: 27s - loss: 0.58 - ETA: 26s - loss: 0.58 - ETA: 24s - loss: 0.58 - ETA: 23s - loss: 0.58 - ETA: 21s - loss: 0.58 - ETA: 19s - loss: 0.58 - ETA: 17s - loss: 0.58 - ETA: 16s - loss: 0.58 - ETA: 14s - loss: 0.58 - ETA: 12s - loss: 0.58 - ETA: 11s - loss: 0.58 - ETA: 9s - loss: 0.5829 - ETA: 7s - loss: 0.582 - ETA: 5s - loss: 0.583 - ETA: 4s - loss: 0.582 - ETA: 2s - loss: 0.581 - ETA: 0s - loss: 0.582 - 61s 3ms/step - loss: 0.5830 - val_loss: 0.5664\n",
      "Epoch 22/75\n",
      "17621/17621 [==============================] - ETA: 55s - loss: 0.54 - ETA: 53s - loss: 0.54 - ETA: 52s - loss: 0.55 - ETA: 50s - loss: 0.55 - ETA: 48s - loss: 0.54 - ETA: 47s - loss: 0.55 - ETA: 46s - loss: 0.55 - ETA: 44s - loss: 0.55 - ETA: 42s - loss: 0.55 - ETA: 41s - loss: 0.54 - ETA: 39s - loss: 0.54 - ETA: 37s - loss: 0.55 - ETA: 36s - loss: 0.55 - ETA: 35s - loss: 0.55 - ETA: 33s - loss: 0.55 - ETA: 31s - loss: 0.55 - ETA: 30s - loss: 0.55 - ETA: 28s - loss: 0.55 - ETA: 26s - loss: 0.55 - ETA: 25s - loss: 0.55 - ETA: 23s - loss: 0.55 - ETA: 21s - loss: 0.55 - ETA: 19s - loss: 0.55 - ETA: 18s - loss: 0.55 - ETA: 16s - loss: 0.55 - ETA: 14s - loss: 0.55 - ETA: 12s - loss: 0.55 - ETA: 11s - loss: 0.56 - ETA: 9s - loss: 0.5637 - ETA: 7s - loss: 0.566 - ETA: 5s - loss: 0.565 - ETA: 4s - loss: 0.564 - ETA: 2s - loss: 0.563 - ETA: 0s - loss: 0.563 - 61s 3ms/step - loss: 0.5646 - val_loss: 0.5470\n",
      "Epoch 23/75\n",
      "17621/17621 [==============================] - ETA: 55s - loss: 0.62 - ETA: 54s - loss: 0.58 - ETA: 53s - loss: 0.56 - ETA: 51s - loss: 0.55 - ETA: 49s - loss: 0.55 - ETA: 47s - loss: 0.54 - ETA: 46s - loss: 0.54 - ETA: 44s - loss: 0.54 - ETA: 42s - loss: 0.55 - ETA: 40s - loss: 0.55 - ETA: 39s - loss: 0.55 - ETA: 37s - loss: 0.55 - ETA: 36s - loss: 0.55 - ETA: 34s - loss: 0.55 - ETA: 33s - loss: 0.55 - ETA: 31s - loss: 0.55 - ETA: 29s - loss: 0.55 - ETA: 27s - loss: 0.55 - ETA: 26s - loss: 0.55 - ETA: 24s - loss: 0.55 - ETA: 22s - loss: 0.55 - ETA: 21s - loss: 0.56 - ETA: 19s - loss: 0.56 - ETA: 17s - loss: 0.56 - ETA: 15s - loss: 0.56 - ETA: 14s - loss: 0.56 - ETA: 12s - loss: 0.56 - ETA: 10s - loss: 0.56 - ETA: 9s - loss: 0.5642 - ETA: 7s - loss: 0.563 - ETA: 5s - loss: 0.562 - ETA: 4s - loss: 0.561 - ETA: 2s - loss: 0.561 - ETA: 0s - loss: 0.560 - 62s 3ms/step - loss: 0.5607 - val_loss: 0.5506\n",
      "Epoch 24/75\n",
      "17621/17621 [==============================] - ETA: 56s - loss: 0.51 - ETA: 54s - loss: 0.52 - ETA: 52s - loss: 0.54 - ETA: 50s - loss: 0.54 - ETA: 48s - loss: 0.55 - ETA: 47s - loss: 0.55 - ETA: 45s - loss: 0.54 - ETA: 44s - loss: 0.55 - ETA: 42s - loss: 0.54 - ETA: 40s - loss: 0.55 - ETA: 39s - loss: 0.55 - ETA: 37s - loss: 0.54 - ETA: 35s - loss: 0.55 - ETA: 34s - loss: 0.55 - ETA: 33s - loss: 0.55 - ETA: 31s - loss: 0.55 - ETA: 29s - loss: 0.55 - ETA: 27s - loss: 0.54 - ETA: 26s - loss: 0.54 - ETA: 24s - loss: 0.54 - ETA: 22s - loss: 0.54 - ETA: 21s - loss: 0.54 - ETA: 19s - loss: 0.54 - ETA: 17s - loss: 0.54 - ETA: 15s - loss: 0.54 - ETA: 14s - loss: 0.54 - ETA: 12s - loss: 0.54 - ETA: 10s - loss: 0.54 - ETA: 9s - loss: 0.5411 - ETA: 7s - loss: 0.542 - ETA: 5s - loss: 0.540 - ETA: 4s - loss: 0.538 - ETA: 2s - loss: 0.538 - ETA: 0s - loss: 0.540 - 61s 3ms/step - loss: 0.5407 - val_loss: 0.5403\n",
      "Epoch 25/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17621/17621 [==============================] - ETA: 54s - loss: 0.52 - ETA: 52s - loss: 0.53 - ETA: 51s - loss: 0.53 - ETA: 50s - loss: 0.54 - ETA: 50s - loss: 0.53 - ETA: 49s - loss: 0.53 - ETA: 47s - loss: 0.53 - ETA: 46s - loss: 0.53 - ETA: 44s - loss: 0.54 - ETA: 42s - loss: 0.54 - ETA: 41s - loss: 0.54 - ETA: 39s - loss: 0.53 - ETA: 37s - loss: 0.53 - ETA: 36s - loss: 0.53 - ETA: 34s - loss: 0.53 - ETA: 32s - loss: 0.53 - ETA: 30s - loss: 0.53 - ETA: 28s - loss: 0.53 - ETA: 27s - loss: 0.53 - ETA: 25s - loss: 0.53 - ETA: 23s - loss: 0.53 - ETA: 21s - loss: 0.52 - ETA: 19s - loss: 0.53 - ETA: 18s - loss: 0.52 - ETA: 16s - loss: 0.53 - ETA: 14s - loss: 0.53 - ETA: 12s - loss: 0.53 - ETA: 11s - loss: 0.53 - ETA: 9s - loss: 0.5308 - ETA: 7s - loss: 0.532 - ETA: 5s - loss: 0.533 - ETA: 4s - loss: 0.534 - ETA: 2s - loss: 0.533 - ETA: 0s - loss: 0.532 - 62s 3ms/step - loss: 0.5332 - val_loss: 0.5407\n",
      "Epoch 26/75\n",
      "17621/17621 [==============================] - ETA: 54s - loss: 0.50 - ETA: 52s - loss: 0.52 - ETA: 51s - loss: 0.51 - ETA: 50s - loss: 0.52 - ETA: 48s - loss: 0.52 - ETA: 47s - loss: 0.53 - ETA: 45s - loss: 0.53 - ETA: 43s - loss: 0.52 - ETA: 42s - loss: 0.52 - ETA: 40s - loss: 0.52 - ETA: 38s - loss: 0.52 - ETA: 37s - loss: 0.52 - ETA: 35s - loss: 0.52 - ETA: 33s - loss: 0.52 - ETA: 32s - loss: 0.52 - ETA: 31s - loss: 0.51 - ETA: 29s - loss: 0.51 - ETA: 27s - loss: 0.51 - ETA: 26s - loss: 0.51 - ETA: 24s - loss: 0.51 - ETA: 23s - loss: 0.51 - ETA: 21s - loss: 0.51 - ETA: 19s - loss: 0.51 - ETA: 17s - loss: 0.50 - ETA: 16s - loss: 0.51 - ETA: 14s - loss: 0.50 - ETA: 12s - loss: 0.50 - ETA: 11s - loss: 0.51 - ETA: 9s - loss: 0.5119 - ETA: 7s - loss: 0.514 - ETA: 5s - loss: 0.514 - ETA: 4s - loss: 0.515 - ETA: 2s - loss: 0.515 - ETA: 0s - loss: 0.516 - 61s 3ms/step - loss: 0.5163 - val_loss: 0.5300\n",
      "Epoch 27/75\n",
      "17621/17621 [==============================] - ETA: 57s - loss: 0.49 - ETA: 54s - loss: 0.51 - ETA: 53s - loss: 0.50 - ETA: 51s - loss: 0.51 - ETA: 49s - loss: 0.52 - ETA: 47s - loss: 0.50 - ETA: 46s - loss: 0.50 - ETA: 44s - loss: 0.50 - ETA: 42s - loss: 0.50 - ETA: 40s - loss: 0.50 - ETA: 39s - loss: 0.50 - ETA: 37s - loss: 0.51 - ETA: 35s - loss: 0.51 - ETA: 34s - loss: 0.51 - ETA: 32s - loss: 0.51 - ETA: 31s - loss: 0.51 - ETA: 29s - loss: 0.51 - ETA: 27s - loss: 0.51 - ETA: 26s - loss: 0.51 - ETA: 24s - loss: 0.51 - ETA: 22s - loss: 0.51 - ETA: 21s - loss: 0.51 - ETA: 19s - loss: 0.52 - ETA: 17s - loss: 0.51 - ETA: 16s - loss: 0.52 - ETA: 14s - loss: 0.52 - ETA: 12s - loss: 0.52 - ETA: 10s - loss: 0.52 - ETA: 9s - loss: 0.5233 - ETA: 7s - loss: 0.523 - ETA: 5s - loss: 0.521 - ETA: 4s - loss: 0.520 - ETA: 2s - loss: 0.520 - ETA: 0s - loss: 0.521 - 61s 3ms/step - loss: 0.5218 - val_loss: 0.5267\n",
      "Epoch 28/75\n",
      "17621/17621 [==============================] - ETA: 55s - loss: 0.46 - ETA: 54s - loss: 0.47 - ETA: 54s - loss: 0.48 - ETA: 52s - loss: 0.48 - ETA: 50s - loss: 0.49 - ETA: 48s - loss: 0.49 - ETA: 46s - loss: 0.49 - ETA: 44s - loss: 0.49 - ETA: 43s - loss: 0.49 - ETA: 41s - loss: 0.50 - ETA: 39s - loss: 0.49 - ETA: 37s - loss: 0.49 - ETA: 36s - loss: 0.49 - ETA: 34s - loss: 0.50 - ETA: 32s - loss: 0.49 - ETA: 31s - loss: 0.49 - ETA: 29s - loss: 0.49 - ETA: 28s - loss: 0.50 - ETA: 26s - loss: 0.50 - ETA: 24s - loss: 0.50 - ETA: 22s - loss: 0.50 - ETA: 21s - loss: 0.50 - ETA: 19s - loss: 0.50 - ETA: 17s - loss: 0.50 - ETA: 16s - loss: 0.50 - ETA: 14s - loss: 0.50 - ETA: 13s - loss: 0.50 - ETA: 11s - loss: 0.50 - ETA: 9s - loss: 0.5050 - ETA: 7s - loss: 0.506 - ETA: 6s - loss: 0.508 - ETA: 4s - loss: 0.507 - ETA: 2s - loss: 0.510 - ETA: 0s - loss: 0.510 - 65s 4ms/step - loss: 0.5108 - val_loss: 0.5232\n",
      "Epoch 29/75\n",
      "17621/17621 [==============================] - ETA: 1:00 - loss: 0.499 - ETA: 1:00 - loss: 0.522 - ETA: 59s - loss: 0.515 - ETA: 57s - loss: 0.50 - ETA: 54s - loss: 0.49 - ETA: 53s - loss: 0.50 - ETA: 52s - loss: 0.50 - ETA: 51s - loss: 0.50 - ETA: 49s - loss: 0.50 - ETA: 48s - loss: 0.50 - ETA: 46s - loss: 0.50 - ETA: 44s - loss: 0.50 - ETA: 43s - loss: 0.49 - ETA: 40s - loss: 0.50 - ETA: 38s - loss: 0.49 - ETA: 36s - loss: 0.49 - ETA: 33s - loss: 0.50 - ETA: 31s - loss: 0.50 - ETA: 29s - loss: 0.50 - ETA: 27s - loss: 0.50 - ETA: 25s - loss: 0.50 - ETA: 23s - loss: 0.49 - ETA: 21s - loss: 0.50 - ETA: 19s - loss: 0.50 - ETA: 17s - loss: 0.50 - ETA: 15s - loss: 0.50 - ETA: 13s - loss: 0.50 - ETA: 11s - loss: 0.49 - ETA: 9s - loss: 0.5003 - ETA: 8s - loss: 0.501 - ETA: 6s - loss: 0.501 - ETA: 4s - loss: 0.500 - ETA: 2s - loss: 0.500 - ETA: 0s - loss: 0.499 - 65s 4ms/step - loss: 0.5003 - val_loss: 0.5303\n",
      "Epoch 30/75\n",
      "17621/17621 [==============================] - ETA: 55s - loss: 0.46 - ETA: 53s - loss: 0.51 - ETA: 52s - loss: 0.52 - ETA: 50s - loss: 0.50 - ETA: 48s - loss: 0.49 - ETA: 47s - loss: 0.49 - ETA: 45s - loss: 0.50 - ETA: 43s - loss: 0.50 - ETA: 42s - loss: 0.50 - ETA: 40s - loss: 0.50 - ETA: 39s - loss: 0.50 - ETA: 37s - loss: 0.49 - ETA: 35s - loss: 0.49 - ETA: 34s - loss: 0.49 - ETA: 33s - loss: 0.49 - ETA: 31s - loss: 0.49 - ETA: 30s - loss: 0.49 - ETA: 29s - loss: 0.49 - ETA: 27s - loss: 0.49 - ETA: 26s - loss: 0.49 - ETA: 24s - loss: 0.49 - ETA: 22s - loss: 0.49 - ETA: 20s - loss: 0.49 - ETA: 18s - loss: 0.49 - ETA: 16s - loss: 0.49 - ETA: 15s - loss: 0.49 - ETA: 13s - loss: 0.49 - ETA: 11s - loss: 0.49 - ETA: 9s - loss: 0.4929 - ETA: 7s - loss: 0.491 - ETA: 6s - loss: 0.491 - ETA: 4s - loss: 0.491 - ETA: 2s - loss: 0.490 - ETA: 0s - loss: 0.490 - 63s 4ms/step - loss: 0.4913 - val_loss: 0.5344\n",
      "Epoch 31/75\n",
      "17621/17621 [==============================] - ETA: 56s - loss: 0.44 - ETA: 53s - loss: 0.45 - ETA: 52s - loss: 0.45 - ETA: 50s - loss: 0.44 - ETA: 48s - loss: 0.46 - ETA: 47s - loss: 0.46 - ETA: 45s - loss: 0.46 - ETA: 43s - loss: 0.46 - ETA: 42s - loss: 0.46 - ETA: 40s - loss: 0.47 - ETA: 38s - loss: 0.47 - ETA: 37s - loss: 0.47 - ETA: 36s - loss: 0.47 - ETA: 35s - loss: 0.48 - ETA: 33s - loss: 0.48 - ETA: 31s - loss: 0.48 - ETA: 30s - loss: 0.47 - ETA: 28s - loss: 0.47 - ETA: 26s - loss: 0.47 - ETA: 25s - loss: 0.47 - ETA: 23s - loss: 0.47 - ETA: 21s - loss: 0.47 - ETA: 19s - loss: 0.48 - ETA: 18s - loss: 0.48 - ETA: 16s - loss: 0.47 - ETA: 14s - loss: 0.47 - ETA: 12s - loss: 0.47 - ETA: 11s - loss: 0.48 - ETA: 9s - loss: 0.4818 - ETA: 7s - loss: 0.481 - ETA: 5s - loss: 0.482 - ETA: 4s - loss: 0.481 - ETA: 2s - loss: 0.482 - ETA: 0s - loss: 0.482 - 61s 3ms/step - loss: 0.4824 - val_loss: 0.5231\n",
      "Epoch 32/75\n",
      "17621/17621 [==============================] - ETA: 56s - loss: 0.50 - ETA: 53s - loss: 0.45 - ETA: 52s - loss: 0.44 - ETA: 50s - loss: 0.44 - ETA: 48s - loss: 0.45 - ETA: 48s - loss: 0.44 - ETA: 46s - loss: 0.44 - ETA: 44s - loss: 0.45 - ETA: 43s - loss: 0.46 - ETA: 41s - loss: 0.46 - ETA: 39s - loss: 0.46 - ETA: 38s - loss: 0.47 - ETA: 36s - loss: 0.47 - ETA: 35s - loss: 0.47 - ETA: 33s - loss: 0.46 - ETA: 31s - loss: 0.46 - ETA: 30s - loss: 0.47 - ETA: 28s - loss: 0.47 - ETA: 26s - loss: 0.47 - ETA: 25s - loss: 0.47 - ETA: 23s - loss: 0.47 - ETA: 21s - loss: 0.47 - ETA: 19s - loss: 0.47 - ETA: 17s - loss: 0.46 - ETA: 16s - loss: 0.46 - ETA: 14s - loss: 0.46 - ETA: 12s - loss: 0.47 - ETA: 11s - loss: 0.47 - ETA: 9s - loss: 0.4718 - ETA: 7s - loss: 0.472 - ETA: 5s - loss: 0.472 - ETA: 4s - loss: 0.469 - ETA: 2s - loss: 0.470 - ETA: 0s - loss: 0.469 - 62s 4ms/step - loss: 0.4706 - val_loss: 0.5274\n",
      "Epoch 33/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17621/17621 [==============================] - ETA: 1:16 - loss: 0.449 - ETA: 1:12 - loss: 0.451 - ETA: 1:14 - loss: 0.462 - ETA: 1:10 - loss: 0.461 - ETA: 1:05 - loss: 0.456 - ETA: 1:03 - loss: 0.455 - ETA: 59s - loss: 0.463 - ETA: 55s - loss: 0.46 - ETA: 52s - loss: 0.46 - ETA: 49s - loss: 0.46 - ETA: 47s - loss: 0.46 - ETA: 46s - loss: 0.46 - ETA: 44s - loss: 0.46 - ETA: 42s - loss: 0.47 - ETA: 40s - loss: 0.46 - ETA: 37s - loss: 0.47 - ETA: 35s - loss: 0.46 - ETA: 33s - loss: 0.46 - ETA: 31s - loss: 0.46 - ETA: 29s - loss: 0.46 - ETA: 27s - loss: 0.46 - ETA: 25s - loss: 0.46 - ETA: 23s - loss: 0.46 - ETA: 21s - loss: 0.46 - ETA: 19s - loss: 0.46 - ETA: 17s - loss: 0.46 - ETA: 15s - loss: 0.46 - ETA: 13s - loss: 0.46 - ETA: 11s - loss: 0.46 - ETA: 9s - loss: 0.4655 - ETA: 6s - loss: 0.464 - ETA: 4s - loss: 0.465 - ETA: 2s - loss: 0.465 - ETA: 0s - loss: 0.465 - 72s 4ms/step - loss: 0.4655 - val_loss: 0.5234\n",
      "Epoch 34/75\n",
      "17621/17621 [==============================] - ETA: 55s - loss: 0.45 - ETA: 53s - loss: 0.45 - ETA: 52s - loss: 0.43 - ETA: 50s - loss: 0.43 - ETA: 49s - loss: 0.45 - ETA: 47s - loss: 0.44 - ETA: 45s - loss: 0.43 - ETA: 43s - loss: 0.43 - ETA: 43s - loss: 0.44 - ETA: 42s - loss: 0.44 - ETA: 41s - loss: 0.44 - ETA: 40s - loss: 0.44 - ETA: 40s - loss: 0.44 - ETA: 39s - loss: 0.44 - ETA: 39s - loss: 0.44 - ETA: 37s - loss: 0.44 - ETA: 35s - loss: 0.44 - ETA: 34s - loss: 0.45 - ETA: 32s - loss: 0.44 - ETA: 29s - loss: 0.44 - ETA: 27s - loss: 0.44 - ETA: 25s - loss: 0.44 - ETA: 22s - loss: 0.44 - ETA: 20s - loss: 0.45 - ETA: 18s - loss: 0.45 - ETA: 16s - loss: 0.45 - ETA: 14s - loss: 0.45 - ETA: 12s - loss: 0.45 - ETA: 10s - loss: 0.45 - ETA: 8s - loss: 0.4569 - ETA: 6s - loss: 0.457 - ETA: 4s - loss: 0.456 - ETA: 2s - loss: 0.458 - ETA: 0s - loss: 0.458 - 68s 4ms/step - loss: 0.4575 - val_loss: 0.5161\n",
      "Epoch 35/75\n",
      "17621/17621 [==============================] - ETA: 52s - loss: 0.45 - ETA: 51s - loss: 0.45 - ETA: 50s - loss: 0.46 - ETA: 49s - loss: 0.45 - ETA: 52s - loss: 0.45 - ETA: 53s - loss: 0.44 - ETA: 52s - loss: 0.44 - ETA: 51s - loss: 0.44 - ETA: 50s - loss: 0.44 - ETA: 48s - loss: 0.44 - ETA: 46s - loss: 0.43 - ETA: 45s - loss: 0.43 - ETA: 43s - loss: 0.43 - ETA: 41s - loss: 0.43 - ETA: 39s - loss: 0.43 - ETA: 37s - loss: 0.43 - ETA: 34s - loss: 0.43 - ETA: 32s - loss: 0.43 - ETA: 30s - loss: 0.43 - ETA: 28s - loss: 0.43 - ETA: 26s - loss: 0.44 - ETA: 23s - loss: 0.44 - ETA: 21s - loss: 0.44 - ETA: 19s - loss: 0.44 - ETA: 17s - loss: 0.44 - ETA: 15s - loss: 0.44 - ETA: 13s - loss: 0.44 - ETA: 11s - loss: 0.44 - ETA: 10s - loss: 0.44 - ETA: 8s - loss: 0.4424 - ETA: 6s - loss: 0.443 - ETA: 4s - loss: 0.444 - ETA: 2s - loss: 0.443 - ETA: 0s - loss: 0.445 - 65s 4ms/step - loss: 0.4452 - val_loss: 0.5217\n",
      "Epoch 36/75\n",
      "17621/17621 [==============================] - ETA: 59s - loss: 0.42 - ETA: 55s - loss: 0.42 - ETA: 52s - loss: 0.42 - ETA: 51s - loss: 0.42 - ETA: 51s - loss: 0.43 - ETA: 49s - loss: 0.43 - ETA: 47s - loss: 0.43 - ETA: 45s - loss: 0.43 - ETA: 43s - loss: 0.43 - ETA: 41s - loss: 0.43 - ETA: 39s - loss: 0.43 - ETA: 37s - loss: 0.43 - ETA: 36s - loss: 0.44 - ETA: 34s - loss: 0.44 - ETA: 32s - loss: 0.44 - ETA: 30s - loss: 0.44 - ETA: 29s - loss: 0.44 - ETA: 27s - loss: 0.44 - ETA: 26s - loss: 0.44 - ETA: 24s - loss: 0.44 - ETA: 22s - loss: 0.44 - ETA: 21s - loss: 0.44 - ETA: 19s - loss: 0.44 - ETA: 17s - loss: 0.44 - ETA: 16s - loss: 0.44 - ETA: 14s - loss: 0.44 - ETA: 12s - loss: 0.44 - ETA: 10s - loss: 0.44 - ETA: 9s - loss: 0.4478 - ETA: 7s - loss: 0.448 - ETA: 5s - loss: 0.450 - ETA: 4s - loss: 0.449 - ETA: 2s - loss: 0.448 - ETA: 0s - loss: 0.447 - 60s 3ms/step - loss: 0.4484 - val_loss: 0.5281\n",
      "Epoch 37/75\n",
      "17621/17621 [==============================] - ETA: 52s - loss: 0.40 - ETA: 52s - loss: 0.41 - ETA: 51s - loss: 0.40 - ETA: 49s - loss: 0.41 - ETA: 48s - loss: 0.40 - ETA: 48s - loss: 0.40 - ETA: 46s - loss: 0.40 - ETA: 44s - loss: 0.40 - ETA: 42s - loss: 0.41 - ETA: 40s - loss: 0.40 - ETA: 38s - loss: 0.40 - ETA: 37s - loss: 0.41 - ETA: 35s - loss: 0.41 - ETA: 33s - loss: 0.41 - ETA: 32s - loss: 0.41 - ETA: 30s - loss: 0.41 - ETA: 28s - loss: 0.41 - ETA: 27s - loss: 0.41 - ETA: 25s - loss: 0.42 - ETA: 23s - loss: 0.41 - ETA: 22s - loss: 0.41 - ETA: 20s - loss: 0.41 - ETA: 18s - loss: 0.42 - ETA: 17s - loss: 0.42 - ETA: 15s - loss: 0.42 - ETA: 13s - loss: 0.42 - ETA: 12s - loss: 0.42 - ETA: 10s - loss: 0.42 - ETA: 8s - loss: 0.4241 - ETA: 7s - loss: 0.425 - ETA: 5s - loss: 0.426 - ETA: 3s - loss: 0.425 - ETA: 2s - loss: 0.425 - ETA: 0s - loss: 0.427 - 59s 3ms/step - loss: 0.4275 - val_loss: 0.5270\n",
      "Epoch 38/75\n",
      "17621/17621 [==============================] - ETA: 52s - loss: 0.43 - ETA: 51s - loss: 0.44 - ETA: 50s - loss: 0.43 - ETA: 48s - loss: 0.42 - ETA: 47s - loss: 0.43 - ETA: 46s - loss: 0.42 - ETA: 44s - loss: 0.42 - ETA: 44s - loss: 0.43 - ETA: 42s - loss: 0.43 - ETA: 40s - loss: 0.43 - ETA: 38s - loss: 0.43 - ETA: 37s - loss: 0.43 - ETA: 35s - loss: 0.43 - ETA: 33s - loss: 0.43 - ETA: 32s - loss: 0.43 - ETA: 30s - loss: 0.43 - ETA: 28s - loss: 0.43 - ETA: 27s - loss: 0.43 - ETA: 25s - loss: 0.43 - ETA: 23s - loss: 0.43 - ETA: 22s - loss: 0.43 - ETA: 20s - loss: 0.43 - ETA: 18s - loss: 0.43 - ETA: 17s - loss: 0.43 - ETA: 15s - loss: 0.43 - ETA: 13s - loss: 0.43 - ETA: 12s - loss: 0.43 - ETA: 10s - loss: 0.43 - ETA: 8s - loss: 0.4318 - ETA: 7s - loss: 0.431 - ETA: 5s - loss: 0.429 - ETA: 3s - loss: 0.430 - ETA: 2s - loss: 0.429 - ETA: 0s - loss: 0.430 - 59s 3ms/step - loss: 0.4300 - val_loss: 0.5224\n",
      "Epoch 39/75\n",
      "17621/17621 [==============================] - ETA: 51s - loss: 0.38 - ETA: 51s - loss: 0.38 - ETA: 53s - loss: 0.38 - ETA: 51s - loss: 0.38 - ETA: 49s - loss: 0.38 - ETA: 47s - loss: 0.39 - ETA: 45s - loss: 0.39 - ETA: 43s - loss: 0.39 - ETA: 43s - loss: 0.39 - ETA: 41s - loss: 0.39 - ETA: 39s - loss: 0.40 - ETA: 37s - loss: 0.40 - ETA: 36s - loss: 0.40 - ETA: 34s - loss: 0.40 - ETA: 32s - loss: 0.40 - ETA: 30s - loss: 0.40 - ETA: 29s - loss: 0.40 - ETA: 27s - loss: 0.40 - ETA: 25s - loss: 0.40 - ETA: 24s - loss: 0.41 - ETA: 22s - loss: 0.41 - ETA: 20s - loss: 0.41 - ETA: 19s - loss: 0.41 - ETA: 17s - loss: 0.41 - ETA: 15s - loss: 0.41 - ETA: 14s - loss: 0.41 - ETA: 12s - loss: 0.41 - ETA: 10s - loss: 0.41 - ETA: 9s - loss: 0.4185 - ETA: 7s - loss: 0.418 - ETA: 5s - loss: 0.418 - ETA: 4s - loss: 0.420 - ETA: 2s - loss: 0.420 - ETA: 0s - loss: 0.418 - 60s 3ms/step - loss: 0.4189 - val_loss: 0.5278\n",
      "Epoch 40/75\n",
      "17621/17621 [==============================] - ETA: 52s - loss: 0.36 - ETA: 51s - loss: 0.38 - ETA: 50s - loss: 0.39 - ETA: 49s - loss: 0.38 - ETA: 47s - loss: 0.38 - ETA: 46s - loss: 0.39 - ETA: 45s - loss: 0.40 - ETA: 45s - loss: 0.40 - ETA: 43s - loss: 0.40 - ETA: 42s - loss: 0.41 - ETA: 41s - loss: 0.41 - ETA: 40s - loss: 0.41 - ETA: 38s - loss: 0.41 - ETA: 36s - loss: 0.41 - ETA: 34s - loss: 0.42 - ETA: 32s - loss: 0.42 - ETA: 30s - loss: 0.42 - ETA: 28s - loss: 0.42 - ETA: 26s - loss: 0.42 - ETA: 25s - loss: 0.42 - ETA: 23s - loss: 0.42 - ETA: 21s - loss: 0.42 - ETA: 20s - loss: 0.42 - ETA: 18s - loss: 0.42 - ETA: 16s - loss: 0.42 - ETA: 14s - loss: 0.42 - ETA: 12s - loss: 0.42 - ETA: 11s - loss: 0.42 - ETA: 9s - loss: 0.4240 - ETA: 7s - loss: 0.423 - ETA: 5s - loss: 0.422 - ETA: 4s - loss: 0.420 - ETA: 2s - loss: 0.418 - ETA: 0s - loss: 0.418 - 62s 3ms/step - loss: 0.4175 - val_loss: 0.5305\n",
      "Epoch 41/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17621/17621 [==============================] - ETA: 55s - loss: 0.36 - ETA: 54s - loss: 0.39 - ETA: 53s - loss: 0.39 - ETA: 52s - loss: 0.40 - ETA: 50s - loss: 0.39 - ETA: 47s - loss: 0.40 - ETA: 45s - loss: 0.40 - ETA: 44s - loss: 0.40 - ETA: 42s - loss: 0.40 - ETA: 40s - loss: 0.40 - ETA: 38s - loss: 0.41 - ETA: 38s - loss: 0.41 - ETA: 36s - loss: 0.41 - ETA: 34s - loss: 0.40 - ETA: 32s - loss: 0.40 - ETA: 31s - loss: 0.40 - ETA: 29s - loss: 0.40 - ETA: 27s - loss: 0.40 - ETA: 25s - loss: 0.40 - ETA: 24s - loss: 0.40 - ETA: 22s - loss: 0.40 - ETA: 20s - loss: 0.40 - ETA: 19s - loss: 0.40 - ETA: 17s - loss: 0.40 - ETA: 15s - loss: 0.40 - ETA: 14s - loss: 0.41 - ETA: 12s - loss: 0.41 - ETA: 10s - loss: 0.41 - ETA: 9s - loss: 0.4114 - ETA: 7s - loss: 0.411 - ETA: 5s - loss: 0.411 - ETA: 4s - loss: 0.411 - ETA: 2s - loss: 0.412 - ETA: 0s - loss: 0.412 - 59s 3ms/step - loss: 0.4123 - val_loss: 0.5277\n",
      "Epoch 42/75\n",
      "17621/17621 [==============================] - ETA: 52s - loss: 0.37 - ETA: 51s - loss: 0.37 - ETA: 49s - loss: 0.38 - ETA: 48s - loss: 0.39 - ETA: 47s - loss: 0.38 - ETA: 45s - loss: 0.37 - ETA: 44s - loss: 0.38 - ETA: 42s - loss: 0.39 - ETA: 40s - loss: 0.39 - ETA: 39s - loss: 0.39 - ETA: 37s - loss: 0.39 - ETA: 36s - loss: 0.40 - ETA: 34s - loss: 0.40 - ETA: 33s - loss: 0.40 - ETA: 31s - loss: 0.40 - ETA: 30s - loss: 0.40 - ETA: 28s - loss: 0.40 - ETA: 26s - loss: 0.40 - ETA: 25s - loss: 0.40 - ETA: 23s - loss: 0.40 - ETA: 22s - loss: 0.40 - ETA: 20s - loss: 0.40 - ETA: 18s - loss: 0.40 - ETA: 17s - loss: 0.40 - ETA: 15s - loss: 0.40 - ETA: 13s - loss: 0.40 - ETA: 12s - loss: 0.40 - ETA: 10s - loss: 0.40 - ETA: 8s - loss: 0.4014 - ETA: 7s - loss: 0.401 - ETA: 5s - loss: 0.401 - ETA: 4s - loss: 0.402 - ETA: 2s - loss: 0.404 - ETA: 0s - loss: 0.404 - 59s 3ms/step - loss: 0.4048 - val_loss: 0.5347\n",
      "Epoch 43/75\n",
      "17621/17621 [==============================] - ETA: 52s - loss: 0.37 - ETA: 51s - loss: 0.38 - ETA: 49s - loss: 0.40 - ETA: 48s - loss: 0.40 - ETA: 47s - loss: 0.41 - ETA: 45s - loss: 0.40 - ETA: 44s - loss: 0.40 - ETA: 42s - loss: 0.40 - ETA: 40s - loss: 0.40 - ETA: 39s - loss: 0.40 - ETA: 37s - loss: 0.40 - ETA: 36s - loss: 0.39 - ETA: 34s - loss: 0.39 - ETA: 32s - loss: 0.39 - ETA: 31s - loss: 0.39 - ETA: 30s - loss: 0.39 - ETA: 28s - loss: 0.39 - ETA: 26s - loss: 0.39 - ETA: 25s - loss: 0.39 - ETA: 23s - loss: 0.39 - ETA: 22s - loss: 0.39 - ETA: 20s - loss: 0.39 - ETA: 19s - loss: 0.39 - ETA: 17s - loss: 0.39 - ETA: 15s - loss: 0.40 - ETA: 14s - loss: 0.39 - ETA: 12s - loss: 0.40 - ETA: 10s - loss: 0.40 - ETA: 9s - loss: 0.4026 - ETA: 7s - loss: 0.401 - ETA: 5s - loss: 0.400 - ETA: 4s - loss: 0.401 - ETA: 2s - loss: 0.401 - ETA: 0s - loss: 0.401 - 61s 3ms/step - loss: 0.4024 - val_loss: 0.5185\n",
      "Epoch 44/75\n",
      "17621/17621 [==============================] - ETA: 52s - loss: 0.38 - ETA: 52s - loss: 0.40 - ETA: 51s - loss: 0.38 - ETA: 49s - loss: 0.38 - ETA: 48s - loss: 0.38 - ETA: 47s - loss: 0.38 - ETA: 45s - loss: 0.38 - ETA: 44s - loss: 0.38 - ETA: 43s - loss: 0.38 - ETA: 41s - loss: 0.38 - ETA: 40s - loss: 0.38 - ETA: 38s - loss: 0.38 - ETA: 36s - loss: 0.38 - ETA: 34s - loss: 0.38 - ETA: 33s - loss: 0.38 - ETA: 31s - loss: 0.38 - ETA: 30s - loss: 0.38 - ETA: 28s - loss: 0.38 - ETA: 26s - loss: 0.38 - ETA: 24s - loss: 0.38 - ETA: 22s - loss: 0.38 - ETA: 21s - loss: 0.38 - ETA: 19s - loss: 0.38 - ETA: 18s - loss: 0.38 - ETA: 16s - loss: 0.38 - ETA: 14s - loss: 0.38 - ETA: 13s - loss: 0.38 - ETA: 11s - loss: 0.38 - ETA: 9s - loss: 0.3863 - ETA: 8s - loss: 0.386 - ETA: 6s - loss: 0.385 - ETA: 4s - loss: 0.384 - ETA: 2s - loss: 0.385 - ETA: 0s - loss: 0.386 - 68s 4ms/step - loss: 0.3865 - val_loss: 0.5262\n",
      "Epoch 45/75\n",
      "17621/17621 [==============================] - ETA: 1:17 - loss: 0.376 - ETA: 1:13 - loss: 0.374 - ETA: 1:08 - loss: 0.387 - ETA: 1:03 - loss: 0.390 - ETA: 58s - loss: 0.395 - ETA: 54s - loss: 0.38 - ETA: 53s - loss: 0.39 - ETA: 51s - loss: 0.38 - ETA: 51s - loss: 0.38 - ETA: 49s - loss: 0.39 - ETA: 49s - loss: 0.38 - ETA: 47s - loss: 0.38 - ETA: 45s - loss: 0.38 - ETA: 43s - loss: 0.38 - ETA: 40s - loss: 0.38 - ETA: 37s - loss: 0.38 - ETA: 36s - loss: 0.38 - ETA: 33s - loss: 0.38 - ETA: 31s - loss: 0.39 - ETA: 29s - loss: 0.39 - ETA: 27s - loss: 0.39 - ETA: 25s - loss: 0.39 - ETA: 23s - loss: 0.39 - ETA: 20s - loss: 0.39 - ETA: 18s - loss: 0.39 - ETA: 16s - loss: 0.39 - ETA: 14s - loss: 0.39 - ETA: 12s - loss: 0.39 - ETA: 10s - loss: 0.39 - ETA: 8s - loss: 0.3964 - ETA: 6s - loss: 0.397 - ETA: 4s - loss: 0.397 - ETA: 2s - loss: 0.395 - ETA: 0s - loss: 0.396 - 69s 4ms/step - loss: 0.3960 - val_loss: 0.5232\n",
      "Epoch 46/75\n",
      "17621/17621 [==============================] - ETA: 55s - loss: 0.39 - ETA: 56s - loss: 0.39 - ETA: 57s - loss: 0.39 - ETA: 57s - loss: 0.37 - ETA: 55s - loss: 0.38 - ETA: 53s - loss: 0.37 - ETA: 50s - loss: 0.38 - ETA: 50s - loss: 0.37 - ETA: 47s - loss: 0.37 - ETA: 45s - loss: 0.38 - ETA: 43s - loss: 0.38 - ETA: 41s - loss: 0.38 - ETA: 39s - loss: 0.38 - ETA: 39s - loss: 0.38 - ETA: 37s - loss: 0.38 - ETA: 35s - loss: 0.38 - ETA: 33s - loss: 0.38 - ETA: 32s - loss: 0.38 - ETA: 30s - loss: 0.38 - ETA: 28s - loss: 0.38 - ETA: 25s - loss: 0.38 - ETA: 23s - loss: 0.38 - ETA: 21s - loss: 0.38 - ETA: 19s - loss: 0.38 - ETA: 17s - loss: 0.38 - ETA: 15s - loss: 0.38 - ETA: 13s - loss: 0.38 - ETA: 12s - loss: 0.38 - ETA: 10s - loss: 0.38 - ETA: 8s - loss: 0.3829 - ETA: 6s - loss: 0.381 - ETA: 4s - loss: 0.381 - ETA: 2s - loss: 0.381 - ETA: 0s - loss: 0.382 - 65s 4ms/step - loss: 0.3823 - val_loss: 0.5276\n",
      "Epoch 47/75\n",
      "17621/17621 [==============================] - ETA: 1:10 - loss: 0.363 - ETA: 1:05 - loss: 0.359 - ETA: 59s - loss: 0.372 - ETA: 55s - loss: 0.36 - ETA: 52s - loss: 0.37 - ETA: 51s - loss: 0.37 - ETA: 49s - loss: 0.38 - ETA: 47s - loss: 0.37 - ETA: 45s - loss: 0.37 - ETA: 43s - loss: 0.37 - ETA: 41s - loss: 0.37 - ETA: 39s - loss: 0.37 - ETA: 37s - loss: 0.37 - ETA: 35s - loss: 0.38 - ETA: 34s - loss: 0.38 - ETA: 32s - loss: 0.37 - ETA: 30s - loss: 0.37 - ETA: 28s - loss: 0.37 - ETA: 26s - loss: 0.37 - ETA: 24s - loss: 0.37 - ETA: 23s - loss: 0.37 - ETA: 21s - loss: 0.37 - ETA: 19s - loss: 0.37 - ETA: 17s - loss: 0.37 - ETA: 16s - loss: 0.37 - ETA: 14s - loss: 0.37 - ETA: 12s - loss: 0.37 - ETA: 11s - loss: 0.38 - ETA: 9s - loss: 0.3800 - ETA: 7s - loss: 0.381 - ETA: 6s - loss: 0.381 - ETA: 4s - loss: 0.381 - ETA: 2s - loss: 0.382 - ETA: 0s - loss: 0.382 - 65s 4ms/step - loss: 0.3837 - val_loss: 0.5073\n",
      "Epoch 48/75\n",
      "17621/17621 [==============================] - ETA: 1:03 - loss: 0.384 - ETA: 56s - loss: 0.368 - ETA: 53s - loss: 0.36 - ETA: 50s - loss: 0.36 - ETA: 51s - loss: 0.36 - ETA: 50s - loss: 0.35 - ETA: 47s - loss: 0.36 - ETA: 45s - loss: 0.36 - ETA: 44s - loss: 0.37 - ETA: 42s - loss: 0.37 - ETA: 40s - loss: 0.37 - ETA: 38s - loss: 0.37 - ETA: 36s - loss: 0.37 - ETA: 34s - loss: 0.38 - ETA: 33s - loss: 0.38 - ETA: 31s - loss: 0.37 - ETA: 29s - loss: 0.37 - ETA: 27s - loss: 0.37 - ETA: 26s - loss: 0.37 - ETA: 24s - loss: 0.37 - ETA: 22s - loss: 0.37 - ETA: 20s - loss: 0.37 - ETA: 19s - loss: 0.37 - ETA: 17s - loss: 0.37 - ETA: 15s - loss: 0.37 - ETA: 14s - loss: 0.37 - ETA: 12s - loss: 0.37 - ETA: 10s - loss: 0.37 - ETA: 9s - loss: 0.3748 - ETA: 7s - loss: 0.374 - ETA: 5s - loss: 0.373 - ETA: 4s - loss: 0.372 - ETA: 2s - loss: 0.372 - ETA: 0s - loss: 0.374 - 59s 3ms/step - loss: 0.3728 - val_loss: 0.5371\n",
      "Epoch 49/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17621/17621 [==============================] - ETA: 53s - loss: 0.39 - ETA: 51s - loss: 0.37 - ETA: 50s - loss: 0.37 - ETA: 48s - loss: 0.37 - ETA: 47s - loss: 0.36 - ETA: 45s - loss: 0.37 - ETA: 44s - loss: 0.36 - ETA: 44s - loss: 0.37 - ETA: 42s - loss: 0.37 - ETA: 40s - loss: 0.37 - ETA: 38s - loss: 0.37 - ETA: 36s - loss: 0.37 - ETA: 35s - loss: 0.37 - ETA: 33s - loss: 0.37 - ETA: 31s - loss: 0.37 - ETA: 30s - loss: 0.37 - ETA: 28s - loss: 0.37 - ETA: 26s - loss: 0.37 - ETA: 25s - loss: 0.37 - ETA: 23s - loss: 0.37 - ETA: 21s - loss: 0.37 - ETA: 20s - loss: 0.37 - ETA: 18s - loss: 0.36 - ETA: 17s - loss: 0.37 - ETA: 15s - loss: 0.37 - ETA: 13s - loss: 0.37 - ETA: 12s - loss: 0.37 - ETA: 10s - loss: 0.37 - ETA: 8s - loss: 0.3693 - ETA: 7s - loss: 0.369 - ETA: 5s - loss: 0.369 - ETA: 3s - loss: 0.370 - ETA: 2s - loss: 0.370 - ETA: 0s - loss: 0.370 - 60s 3ms/step - loss: 0.3701 - val_loss: 0.5295\n",
      "Epoch 50/75\n",
      "17621/17621 [==============================] - ETA: 1:00 - loss: 0.376 - ETA: 56s - loss: 0.364 - ETA: 53s - loss: 0.35 - ETA: 52s - loss: 0.36 - ETA: 49s - loss: 0.35 - ETA: 47s - loss: 0.35 - ETA: 45s - loss: 0.35 - ETA: 44s - loss: 0.35 - ETA: 43s - loss: 0.36 - ETA: 41s - loss: 0.36 - ETA: 39s - loss: 0.36 - ETA: 37s - loss: 0.36 - ETA: 35s - loss: 0.36 - ETA: 34s - loss: 0.36 - ETA: 32s - loss: 0.36 - ETA: 30s - loss: 0.36 - ETA: 28s - loss: 0.36 - ETA: 27s - loss: 0.36 - ETA: 25s - loss: 0.36 - ETA: 23s - loss: 0.36 - ETA: 22s - loss: 0.36 - ETA: 20s - loss: 0.36 - ETA: 18s - loss: 0.36 - ETA: 17s - loss: 0.36 - ETA: 15s - loss: 0.36 - ETA: 13s - loss: 0.36 - ETA: 12s - loss: 0.36 - ETA: 10s - loss: 0.36 - ETA: 8s - loss: 0.3612 - ETA: 7s - loss: 0.361 - ETA: 5s - loss: 0.364 - ETA: 3s - loss: 0.364 - ETA: 2s - loss: 0.364 - ETA: 0s - loss: 0.364 - 59s 3ms/step - loss: 0.3650 - val_loss: 0.5346\n",
      "Epoch 51/75\n",
      "17621/17621 [==============================] - ETA: 53s - loss: 0.36 - ETA: 51s - loss: 0.37 - ETA: 50s - loss: 0.35 - ETA: 48s - loss: 0.36 - ETA: 47s - loss: 0.35 - ETA: 45s - loss: 0.34 - ETA: 43s - loss: 0.34 - ETA: 42s - loss: 0.35 - ETA: 41s - loss: 0.35 - ETA: 39s - loss: 0.35 - ETA: 38s - loss: 0.34 - ETA: 37s - loss: 0.34 - ETA: 35s - loss: 0.35 - ETA: 33s - loss: 0.35 - ETA: 32s - loss: 0.35 - ETA: 30s - loss: 0.36 - ETA: 28s - loss: 0.36 - ETA: 27s - loss: 0.35 - ETA: 25s - loss: 0.35 - ETA: 23s - loss: 0.35 - ETA: 22s - loss: 0.36 - ETA: 20s - loss: 0.36 - ETA: 18s - loss: 0.36 - ETA: 17s - loss: 0.35 - ETA: 15s - loss: 0.35 - ETA: 13s - loss: 0.35 - ETA: 12s - loss: 0.35 - ETA: 10s - loss: 0.35 - ETA: 8s - loss: 0.3552 - ETA: 7s - loss: 0.356 - ETA: 5s - loss: 0.355 - ETA: 3s - loss: 0.357 - ETA: 2s - loss: 0.358 - ETA: 0s - loss: 0.357 - 59s 3ms/step - loss: 0.3585 - val_loss: 0.5279\n",
      "Epoch 52/75\n",
      "17621/17621 [==============================] - ETA: 51s - loss: 0.38 - ETA: 51s - loss: 0.34 - ETA: 49s - loss: 0.34 - ETA: 48s - loss: 0.33 - ETA: 46s - loss: 0.33 - ETA: 45s - loss: 0.33 - ETA: 43s - loss: 0.34 - ETA: 42s - loss: 0.34 - ETA: 40s - loss: 0.34 - ETA: 39s - loss: 0.34 - ETA: 37s - loss: 0.35 - ETA: 36s - loss: 0.35 - ETA: 35s - loss: 0.35 - ETA: 33s - loss: 0.35 - ETA: 31s - loss: 0.35 - ETA: 30s - loss: 0.35 - ETA: 28s - loss: 0.35 - ETA: 27s - loss: 0.35 - ETA: 25s - loss: 0.35 - ETA: 24s - loss: 0.35 - ETA: 22s - loss: 0.35 - ETA: 20s - loss: 0.35 - ETA: 19s - loss: 0.35 - ETA: 17s - loss: 0.35 - ETA: 15s - loss: 0.35 - ETA: 14s - loss: 0.35 - ETA: 12s - loss: 0.35 - ETA: 10s - loss: 0.35 - ETA: 9s - loss: 0.3590 - ETA: 7s - loss: 0.358 - ETA: 5s - loss: 0.359 - ETA: 4s - loss: 0.359 - ETA: 2s - loss: 0.359 - ETA: 0s - loss: 0.357 - 63s 4ms/step - loss: 0.3569 - val_loss: 0.5398\n",
      "Epoch 53/75\n",
      "17621/17621 [==============================] - ETA: 1:19 - loss: 0.357 - ETA: 1:15 - loss: 0.379 - ETA: 1:06 - loss: 0.370 - ETA: 1:00 - loss: 0.375 - ETA: 56s - loss: 0.375 - ETA: 53s - loss: 0.36 - ETA: 51s - loss: 0.36 - ETA: 49s - loss: 0.36 - ETA: 48s - loss: 0.36 - ETA: 47s - loss: 0.35 - ETA: 44s - loss: 0.35 - ETA: 42s - loss: 0.35 - ETA: 39s - loss: 0.35 - ETA: 37s - loss: 0.35 - ETA: 36s - loss: 0.35 - ETA: 35s - loss: 0.35 - ETA: 34s - loss: 0.35 - ETA: 31s - loss: 0.35 - ETA: 30s - loss: 0.35 - ETA: 28s - loss: 0.35 - ETA: 25s - loss: 0.35 - ETA: 24s - loss: 0.35 - ETA: 22s - loss: 0.35 - ETA: 20s - loss: 0.35 - ETA: 18s - loss: 0.35 - ETA: 16s - loss: 0.35 - ETA: 14s - loss: 0.35 - ETA: 12s - loss: 0.35 - ETA: 10s - loss: 0.35 - ETA: 8s - loss: 0.3592 - ETA: 6s - loss: 0.359 - ETA: 4s - loss: 0.359 - ETA: 2s - loss: 0.359 - ETA: 0s - loss: 0.358 - 67s 4ms/step - loss: 0.3578 - val_loss: 0.5494\n",
      "Epoch 54/75\n",
      "17621/17621 [==============================] - ETA: 54s - loss: 0.33 - ETA: 53s - loss: 0.36 - ETA: 52s - loss: 0.35 - ETA: 50s - loss: 0.35 - ETA: 48s - loss: 0.34 - ETA: 47s - loss: 0.34 - ETA: 45s - loss: 0.34 - ETA: 43s - loss: 0.34 - ETA: 42s - loss: 0.34 - ETA: 40s - loss: 0.34 - ETA: 38s - loss: 0.34 - ETA: 37s - loss: 0.35 - ETA: 35s - loss: 0.35 - ETA: 33s - loss: 0.35 - ETA: 32s - loss: 0.35 - ETA: 31s - loss: 0.35 - ETA: 29s - loss: 0.35 - ETA: 27s - loss: 0.35 - ETA: 25s - loss: 0.35 - ETA: 24s - loss: 0.35 - ETA: 22s - loss: 0.35 - ETA: 20s - loss: 0.35 - ETA: 19s - loss: 0.35 - ETA: 17s - loss: 0.35 - ETA: 15s - loss: 0.35 - ETA: 14s - loss: 0.35 - ETA: 12s - loss: 0.35 - ETA: 10s - loss: 0.35 - ETA: 9s - loss: 0.3557 - ETA: 7s - loss: 0.355 - ETA: 5s - loss: 0.355 - ETA: 4s - loss: 0.355 - ETA: 2s - loss: 0.355 - ETA: 0s - loss: 0.356 - 60s 3ms/step - loss: 0.3564 - val_loss: 0.5255\n",
      "Epoch 55/75\n",
      "17621/17621 [==============================] - ETA: 56s - loss: 0.36 - ETA: 53s - loss: 0.36 - ETA: 52s - loss: 0.35 - ETA: 50s - loss: 0.34 - ETA: 49s - loss: 0.34 - ETA: 47s - loss: 0.34 - ETA: 46s - loss: 0.34 - ETA: 44s - loss: 0.34 - ETA: 43s - loss: 0.34 - ETA: 43s - loss: 0.33 - ETA: 42s - loss: 0.34 - ETA: 40s - loss: 0.34 - ETA: 39s - loss: 0.33 - ETA: 38s - loss: 0.34 - ETA: 36s - loss: 0.34 - ETA: 35s - loss: 0.34 - ETA: 33s - loss: 0.34 - ETA: 31s - loss: 0.34 - ETA: 29s - loss: 0.34 - ETA: 27s - loss: 0.34 - ETA: 25s - loss: 0.34 - ETA: 23s - loss: 0.34 - ETA: 21s - loss: 0.34 - ETA: 20s - loss: 0.34 - ETA: 18s - loss: 0.34 - ETA: 16s - loss: 0.34 - ETA: 14s - loss: 0.34 - ETA: 12s - loss: 0.34 - ETA: 10s - loss: 0.34 - ETA: 8s - loss: 0.3482 - ETA: 6s - loss: 0.349 - ETA: 4s - loss: 0.348 - ETA: 2s - loss: 0.347 - ETA: 0s - loss: 0.348 - 71s 4ms/step - loss: 0.3496 - val_loss: 0.5202\n",
      "Epoch 56/75\n",
      "17621/17621 [==============================] - ETA: 1:07 - loss: 0.342 - ETA: 1:04 - loss: 0.344 - ETA: 58s - loss: 0.351 - ETA: 55s - loss: 0.35 - ETA: 53s - loss: 0.34 - ETA: 51s - loss: 0.33 - ETA: 48s - loss: 0.33 - ETA: 47s - loss: 0.33 - ETA: 45s - loss: 0.33 - ETA: 43s - loss: 0.33 - ETA: 42s - loss: 0.33 - ETA: 40s - loss: 0.33 - ETA: 38s - loss: 0.33 - ETA: 36s - loss: 0.33 - ETA: 34s - loss: 0.33 - ETA: 32s - loss: 0.33 - ETA: 30s - loss: 0.33 - ETA: 28s - loss: 0.33 - ETA: 26s - loss: 0.33 - ETA: 25s - loss: 0.33 - ETA: 23s - loss: 0.33 - ETA: 21s - loss: 0.33 - ETA: 19s - loss: 0.33 - ETA: 18s - loss: 0.33 - ETA: 16s - loss: 0.33 - ETA: 14s - loss: 0.33 - ETA: 12s - loss: 0.33 - ETA: 11s - loss: 0.32 - ETA: 9s - loss: 0.3300 - ETA: 7s - loss: 0.331 - ETA: 5s - loss: 0.331 - ETA: 4s - loss: 0.332 - ETA: 2s - loss: 0.332 - ETA: 0s - loss: 0.332 - 65s 4ms/step - loss: 0.3328 - val_loss: 0.5382\n",
      "Epoch 57/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17621/17621 [==============================] - ETA: 1:00 - loss: 0.340 - ETA: 1:05 - loss: 0.318 - ETA: 1:04 - loss: 0.319 - ETA: 1:04 - loss: 0.329 - ETA: 1:03 - loss: 0.332 - ETA: 1:00 - loss: 0.342 - ETA: 57s - loss: 0.338 - ETA: 55s - loss: 0.33 - ETA: 55s - loss: 0.33 - ETA: 53s - loss: 0.32 - ETA: 51s - loss: 0.32 - ETA: 49s - loss: 0.32 - ETA: 47s - loss: 0.33 - ETA: 44s - loss: 0.32 - ETA: 43s - loss: 0.32 - ETA: 40s - loss: 0.33 - ETA: 39s - loss: 0.33 - ETA: 36s - loss: 0.33 - ETA: 34s - loss: 0.33 - ETA: 31s - loss: 0.33 - ETA: 29s - loss: 0.33 - ETA: 26s - loss: 0.33 - ETA: 24s - loss: 0.33 - ETA: 22s - loss: 0.33 - ETA: 20s - loss: 0.33 - ETA: 17s - loss: 0.33 - ETA: 15s - loss: 0.33 - ETA: 13s - loss: 0.33 - ETA: 11s - loss: 0.33 - ETA: 9s - loss: 0.3381 - ETA: 7s - loss: 0.337 - ETA: 5s - loss: 0.337 - ETA: 2s - loss: 0.337 - ETA: 0s - loss: 0.336 - 77s 4ms/step - loss: 0.3372 - val_loss: 0.5339\n",
      "Epoch 58/75\n",
      "17621/17621 [==============================] - ETA: 1:31 - loss: 0.321 - ETA: 1:22 - loss: 0.330 - ETA: 1:22 - loss: 0.328 - ETA: 1:14 - loss: 0.323 - ETA: 1:07 - loss: 0.325 - ETA: 1:02 - loss: 0.336 - ETA: 58s - loss: 0.332 - ETA: 54s - loss: 0.33 - ETA: 51s - loss: 0.33 - ETA: 48s - loss: 0.33 - ETA: 46s - loss: 0.33 - ETA: 43s - loss: 0.33 - ETA: 41s - loss: 0.33 - ETA: 38s - loss: 0.33 - ETA: 36s - loss: 0.33 - ETA: 34s - loss: 0.33 - ETA: 32s - loss: 0.33 - ETA: 30s - loss: 0.33 - ETA: 28s - loss: 0.33 - ETA: 26s - loss: 0.33 - ETA: 24s - loss: 0.32 - ETA: 22s - loss: 0.33 - ETA: 20s - loss: 0.32 - ETA: 18s - loss: 0.32 - ETA: 16s - loss: 0.32 - ETA: 15s - loss: 0.32 - ETA: 13s - loss: 0.32 - ETA: 11s - loss: 0.32 - ETA: 9s - loss: 0.3289 - ETA: 7s - loss: 0.328 - ETA: 6s - loss: 0.328 - ETA: 4s - loss: 0.327 - ETA: 2s - loss: 0.327 - ETA: 0s - loss: 0.328 - 63s 4ms/step - loss: 0.3275 - val_loss: 0.5474\n",
      "Epoch 59/75\n",
      "17621/17621 [==============================] - ETA: 1:02 - loss: 0.297 - ETA: 1:00 - loss: 0.307 - ETA: 58s - loss: 0.317 - ETA: 57s - loss: 0.31 - ETA: 56s - loss: 0.31 - ETA: 52s - loss: 0.32 - ETA: 50s - loss: 0.32 - ETA: 47s - loss: 0.32 - ETA: 45s - loss: 0.32 - ETA: 43s - loss: 0.32 - ETA: 41s - loss: 0.32 - ETA: 39s - loss: 0.32 - ETA: 38s - loss: 0.31 - ETA: 36s - loss: 0.32 - ETA: 34s - loss: 0.31 - ETA: 32s - loss: 0.31 - ETA: 31s - loss: 0.31 - ETA: 29s - loss: 0.31 - ETA: 27s - loss: 0.31 - ETA: 25s - loss: 0.31 - ETA: 23s - loss: 0.31 - ETA: 21s - loss: 0.31 - ETA: 19s - loss: 0.31 - ETA: 18s - loss: 0.31 - ETA: 16s - loss: 0.31 - ETA: 14s - loss: 0.32 - ETA: 12s - loss: 0.32 - ETA: 11s - loss: 0.32 - ETA: 9s - loss: 0.3238 - ETA: 7s - loss: 0.325 - ETA: 5s - loss: 0.324 - ETA: 4s - loss: 0.324 - ETA: 2s - loss: 0.323 - ETA: 0s - loss: 0.323 - 63s 4ms/step - loss: 0.3241 - val_loss: 0.5300\n",
      "Epoch 60/75\n",
      "17621/17621 [==============================] - ETA: 1:08 - loss: 0.286 - ETA: 1:00 - loss: 0.319 - ETA: 56s - loss: 0.323 - ETA: 54s - loss: 0.32 - ETA: 56s - loss: 0.32 - ETA: 54s - loss: 0.32 - ETA: 53s - loss: 0.32 - ETA: 53s - loss: 0.33 - ETA: 51s - loss: 0.32 - ETA: 49s - loss: 0.32 - ETA: 46s - loss: 0.32 - ETA: 44s - loss: 0.33 - ETA: 41s - loss: 0.33 - ETA: 39s - loss: 0.32 - ETA: 37s - loss: 0.32 - ETA: 34s - loss: 0.32 - ETA: 32s - loss: 0.32 - ETA: 30s - loss: 0.32 - ETA: 28s - loss: 0.32 - ETA: 26s - loss: 0.32 - ETA: 24s - loss: 0.32 - ETA: 22s - loss: 0.31 - ETA: 20s - loss: 0.31 - ETA: 18s - loss: 0.31 - ETA: 16s - loss: 0.32 - ETA: 15s - loss: 0.32 - ETA: 13s - loss: 0.32 - ETA: 11s - loss: 0.32 - ETA: 9s - loss: 0.3230 - ETA: 7s - loss: 0.322 - ETA: 6s - loss: 0.325 - ETA: 4s - loss: 0.323 - ETA: 2s - loss: 0.324 - ETA: 0s - loss: 0.326 - 66s 4ms/step - loss: 0.3266 - val_loss: 0.5419\n",
      "Epoch 61/75\n",
      "17621/17621 [==============================] - ETA: 1:05 - loss: 0.308 - ETA: 1:08 - loss: 0.304 - ETA: 1:10 - loss: 0.320 - ETA: 1:06 - loss: 0.325 - ETA: 1:03 - loss: 0.331 - ETA: 59s - loss: 0.326 - ETA: 55s - loss: 0.32 - ETA: 53s - loss: 0.32 - ETA: 50s - loss: 0.32 - ETA: 48s - loss: 0.32 - ETA: 45s - loss: 0.32 - ETA: 43s - loss: 0.32 - ETA: 41s - loss: 0.32 - ETA: 39s - loss: 0.32 - ETA: 37s - loss: 0.33 - ETA: 35s - loss: 0.32 - ETA: 33s - loss: 0.33 - ETA: 31s - loss: 0.32 - ETA: 29s - loss: 0.33 - ETA: 27s - loss: 0.33 - ETA: 25s - loss: 0.33 - ETA: 23s - loss: 0.32 - ETA: 22s - loss: 0.32 - ETA: 20s - loss: 0.32 - ETA: 18s - loss: 0.32 - ETA: 16s - loss: 0.32 - ETA: 14s - loss: 0.32 - ETA: 12s - loss: 0.32 - ETA: 10s - loss: 0.32 - ETA: 8s - loss: 0.3256 - ETA: 6s - loss: 0.326 - ETA: 4s - loss: 0.325 - ETA: 2s - loss: 0.325 - ETA: 0s - loss: 0.324 - 71s 4ms/step - loss: 0.3241 - val_loss: 0.5494\n",
      "Epoch 62/75\n",
      "17621/17621 [==============================] - ETA: 1:06 - loss: 0.321 - ETA: 1:02 - loss: 0.313 - ETA: 1:00 - loss: 0.335 - ETA: 58s - loss: 0.328 - ETA: 55s - loss: 0.32 - ETA: 52s - loss: 0.32 - ETA: 50s - loss: 0.32 - ETA: 49s - loss: 0.32 - ETA: 47s - loss: 0.32 - ETA: 45s - loss: 0.32 - ETA: 43s - loss: 0.32 - ETA: 41s - loss: 0.32 - ETA: 39s - loss: 0.32 - ETA: 37s - loss: 0.32 - ETA: 35s - loss: 0.32 - ETA: 33s - loss: 0.31 - ETA: 32s - loss: 0.31 - ETA: 30s - loss: 0.31 - ETA: 28s - loss: 0.31 - ETA: 26s - loss: 0.31 - ETA: 25s - loss: 0.31 - ETA: 23s - loss: 0.31 - ETA: 21s - loss: 0.31 - ETA: 19s - loss: 0.31 - ETA: 17s - loss: 0.32 - ETA: 15s - loss: 0.32 - ETA: 13s - loss: 0.32 - ETA: 12s - loss: 0.32 - ETA: 10s - loss: 0.32 - ETA: 8s - loss: 0.3230 - ETA: 6s - loss: 0.322 - ETA: 4s - loss: 0.322 - ETA: 2s - loss: 0.319 - ETA: 0s - loss: 0.320 - 66s 4ms/step - loss: 0.3203 - val_loss: 0.5374\n",
      "Epoch 63/75\n",
      "17621/17621 [==============================] - ETA: 53s - loss: 0.36 - ETA: 52s - loss: 0.31 - ETA: 51s - loss: 0.32 - ETA: 50s - loss: 0.32 - ETA: 50s - loss: 0.32 - ETA: 48s - loss: 0.32 - ETA: 46s - loss: 0.32 - ETA: 44s - loss: 0.32 - ETA: 42s - loss: 0.31 - ETA: 41s - loss: 0.32 - ETA: 39s - loss: 0.31 - ETA: 37s - loss: 0.31 - ETA: 35s - loss: 0.31 - ETA: 34s - loss: 0.31 - ETA: 32s - loss: 0.31 - ETA: 30s - loss: 0.31 - ETA: 29s - loss: 0.31 - ETA: 27s - loss: 0.31 - ETA: 26s - loss: 0.31 - ETA: 24s - loss: 0.31 - ETA: 23s - loss: 0.31 - ETA: 21s - loss: 0.31 - ETA: 19s - loss: 0.31 - ETA: 18s - loss: 0.31 - ETA: 16s - loss: 0.31 - ETA: 14s - loss: 0.31 - ETA: 12s - loss: 0.31 - ETA: 11s - loss: 0.31 - ETA: 9s - loss: 0.3150 - ETA: 7s - loss: 0.317 - ETA: 5s - loss: 0.318 - ETA: 4s - loss: 0.316 - ETA: 2s - loss: 0.315 - ETA: 0s - loss: 0.315 - 62s 4ms/step - loss: 0.3166 - val_loss: 0.5415\n",
      "Epoch 64/75\n",
      "17621/17621 [==============================] - ETA: 1:00 - loss: 0.295 - ETA: 59s - loss: 0.291 - ETA: 56s - loss: 0.30 - ETA: 54s - loss: 0.31 - ETA: 52s - loss: 0.31 - ETA: 50s - loss: 0.30 - ETA: 48s - loss: 0.30 - ETA: 46s - loss: 0.30 - ETA: 44s - loss: 0.31 - ETA: 42s - loss: 0.30 - ETA: 40s - loss: 0.30 - ETA: 38s - loss: 0.31 - ETA: 36s - loss: 0.31 - ETA: 35s - loss: 0.30 - ETA: 33s - loss: 0.31 - ETA: 31s - loss: 0.31 - ETA: 29s - loss: 0.31 - ETA: 27s - loss: 0.31 - ETA: 26s - loss: 0.31 - ETA: 24s - loss: 0.31 - ETA: 22s - loss: 0.30 - ETA: 21s - loss: 0.30 - ETA: 19s - loss: 0.30 - ETA: 17s - loss: 0.30 - ETA: 16s - loss: 0.30 - ETA: 14s - loss: 0.30 - ETA: 12s - loss: 0.30 - ETA: 10s - loss: 0.30 - ETA: 9s - loss: 0.3053 - ETA: 7s - loss: 0.305 - ETA: 5s - loss: 0.307 - ETA: 4s - loss: 0.308 - ETA: 2s - loss: 0.308 - ETA: 0s - loss: 0.307 - 62s 3ms/step - loss: 0.3084 - val_loss: 0.5396\n",
      "Epoch 65/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17621/17621 [==============================] - ETA: 53s - loss: 0.28 - ETA: 52s - loss: 0.30 - ETA: 51s - loss: 0.29 - ETA: 49s - loss: 0.31 - ETA: 48s - loss: 0.30 - ETA: 46s - loss: 0.30 - ETA: 45s - loss: 0.30 - ETA: 43s - loss: 0.30 - ETA: 41s - loss: 0.30 - ETA: 40s - loss: 0.30 - ETA: 38s - loss: 0.30 - ETA: 36s - loss: 0.30 - ETA: 35s - loss: 0.30 - ETA: 33s - loss: 0.30 - ETA: 31s - loss: 0.30 - ETA: 30s - loss: 0.30 - ETA: 28s - loss: 0.30 - ETA: 26s - loss: 0.30 - ETA: 25s - loss: 0.31 - ETA: 23s - loss: 0.30 - ETA: 22s - loss: 0.30 - ETA: 20s - loss: 0.30 - ETA: 18s - loss: 0.30 - ETA: 17s - loss: 0.30 - ETA: 15s - loss: 0.30 - ETA: 13s - loss: 0.30 - ETA: 12s - loss: 0.30 - ETA: 10s - loss: 0.30 - ETA: 8s - loss: 0.3095 - ETA: 7s - loss: 0.308 - ETA: 5s - loss: 0.309 - ETA: 4s - loss: 0.310 - ETA: 2s - loss: 0.310 - ETA: 0s - loss: 0.310 - 61s 3ms/step - loss: 0.3100 - val_loss: 0.5379\n",
      "Epoch 66/75\n",
      "17621/17621 [==============================] - ETA: 1:07 - loss: 0.284 - ETA: 1:02 - loss: 0.286 - ETA: 58s - loss: 0.303 - ETA: 54s - loss: 0.30 - ETA: 52s - loss: 0.30 - ETA: 49s - loss: 0.30 - ETA: 47s - loss: 0.30 - ETA: 45s - loss: 0.29 - ETA: 43s - loss: 0.30 - ETA: 43s - loss: 0.30 - ETA: 43s - loss: 0.30 - ETA: 42s - loss: 0.30 - ETA: 40s - loss: 0.30 - ETA: 39s - loss: 0.30 - ETA: 37s - loss: 0.30 - ETA: 36s - loss: 0.30 - ETA: 34s - loss: 0.30 - ETA: 32s - loss: 0.30 - ETA: 30s - loss: 0.30 - ETA: 28s - loss: 0.30 - ETA: 26s - loss: 0.30 - ETA: 24s - loss: 0.30 - ETA: 22s - loss: 0.30 - ETA: 20s - loss: 0.30 - ETA: 18s - loss: 0.30 - ETA: 16s - loss: 0.30 - ETA: 14s - loss: 0.30 - ETA: 12s - loss: 0.30 - ETA: 10s - loss: 0.30 - ETA: 8s - loss: 0.3067 - ETA: 6s - loss: 0.306 - ETA: 4s - loss: 0.306 - ETA: 2s - loss: 0.306 - ETA: 0s - loss: 0.305 - 71s 4ms/step - loss: 0.3049 - val_loss: 0.5475\n",
      "Epoch 67/75\n",
      "17621/17621 [==============================] - ETA: 1:00 - loss: 0.264 - ETA: 1:01 - loss: 0.296 - ETA: 58s - loss: 0.292 - ETA: 56s - loss: 0.28 - ETA: 56s - loss: 0.28 - ETA: 54s - loss: 0.28 - ETA: 53s - loss: 0.28 - ETA: 51s - loss: 0.28 - ETA: 50s - loss: 0.28 - ETA: 49s - loss: 0.28 - ETA: 47s - loss: 0.28 - ETA: 45s - loss: 0.28 - ETA: 42s - loss: 0.29 - ETA: 40s - loss: 0.29 - ETA: 38s - loss: 0.29 - ETA: 36s - loss: 0.29 - ETA: 34s - loss: 0.29 - ETA: 31s - loss: 0.29 - ETA: 29s - loss: 0.29 - ETA: 27s - loss: 0.29 - ETA: 25s - loss: 0.30 - ETA: 23s - loss: 0.30 - ETA: 21s - loss: 0.30 - ETA: 19s - loss: 0.30 - ETA: 17s - loss: 0.30 - ETA: 15s - loss: 0.30 - ETA: 13s - loss: 0.30 - ETA: 11s - loss: 0.30 - ETA: 10s - loss: 0.30 - ETA: 8s - loss: 0.3016 - ETA: 6s - loss: 0.300 - ETA: 4s - loss: 0.301 - ETA: 2s - loss: 0.302 - ETA: 0s - loss: 0.301 - 68s 4ms/step - loss: 0.3013 - val_loss: 0.5392\n",
      "Epoch 68/75\n",
      "17621/17621 [==============================] - ETA: 57s - loss: 0.28 - ETA: 54s - loss: 0.26 - ETA: 52s - loss: 0.28 - ETA: 53s - loss: 0.28 - ETA: 52s - loss: 0.28 - ETA: 53s - loss: 0.29 - ETA: 53s - loss: 0.29 - ETA: 51s - loss: 0.29 - ETA: 49s - loss: 0.28 - ETA: 46s - loss: 0.29 - ETA: 44s - loss: 0.28 - ETA: 42s - loss: 0.28 - ETA: 40s - loss: 0.29 - ETA: 38s - loss: 0.29 - ETA: 37s - loss: 0.28 - ETA: 35s - loss: 0.29 - ETA: 33s - loss: 0.29 - ETA: 31s - loss: 0.29 - ETA: 30s - loss: 0.29 - ETA: 28s - loss: 0.29 - ETA: 26s - loss: 0.29 - ETA: 24s - loss: 0.29 - ETA: 22s - loss: 0.29 - ETA: 21s - loss: 0.29 - ETA: 19s - loss: 0.29 - ETA: 17s - loss: 0.29 - ETA: 15s - loss: 0.29 - ETA: 13s - loss: 0.29 - ETA: 11s - loss: 0.29 - ETA: 9s - loss: 0.3003 - ETA: 7s - loss: 0.300 - ETA: 5s - loss: 0.300 - ETA: 2s - loss: 0.301 - ETA: 0s - loss: 0.301 - 74s 4ms/step - loss: 0.3020 - val_loss: 0.5271\n",
      "Epoch 69/75\n",
      "17621/17621 [==============================] - ETA: 1:25 - loss: 0.306 - ETA: 1:26 - loss: 0.303 - ETA: 1:24 - loss: 0.292 - ETA: 1:17 - loss: 0.289 - ETA: 1:14 - loss: 0.282 - ETA: 1:09 - loss: 0.286 - ETA: 1:07 - loss: 0.283 - ETA: 1:03 - loss: 0.280 - ETA: 1:01 - loss: 0.282 - ETA: 57s - loss: 0.280 - ETA: 54s - loss: 0.28 - ETA: 50s - loss: 0.28 - ETA: 48s - loss: 0.27 - ETA: 45s - loss: 0.27 - ETA: 42s - loss: 0.28 - ETA: 39s - loss: 0.28 - ETA: 37s - loss: 0.28 - ETA: 35s - loss: 0.28 - ETA: 32s - loss: 0.28 - ETA: 30s - loss: 0.28 - ETA: 27s - loss: 0.28 - ETA: 25s - loss: 0.28 - ETA: 23s - loss: 0.28 - ETA: 21s - loss: 0.28 - ETA: 19s - loss: 0.28 - ETA: 17s - loss: 0.28 - ETA: 15s - loss: 0.28 - ETA: 13s - loss: 0.28 - ETA: 11s - loss: 0.29 - ETA: 9s - loss: 0.2920 - ETA: 6s - loss: 0.292 - ETA: 4s - loss: 0.293 - ETA: 2s - loss: 0.294 - ETA: 0s - loss: 0.295 - 72s 4ms/step - loss: 0.2964 - val_loss: 0.5459\n",
      "Epoch 70/75\n",
      "17621/17621 [==============================] - ETA: 53s - loss: 0.30 - ETA: 51s - loss: 0.30 - ETA: 51s - loss: 0.30 - ETA: 49s - loss: 0.30 - ETA: 48s - loss: 0.29 - ETA: 47s - loss: 0.30 - ETA: 45s - loss: 0.30 - ETA: 44s - loss: 0.30 - ETA: 42s - loss: 0.30 - ETA: 41s - loss: 0.29 - ETA: 39s - loss: 0.29 - ETA: 37s - loss: 0.29 - ETA: 35s - loss: 0.29 - ETA: 34s - loss: 0.29 - ETA: 33s - loss: 0.29 - ETA: 33s - loss: 0.29 - ETA: 32s - loss: 0.29 - ETA: 31s - loss: 0.30 - ETA: 29s - loss: 0.29 - ETA: 27s - loss: 0.29 - ETA: 25s - loss: 0.30 - ETA: 23s - loss: 0.29 - ETA: 21s - loss: 0.30 - ETA: 19s - loss: 0.30 - ETA: 17s - loss: 0.29 - ETA: 15s - loss: 0.30 - ETA: 14s - loss: 0.29 - ETA: 12s - loss: 0.30 - ETA: 10s - loss: 0.29 - ETA: 8s - loss: 0.2975 - ETA: 6s - loss: 0.298 - ETA: 4s - loss: 0.297 - ETA: 2s - loss: 0.297 - ETA: 0s - loss: 0.298 - 67s 4ms/step - loss: 0.2980 - val_loss: 0.5519\n",
      "Epoch 71/75\n",
      "17621/17621 [==============================] - ETA: 53s - loss: 0.36 - ETA: 52s - loss: 0.34 - ETA: 50s - loss: 0.33 - ETA: 49s - loss: 0.31 - ETA: 47s - loss: 0.30 - ETA: 45s - loss: 0.30 - ETA: 44s - loss: 0.30 - ETA: 42s - loss: 0.30 - ETA: 41s - loss: 0.30 - ETA: 39s - loss: 0.30 - ETA: 37s - loss: 0.30 - ETA: 36s - loss: 0.30 - ETA: 34s - loss: 0.30 - ETA: 33s - loss: 0.30 - ETA: 31s - loss: 0.30 - ETA: 29s - loss: 0.30 - ETA: 28s - loss: 0.30 - ETA: 27s - loss: 0.30 - ETA: 25s - loss: 0.30 - ETA: 23s - loss: 0.30 - ETA: 22s - loss: 0.30 - ETA: 20s - loss: 0.30 - ETA: 18s - loss: 0.29 - ETA: 17s - loss: 0.29 - ETA: 15s - loss: 0.29 - ETA: 13s - loss: 0.29 - ETA: 12s - loss: 0.29 - ETA: 10s - loss: 0.29 - ETA: 8s - loss: 0.2939 - ETA: 7s - loss: 0.292 - ETA: 5s - loss: 0.293 - ETA: 3s - loss: 0.293 - ETA: 2s - loss: 0.293 - ETA: 0s - loss: 0.292 - 59s 3ms/step - loss: 0.2929 - val_loss: 0.5634\n",
      "Epoch 72/75\n",
      "17621/17621 [==============================] - ETA: 51s - loss: 0.26 - ETA: 51s - loss: 0.28 - ETA: 49s - loss: 0.28 - ETA: 48s - loss: 0.28 - ETA: 47s - loss: 0.28 - ETA: 45s - loss: 0.29 - ETA: 44s - loss: 0.29 - ETA: 42s - loss: 0.30 - ETA: 41s - loss: 0.30 - ETA: 39s - loss: 0.29 - ETA: 37s - loss: 0.30 - ETA: 36s - loss: 0.30 - ETA: 34s - loss: 0.30 - ETA: 33s - loss: 0.29 - ETA: 31s - loss: 0.30 - ETA: 29s - loss: 0.29 - ETA: 28s - loss: 0.30 - ETA: 26s - loss: 0.29 - ETA: 25s - loss: 0.29 - ETA: 23s - loss: 0.29 - ETA: 21s - loss: 0.29 - ETA: 20s - loss: 0.29 - ETA: 18s - loss: 0.29 - ETA: 16s - loss: 0.29 - ETA: 15s - loss: 0.29 - ETA: 13s - loss: 0.29 - ETA: 12s - loss: 0.29 - ETA: 10s - loss: 0.29 - ETA: 8s - loss: 0.2937 - ETA: 7s - loss: 0.292 - ETA: 5s - loss: 0.291 - ETA: 3s - loss: 0.290 - ETA: 2s - loss: 0.290 - ETA: 0s - loss: 0.290 - 58s 3ms/step - loss: 0.2905 - val_loss: 0.5753\n",
      "Epoch 73/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17621/17621 [==============================] - ETA: 52s - loss: 0.26 - ETA: 51s - loss: 0.30 - ETA: 49s - loss: 0.32 - ETA: 48s - loss: 0.31 - ETA: 47s - loss: 0.29 - ETA: 45s - loss: 0.29 - ETA: 44s - loss: 0.29 - ETA: 42s - loss: 0.29 - ETA: 41s - loss: 0.29 - ETA: 40s - loss: 0.29 - ETA: 38s - loss: 0.29 - ETA: 37s - loss: 0.29 - ETA: 35s - loss: 0.29 - ETA: 33s - loss: 0.29 - ETA: 31s - loss: 0.29 - ETA: 30s - loss: 0.29 - ETA: 28s - loss: 0.29 - ETA: 26s - loss: 0.29 - ETA: 25s - loss: 0.29 - ETA: 23s - loss: 0.29 - ETA: 22s - loss: 0.29 - ETA: 20s - loss: 0.29 - ETA: 19s - loss: 0.29 - ETA: 17s - loss: 0.29 - ETA: 15s - loss: 0.29 - ETA: 14s - loss: 0.29 - ETA: 12s - loss: 0.29 - ETA: 10s - loss: 0.29 - ETA: 9s - loss: 0.2982 - ETA: 7s - loss: 0.299 - ETA: 5s - loss: 0.298 - ETA: 4s - loss: 0.297 - ETA: 2s - loss: 0.297 - ETA: 0s - loss: 0.297 - 61s 3ms/step - loss: 0.2984 - val_loss: 0.5484\n",
      "Epoch 74/75\n",
      "17621/17621 [==============================] - ETA: 54s - loss: 0.27 - ETA: 52s - loss: 0.29 - ETA: 51s - loss: 0.28 - ETA: 49s - loss: 0.28 - ETA: 49s - loss: 0.27 - ETA: 47s - loss: 0.27 - ETA: 46s - loss: 0.27 - ETA: 44s - loss: 0.27 - ETA: 42s - loss: 0.27 - ETA: 41s - loss: 0.27 - ETA: 39s - loss: 0.26 - ETA: 37s - loss: 0.27 - ETA: 36s - loss: 0.27 - ETA: 34s - loss: 0.27 - ETA: 32s - loss: 0.28 - ETA: 31s - loss: 0.27 - ETA: 29s - loss: 0.27 - ETA: 27s - loss: 0.27 - ETA: 25s - loss: 0.28 - ETA: 24s - loss: 0.28 - ETA: 22s - loss: 0.28 - ETA: 21s - loss: 0.28 - ETA: 19s - loss: 0.28 - ETA: 17s - loss: 0.28 - ETA: 15s - loss: 0.28 - ETA: 14s - loss: 0.28 - ETA: 12s - loss: 0.28 - ETA: 10s - loss: 0.28 - ETA: 9s - loss: 0.2839 - ETA: 7s - loss: 0.285 - ETA: 5s - loss: 0.284 - ETA: 4s - loss: 0.284 - ETA: 2s - loss: 0.285 - ETA: 0s - loss: 0.284 - 61s 3ms/step - loss: 0.2836 - val_loss: 0.5497\n",
      "Epoch 75/75\n",
      "17621/17621 [==============================] - ETA: 59s - loss: 0.26 - ETA: 55s - loss: 0.26 - ETA: 53s - loss: 0.26 - ETA: 51s - loss: 0.27 - ETA: 49s - loss: 0.26 - ETA: 47s - loss: 0.27 - ETA: 45s - loss: 0.27 - ETA: 44s - loss: 0.27 - ETA: 42s - loss: 0.27 - ETA: 40s - loss: 0.27 - ETA: 39s - loss: 0.27 - ETA: 37s - loss: 0.26 - ETA: 35s - loss: 0.27 - ETA: 33s - loss: 0.28 - ETA: 32s - loss: 0.28 - ETA: 30s - loss: 0.27 - ETA: 28s - loss: 0.27 - ETA: 27s - loss: 0.27 - ETA: 25s - loss: 0.27 - ETA: 23s - loss: 0.27 - ETA: 22s - loss: 0.27 - ETA: 20s - loss: 0.27 - ETA: 19s - loss: 0.27 - ETA: 17s - loss: 0.28 - ETA: 15s - loss: 0.27 - ETA: 14s - loss: 0.27 - ETA: 12s - loss: 0.27 - ETA: 10s - loss: 0.27 - ETA: 9s - loss: 0.2782 - ETA: 7s - loss: 0.278 - ETA: 5s - loss: 0.278 - ETA: 4s - loss: 0.278 - ETA: 2s - loss: 0.278 - ETA: 0s - loss: 0.278 - 60s 3ms/step - loss: 0.2789 - val_loss: 0.5556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xd75d56a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=75, verbose=1, validation_data=(xvalid_pad, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/75\n",
      "17621/17621 [==============================] - ETA: 3:25 - loss: 1.153 - ETA: 2:44 - loss: 1.148 - ETA: 2:28 - loss: 1.150 - ETA: 2:19 - loss: 1.146 - ETA: 2:14 - loss: 1.143 - ETA: 2:09 - loss: 1.143 - ETA: 2:03 - loss: 1.140 - ETA: 1:59 - loss: 1.138 - ETA: 1:53 - loss: 1.138 - ETA: 1:49 - loss: 1.136 - ETA: 1:45 - loss: 1.134 - ETA: 1:40 - loss: 1.131 - ETA: 1:36 - loss: 1.126 - ETA: 1:32 - loss: 1.124 - ETA: 1:27 - loss: 1.123 - ETA: 1:22 - loss: 1.121 - ETA: 1:18 - loss: 1.120 - ETA: 1:13 - loss: 1.118 - ETA: 1:09 - loss: 1.117 - ETA: 1:04 - loss: 1.116 - ETA: 1:00 - loss: 1.114 - ETA: 56s - loss: 1.112 - ETA: 51s - loss: 1.11 - ETA: 47s - loss: 1.10 - ETA: 42s - loss: 1.10 - ETA: 38s - loss: 1.10 - ETA: 33s - loss: 1.10 - ETA: 29s - loss: 1.10 - ETA: 24s - loss: 1.10 - ETA: 20s - loss: 1.10 - ETA: 15s - loss: 1.09 - ETA: 10s - loss: 1.09 - ETA: 6s - loss: 1.0962 - ETA: 1s - loss: 1.094 - 164s 9ms/step - loss: 1.0941 - val_loss: 1.0045\n",
      "Epoch 2/75\n",
      "17621/17621 [==============================] - ETA: 2:29 - loss: 1.045 - ETA: 2:26 - loss: 1.033 - ETA: 2:22 - loss: 1.025 - ETA: 2:18 - loss: 1.024 - ETA: 2:13 - loss: 1.021 - ETA: 2:08 - loss: 1.020 - ETA: 2:04 - loss: 1.018 - ETA: 2:00 - loss: 1.011 - ETA: 1:55 - loss: 1.009 - ETA: 1:50 - loss: 1.005 - ETA: 1:46 - loss: 0.999 - ETA: 1:41 - loss: 0.998 - ETA: 1:37 - loss: 1.000 - ETA: 1:33 - loss: 0.999 - ETA: 1:28 - loss: 0.991 - ETA: 1:24 - loss: 0.990 - ETA: 1:19 - loss: 0.990 - ETA: 1:15 - loss: 0.988 - ETA: 1:10 - loss: 0.988 - ETA: 1:06 - loss: 0.986 - ETA: 1:01 - loss: 0.984 - ETA: 56s - loss: 0.982 - ETA: 52s - loss: 0.98 - ETA: 47s - loss: 0.97 - ETA: 43s - loss: 0.97 - ETA: 38s - loss: 0.97 - ETA: 34s - loss: 0.97 - ETA: 29s - loss: 0.97 - ETA: 24s - loss: 0.97 - ETA: 20s - loss: 0.97 - ETA: 15s - loss: 0.96 - ETA: 11s - loss: 0.96 - ETA: 6s - loss: 0.9676 - ETA: 1s - loss: 0.964 - 165s 9ms/step - loss: 0.9634 - val_loss: 0.8797\n",
      "Epoch 3/75\n",
      "17621/17621 [==============================] - ETA: 2:33 - loss: 0.924 - ETA: 2:27 - loss: 0.900 - ETA: 2:23 - loss: 0.901 - ETA: 2:19 - loss: 0.916 - ETA: 2:19 - loss: 0.923 - ETA: 2:14 - loss: 0.918 - ETA: 2:08 - loss: 0.914 - ETA: 2:04 - loss: 0.913 - ETA: 2:00 - loss: 0.905 - ETA: 1:55 - loss: 0.904 - ETA: 1:50 - loss: 0.903 - ETA: 1:45 - loss: 0.906 - ETA: 1:40 - loss: 0.906 - ETA: 1:35 - loss: 0.906 - ETA: 1:31 - loss: 0.903 - ETA: 1:26 - loss: 0.902 - ETA: 1:21 - loss: 0.901 - ETA: 1:17 - loss: 0.904 - ETA: 1:12 - loss: 0.904 - ETA: 1:07 - loss: 0.901 - ETA: 1:03 - loss: 0.899 - ETA: 58s - loss: 0.896 - ETA: 53s - loss: 0.89 - ETA: 48s - loss: 0.89 - ETA: 44s - loss: 0.89 - ETA: 39s - loss: 0.89 - ETA: 35s - loss: 0.89 - ETA: 30s - loss: 0.89 - ETA: 25s - loss: 0.89 - ETA: 21s - loss: 0.89 - ETA: 16s - loss: 0.89 - ETA: 11s - loss: 0.88 - ETA: 6s - loss: 0.8887 - ETA: 1s - loss: 0.889 - 173s 10ms/step - loss: 0.8895 - val_loss: 0.8194\n",
      "Epoch 4/75\n",
      "17621/17621 [==============================] - ETA: 2:51 - loss: 0.840 - ETA: 2:45 - loss: 0.857 - ETA: 2:39 - loss: 0.876 - ETA: 2:32 - loss: 0.865 - ETA: 2:25 - loss: 0.859 - ETA: 2:19 - loss: 0.865 - ETA: 2:14 - loss: 0.859 - ETA: 2:10 - loss: 0.854 - ETA: 2:04 - loss: 0.859 - ETA: 1:59 - loss: 0.860 - ETA: 1:53 - loss: 0.863 - ETA: 1:48 - loss: 0.860 - ETA: 1:43 - loss: 0.859 - ETA: 1:38 - loss: 0.861 - ETA: 1:33 - loss: 0.861 - ETA: 1:28 - loss: 0.857 - ETA: 1:23 - loss: 0.856 - ETA: 1:19 - loss: 0.858 - ETA: 1:14 - loss: 0.858 - ETA: 1:09 - loss: 0.860 - ETA: 1:04 - loss: 0.862 - ETA: 59s - loss: 0.859 - ETA: 55s - loss: 0.85 - ETA: 50s - loss: 0.85 - ETA: 45s - loss: 0.85 - ETA: 40s - loss: 0.85 - ETA: 35s - loss: 0.85 - ETA: 31s - loss: 0.85 - ETA: 26s - loss: 0.85 - ETA: 21s - loss: 0.85 - ETA: 16s - loss: 0.85 - ETA: 11s - loss: 0.85 - ETA: 6s - loss: 0.8558 - ETA: 2s - loss: 0.853 - 172s 10ms/step - loss: 0.8547 - val_loss: 0.7544\n",
      "Epoch 5/75\n",
      "17621/17621 [==============================] - ETA: 2:35 - loss: 0.818 - ETA: 2:30 - loss: 0.825 - ETA: 2:25 - loss: 0.826 - ETA: 2:20 - loss: 0.842 - ETA: 2:15 - loss: 0.845 - ETA: 2:11 - loss: 0.840 - ETA: 2:06 - loss: 0.843 - ETA: 2:02 - loss: 0.841 - ETA: 1:57 - loss: 0.838 - ETA: 1:52 - loss: 0.838 - ETA: 1:48 - loss: 0.838 - ETA: 1:43 - loss: 0.838 - ETA: 1:40 - loss: 0.833 - ETA: 1:35 - loss: 0.832 - ETA: 1:30 - loss: 0.829 - ETA: 1:26 - loss: 0.829 - ETA: 1:21 - loss: 0.827 - ETA: 1:16 - loss: 0.824 - ETA: 1:12 - loss: 0.823 - ETA: 1:07 - loss: 0.821 - ETA: 1:02 - loss: 0.823 - ETA: 57s - loss: 0.823 - ETA: 53s - loss: 0.82 - ETA: 48s - loss: 0.82 - ETA: 44s - loss: 0.82 - ETA: 39s - loss: 0.82 - ETA: 34s - loss: 0.82 - ETA: 30s - loss: 0.82 - ETA: 25s - loss: 0.82 - ETA: 20s - loss: 0.82 - ETA: 16s - loss: 0.82 - ETA: 11s - loss: 0.82 - ETA: 6s - loss: 0.8233 - ETA: 1s - loss: 0.822 - 168s 10ms/step - loss: 0.8216 - val_loss: 0.7406\n",
      "Epoch 6/75\n",
      "17621/17621 [==============================] - ETA: 2:34 - loss: 0.842 - ETA: 2:29 - loss: 0.797 - ETA: 2:25 - loss: 0.808 - ETA: 2:24 - loss: 0.824 - ETA: 2:19 - loss: 0.823 - ETA: 2:14 - loss: 0.809 - ETA: 2:09 - loss: 0.799 - ETA: 2:04 - loss: 0.799 - ETA: 1:59 - loss: 0.804 - ETA: 1:54 - loss: 0.810 - ETA: 1:50 - loss: 0.808 - ETA: 1:45 - loss: 0.802 - ETA: 1:40 - loss: 0.799 - ETA: 1:36 - loss: 0.802 - ETA: 1:31 - loss: 0.800 - ETA: 1:26 - loss: 0.801 - ETA: 1:22 - loss: 0.800 - ETA: 1:17 - loss: 0.800 - ETA: 1:12 - loss: 0.800 - ETA: 1:08 - loss: 0.801 - ETA: 1:03 - loss: 0.801 - ETA: 58s - loss: 0.799 - ETA: 53s - loss: 0.80 - ETA: 49s - loss: 0.80 - ETA: 44s - loss: 0.79 - ETA: 39s - loss: 0.79 - ETA: 35s - loss: 0.79 - ETA: 30s - loss: 0.79 - ETA: 26s - loss: 0.79 - ETA: 21s - loss: 0.79 - ETA: 16s - loss: 0.79 - ETA: 11s - loss: 0.79 - ETA: 6s - loss: 0.7979 - ETA: 2s - loss: 0.794 - 172s 10ms/step - loss: 0.7948 - val_loss: 0.7140\n",
      "Epoch 7/75\n",
      "17621/17621 [==============================] - ETA: 2:37 - loss: 0.800 - ETA: 2:30 - loss: 0.799 - ETA: 2:28 - loss: 0.777 - ETA: 2:23 - loss: 0.787 - ETA: 2:18 - loss: 0.787 - ETA: 2:14 - loss: 0.793 - ETA: 2:15 - loss: 0.790 - ETA: 2:13 - loss: 0.788 - ETA: 2:09 - loss: 0.786 - ETA: 2:03 - loss: 0.785 - ETA: 1:58 - loss: 0.786 - ETA: 1:52 - loss: 0.784 - ETA: 1:46 - loss: 0.778 - ETA: 1:41 - loss: 0.779 - ETA: 1:36 - loss: 0.780 - ETA: 1:31 - loss: 0.779 - ETA: 1:26 - loss: 0.781 - ETA: 1:21 - loss: 0.782 - ETA: 1:16 - loss: 0.780 - ETA: 1:11 - loss: 0.779 - ETA: 1:06 - loss: 0.780 - ETA: 1:01 - loss: 0.781 - ETA: 57s - loss: 0.779 - ETA: 53s - loss: 0.77 - ETA: 48s - loss: 0.77 - ETA: 43s - loss: 0.77 - ETA: 38s - loss: 0.77 - ETA: 33s - loss: 0.77 - ETA: 27s - loss: 0.77 - ETA: 22s - loss: 0.77 - ETA: 17s - loss: 0.77 - ETA: 12s - loss: 0.77 - ETA: 7s - loss: 0.7733 - ETA: 2s - loss: 0.773 - 182s 10ms/step - loss: 0.7731 - val_loss: 0.6861\n",
      "Epoch 8/75\n",
      "17621/17621 [==============================] - ETA: 2:36 - loss: 0.703 - ETA: 2:41 - loss: 0.731 - ETA: 2:34 - loss: 0.733 - ETA: 2:27 - loss: 0.741 - ETA: 2:21 - loss: 0.744 - ETA: 2:16 - loss: 0.743 - ETA: 2:11 - loss: 0.742 - ETA: 2:06 - loss: 0.741 - ETA: 2:02 - loss: 0.753 - ETA: 1:58 - loss: 0.750 - ETA: 1:55 - loss: 0.751 - ETA: 1:49 - loss: 0.750 - ETA: 1:45 - loss: 0.748 - ETA: 1:39 - loss: 0.750 - ETA: 1:34 - loss: 0.748 - ETA: 1:29 - loss: 0.750 - ETA: 1:24 - loss: 0.746 - ETA: 1:19 - loss: 0.749 - ETA: 1:14 - loss: 0.746 - ETA: 1:09 - loss: 0.746 - ETA: 1:04 - loss: 0.748 - ETA: 1:00 - loss: 0.747 - ETA: 55s - loss: 0.745 - ETA: 50s - loss: 0.74 - ETA: 45s - loss: 0.74 - ETA: 40s - loss: 0.74 - ETA: 35s - loss: 0.74 - ETA: 30s - loss: 0.74 - ETA: 25s - loss: 0.74 - ETA: 21s - loss: 0.74 - ETA: 16s - loss: 0.74 - ETA: 11s - loss: 0.74 - ETA: 6s - loss: 0.7458 - ETA: 1s - loss: 0.745 - 171s 10ms/step - loss: 0.7452 - val_loss: 0.6827\n",
      "Epoch 9/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17621/17621 [==============================] - ETA: 2:39 - loss: 0.750 - ETA: 2:32 - loss: 0.719 - ETA: 2:27 - loss: 0.709 - ETA: 2:21 - loss: 0.710 - ETA: 2:16 - loss: 0.714 - ETA: 2:12 - loss: 0.711 - ETA: 2:07 - loss: 0.725 - ETA: 2:02 - loss: 0.728 - ETA: 1:58 - loss: 0.733 - ETA: 1:53 - loss: 0.731 - ETA: 1:49 - loss: 0.733 - ETA: 1:44 - loss: 0.728 - ETA: 1:40 - loss: 0.728 - ETA: 1:36 - loss: 0.725 - ETA: 1:31 - loss: 0.725 - ETA: 1:26 - loss: 0.726 - ETA: 1:21 - loss: 0.725 - ETA: 1:17 - loss: 0.725 - ETA: 1:12 - loss: 0.725 - ETA: 1:07 - loss: 0.726 - ETA: 1:02 - loss: 0.728 - ETA: 58s - loss: 0.725 - ETA: 53s - loss: 0.72 - ETA: 48s - loss: 0.72 - ETA: 44s - loss: 0.72 - ETA: 39s - loss: 0.72 - ETA: 34s - loss: 0.72 - ETA: 30s - loss: 0.72 - ETA: 25s - loss: 0.72 - ETA: 20s - loss: 0.72 - ETA: 16s - loss: 0.72 - ETA: 11s - loss: 0.72 - ETA: 6s - loss: 0.7274 - ETA: 1s - loss: 0.727 - 168s 10ms/step - loss: 0.7274 - val_loss: 0.6716\n",
      "Epoch 10/75\n",
      "17621/17621 [==============================] - ETA: 2:35 - loss: 0.690 - ETA: 2:30 - loss: 0.701 - ETA: 2:30 - loss: 0.706 - ETA: 2:24 - loss: 0.712 - ETA: 2:19 - loss: 0.723 - ETA: 2:15 - loss: 0.727 - ETA: 2:10 - loss: 0.720 - ETA: 2:05 - loss: 0.721 - ETA: 2:00 - loss: 0.719 - ETA: 1:55 - loss: 0.715 - ETA: 1:51 - loss: 0.715 - ETA: 1:47 - loss: 0.721 - ETA: 1:42 - loss: 0.729 - ETA: 1:37 - loss: 0.723 - ETA: 1:33 - loss: 0.722 - ETA: 1:28 - loss: 0.718 - ETA: 1:23 - loss: 0.718 - ETA: 1:18 - loss: 0.719 - ETA: 1:13 - loss: 0.721 - ETA: 1:08 - loss: 0.717 - ETA: 1:03 - loss: 0.715 - ETA: 59s - loss: 0.714 - ETA: 54s - loss: 0.71 - ETA: 49s - loss: 0.71 - ETA: 44s - loss: 0.71 - ETA: 39s - loss: 0.71 - ETA: 35s - loss: 0.71 - ETA: 30s - loss: 0.71 - ETA: 25s - loss: 0.71 - ETA: 20s - loss: 0.71 - ETA: 16s - loss: 0.71 - ETA: 11s - loss: 0.71 - ETA: 6s - loss: 0.7131 - ETA: 1s - loss: 0.712 - 169s 10ms/step - loss: 0.7130 - val_loss: 0.6410\n",
      "Epoch 11/75\n",
      "17621/17621 [==============================] - ETA: 2:41 - loss: 0.681 - ETA: 2:33 - loss: 0.712 - ETA: 2:28 - loss: 0.709 - ETA: 2:22 - loss: 0.697 - ETA: 2:17 - loss: 0.694 - ETA: 2:12 - loss: 0.699 - ETA: 2:09 - loss: 0.689 - ETA: 2:04 - loss: 0.690 - ETA: 1:59 - loss: 0.695 - ETA: 1:54 - loss: 0.696 - ETA: 1:49 - loss: 0.693 - ETA: 1:45 - loss: 0.690 - ETA: 1:40 - loss: 0.693 - ETA: 1:35 - loss: 0.694 - ETA: 1:30 - loss: 0.694 - ETA: 1:26 - loss: 0.693 - ETA: 1:21 - loss: 0.696 - ETA: 1:16 - loss: 0.696 - ETA: 1:12 - loss: 0.694 - ETA: 1:07 - loss: 0.694 - ETA: 1:03 - loss: 0.696 - ETA: 58s - loss: 0.693 - ETA: 54s - loss: 0.69 - ETA: 49s - loss: 0.69 - ETA: 44s - loss: 0.69 - ETA: 39s - loss: 0.69 - ETA: 35s - loss: 0.69 - ETA: 30s - loss: 0.69 - ETA: 25s - loss: 0.69 - ETA: 20s - loss: 0.69 - ETA: 16s - loss: 0.69 - ETA: 11s - loss: 0.69 - ETA: 6s - loss: 0.6918 - ETA: 2s - loss: 0.691 - 174s 10ms/step - loss: 0.6917 - val_loss: 0.6281\n",
      "Epoch 12/75\n",
      "17621/17621 [==============================] - ETA: 2:38 - loss: 0.610 - ETA: 2:31 - loss: 0.661 - ETA: 2:27 - loss: 0.670 - ETA: 2:22 - loss: 0.680 - ETA: 2:18 - loss: 0.682 - ETA: 2:13 - loss: 0.672 - ETA: 2:09 - loss: 0.676 - ETA: 2:04 - loss: 0.673 - ETA: 1:59 - loss: 0.674 - ETA: 1:55 - loss: 0.672 - ETA: 1:51 - loss: 0.674 - ETA: 1:46 - loss: 0.675 - ETA: 1:41 - loss: 0.677 - ETA: 1:36 - loss: 0.676 - ETA: 1:31 - loss: 0.676 - ETA: 1:26 - loss: 0.676 - ETA: 1:22 - loss: 0.672 - ETA: 1:17 - loss: 0.672 - ETA: 1:12 - loss: 0.673 - ETA: 1:07 - loss: 0.674 - ETA: 1:03 - loss: 0.671 - ETA: 58s - loss: 0.670 - ETA: 53s - loss: 0.67 - ETA: 49s - loss: 0.67 - ETA: 44s - loss: 0.67 - ETA: 39s - loss: 0.67 - ETA: 34s - loss: 0.66 - ETA: 30s - loss: 0.66 - ETA: 25s - loss: 0.67 - ETA: 20s - loss: 0.67 - ETA: 16s - loss: 0.67 - ETA: 11s - loss: 0.67 - ETA: 6s - loss: 0.6723 - ETA: 1s - loss: 0.673 - 170s 10ms/step - loss: 0.6738 - val_loss: 0.6448\n",
      "Epoch 13/75\n",
      "17621/17621 [==============================] - ETA: 3:04 - loss: 0.630 - ETA: 2:49 - loss: 0.627 - ETA: 2:40 - loss: 0.634 - ETA: 2:34 - loss: 0.639 - ETA: 2:28 - loss: 0.650 - ETA: 2:23 - loss: 0.667 - ETA: 2:19 - loss: 0.670 - ETA: 2:14 - loss: 0.667 - ETA: 2:09 - loss: 0.662 - ETA: 2:05 - loss: 0.665 - ETA: 2:00 - loss: 0.664 - ETA: 1:56 - loss: 0.665 - ETA: 1:52 - loss: 0.662 - ETA: 1:48 - loss: 0.660 - ETA: 1:43 - loss: 0.662 - ETA: 1:37 - loss: 0.662 - ETA: 1:32 - loss: 0.662 - ETA: 1:26 - loss: 0.662 - ETA: 1:21 - loss: 0.663 - ETA: 1:15 - loss: 0.664 - ETA: 1:10 - loss: 0.664 - ETA: 1:05 - loss: 0.664 - ETA: 1:00 - loss: 0.666 - ETA: 54s - loss: 0.668 - ETA: 49s - loss: 0.66 - ETA: 44s - loss: 0.66 - ETA: 39s - loss: 0.66 - ETA: 33s - loss: 0.66 - ETA: 28s - loss: 0.66 - ETA: 23s - loss: 0.66 - ETA: 18s - loss: 0.66 - ETA: 12s - loss: 0.66 - ETA: 7s - loss: 0.6656 - ETA: 2s - loss: 0.663 - 188s 11ms/step - loss: 0.6639 - val_loss: 0.5978\n",
      "Epoch 14/75\n",
      "17621/17621 [==============================] - ETA: 2:54 - loss: 0.618 - ETA: 2:51 - loss: 0.602 - ETA: 2:43 - loss: 0.585 - ETA: 2:37 - loss: 0.596 - ETA: 2:31 - loss: 0.599 - ETA: 2:26 - loss: 0.615 - ETA: 2:20 - loss: 0.615 - ETA: 2:15 - loss: 0.620 - ETA: 2:09 - loss: 0.624 - ETA: 2:04 - loss: 0.621 - ETA: 1:58 - loss: 0.618 - ETA: 1:53 - loss: 0.621 - ETA: 1:48 - loss: 0.625 - ETA: 1:44 - loss: 0.628 - ETA: 1:38 - loss: 0.629 - ETA: 1:33 - loss: 0.627 - ETA: 1:28 - loss: 0.629 - ETA: 1:23 - loss: 0.630 - ETA: 1:18 - loss: 0.629 - ETA: 1:13 - loss: 0.634 - ETA: 1:08 - loss: 0.634 - ETA: 1:03 - loss: 0.635 - ETA: 58s - loss: 0.637 - ETA: 52s - loss: 0.63 - ETA: 47s - loss: 0.63 - ETA: 42s - loss: 0.63 - ETA: 37s - loss: 0.63 - ETA: 32s - loss: 0.63 - ETA: 27s - loss: 0.63 - ETA: 22s - loss: 0.63 - ETA: 17s - loss: 0.63 - ETA: 12s - loss: 0.63 - ETA: 7s - loss: 0.6363 - ETA: 2s - loss: 0.635 - 181s 10ms/step - loss: 0.6351 - val_loss: 0.5855\n",
      "Epoch 15/75\n",
      "17621/17621 [==============================] - ETA: 2:44 - loss: 0.555 - ETA: 2:39 - loss: 0.607 - ETA: 2:39 - loss: 0.608 - ETA: 2:33 - loss: 0.598 - ETA: 2:28 - loss: 0.592 - ETA: 2:22 - loss: 0.586 - ETA: 2:18 - loss: 0.591 - ETA: 2:12 - loss: 0.598 - ETA: 2:07 - loss: 0.603 - ETA: 2:03 - loss: 0.603 - ETA: 1:59 - loss: 0.611 - ETA: 1:54 - loss: 0.614 - ETA: 1:49 - loss: 0.611 - ETA: 1:44 - loss: 0.608 - ETA: 1:38 - loss: 0.608 - ETA: 1:34 - loss: 0.610 - ETA: 1:28 - loss: 0.610 - ETA: 1:23 - loss: 0.610 - ETA: 1:18 - loss: 0.613 - ETA: 1:13 - loss: 0.610 - ETA: 1:08 - loss: 0.612 - ETA: 1:02 - loss: 0.614 - ETA: 57s - loss: 0.614 - ETA: 52s - loss: 0.61 - ETA: 47s - loss: 0.61 - ETA: 42s - loss: 0.61 - ETA: 37s - loss: 0.61 - ETA: 32s - loss: 0.61 - ETA: 27s - loss: 0.62 - ETA: 22s - loss: 0.62 - ETA: 17s - loss: 0.61 - ETA: 12s - loss: 0.61 - ETA: 7s - loss: 0.6182 - ETA: 2s - loss: 0.618 - 180s 10ms/step - loss: 0.6184 - val_loss: 0.5786\n",
      "Epoch 16/75\n",
      "17621/17621 [==============================] - ETA: 2:45 - loss: 0.620 - ETA: 2:39 - loss: 0.631 - ETA: 2:34 - loss: 0.623 - ETA: 2:40 - loss: 0.600 - ETA: 2:35 - loss: 0.596 - ETA: 2:28 - loss: 0.588 - ETA: 2:22 - loss: 0.591 - ETA: 2:16 - loss: 0.598 - ETA: 2:10 - loss: 0.598 - ETA: 2:08 - loss: 0.598 - ETA: 2:05 - loss: 0.600 - ETA: 2:01 - loss: 0.603 - ETA: 1:57 - loss: 0.602 - ETA: 1:53 - loss: 0.602 - ETA: 1:48 - loss: 0.601 - ETA: 1:43 - loss: 0.600 - ETA: 1:37 - loss: 0.599 - ETA: 1:31 - loss: 0.600 - ETA: 1:25 - loss: 0.599 - ETA: 1:19 - loss: 0.596 - ETA: 1:13 - loss: 0.599 - ETA: 1:07 - loss: 0.601 - ETA: 1:01 - loss: 0.602 - ETA: 56s - loss: 0.600 - ETA: 50s - loss: 0.60 - ETA: 44s - loss: 0.60 - ETA: 39s - loss: 0.60 - ETA: 33s - loss: 0.60 - ETA: 28s - loss: 0.60 - ETA: 23s - loss: 0.60 - ETA: 17s - loss: 0.60 - ETA: 12s - loss: 0.60 - ETA: 7s - loss: 0.6063 - ETA: 2s - loss: 0.607 - 185s 10ms/step - loss: 0.6070 - val_loss: 0.6014\n",
      "Epoch 17/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17621/17621 [==============================] - ETA: 2:42 - loss: 0.579 - ETA: 2:36 - loss: 0.581 - ETA: 2:29 - loss: 0.584 - ETA: 2:24 - loss: 0.598 - ETA: 2:19 - loss: 0.602 - ETA: 2:16 - loss: 0.592 - ETA: 2:10 - loss: 0.593 - ETA: 2:08 - loss: 0.592 - ETA: 2:07 - loss: 0.584 - ETA: 2:03 - loss: 0.584 - ETA: 1:58 - loss: 0.584 - ETA: 1:53 - loss: 0.584 - ETA: 1:48 - loss: 0.584 - ETA: 1:42 - loss: 0.584 - ETA: 1:37 - loss: 0.585 - ETA: 1:31 - loss: 0.583 - ETA: 1:26 - loss: 0.583 - ETA: 1:21 - loss: 0.582 - ETA: 1:16 - loss: 0.580 - ETA: 1:11 - loss: 0.581 - ETA: 1:05 - loss: 0.581 - ETA: 1:00 - loss: 0.581 - ETA: 55s - loss: 0.585 - ETA: 50s - loss: 0.58 - ETA: 45s - loss: 0.58 - ETA: 40s - loss: 0.58 - ETA: 36s - loss: 0.58 - ETA: 31s - loss: 0.58 - ETA: 26s - loss: 0.58 - ETA: 21s - loss: 0.59 - ETA: 16s - loss: 0.59 - ETA: 11s - loss: 0.58 - ETA: 6s - loss: 0.5903 - ETA: 2s - loss: 0.591 - 172s 10ms/step - loss: 0.5904 - val_loss: 0.5553\n",
      "Epoch 18/75\n",
      "17621/17621 [==============================] - ETA: 2:35 - loss: 0.537 - ETA: 2:30 - loss: 0.585 - ETA: 2:25 - loss: 0.579 - ETA: 2:21 - loss: 0.575 - ETA: 2:16 - loss: 0.577 - ETA: 2:11 - loss: 0.581 - ETA: 2:06 - loss: 0.568 - ETA: 2:02 - loss: 0.566 - ETA: 1:58 - loss: 0.569 - ETA: 1:54 - loss: 0.569 - ETA: 1:49 - loss: 0.569 - ETA: 1:45 - loss: 0.567 - ETA: 1:40 - loss: 0.566 - ETA: 1:35 - loss: 0.569 - ETA: 1:31 - loss: 0.569 - ETA: 1:26 - loss: 0.568 - ETA: 1:21 - loss: 0.567 - ETA: 1:17 - loss: 0.567 - ETA: 1:12 - loss: 0.565 - ETA: 1:07 - loss: 0.566 - ETA: 1:02 - loss: 0.565 - ETA: 58s - loss: 0.566 - ETA: 53s - loss: 0.56 - ETA: 49s - loss: 0.56 - ETA: 44s - loss: 0.56 - ETA: 39s - loss: 0.56 - ETA: 35s - loss: 0.57 - ETA: 30s - loss: 0.57 - ETA: 25s - loss: 0.57 - ETA: 21s - loss: 0.57 - ETA: 16s - loss: 0.57 - ETA: 11s - loss: 0.57 - ETA: 6s - loss: 0.5748 - ETA: 1s - loss: 0.574 - 170s 10ms/step - loss: 0.5731 - val_loss: 0.5497\n",
      "Epoch 19/75\n",
      "17621/17621 [==============================] - ETA: 2:36 - loss: 0.529 - ETA: 2:32 - loss: 0.546 - ETA: 2:27 - loss: 0.537 - ETA: 2:22 - loss: 0.547 - ETA: 2:17 - loss: 0.552 - ETA: 2:12 - loss: 0.553 - ETA: 2:07 - loss: 0.555 - ETA: 2:03 - loss: 0.553 - ETA: 1:58 - loss: 0.551 - ETA: 1:53 - loss: 0.556 - ETA: 1:48 - loss: 0.558 - ETA: 1:44 - loss: 0.559 - ETA: 1:40 - loss: 0.561 - ETA: 1:35 - loss: 0.559 - ETA: 1:31 - loss: 0.555 - ETA: 1:26 - loss: 0.555 - ETA: 1:21 - loss: 0.554 - ETA: 1:17 - loss: 0.556 - ETA: 1:12 - loss: 0.554 - ETA: 1:07 - loss: 0.551 - ETA: 1:02 - loss: 0.550 - ETA: 58s - loss: 0.548 - ETA: 53s - loss: 0.54 - ETA: 48s - loss: 0.54 - ETA: 44s - loss: 0.55 - ETA: 39s - loss: 0.54 - ETA: 34s - loss: 0.55 - ETA: 30s - loss: 0.55 - ETA: 25s - loss: 0.55 - ETA: 20s - loss: 0.55 - ETA: 16s - loss: 0.55 - ETA: 11s - loss: 0.55 - ETA: 6s - loss: 0.5538 - ETA: 1s - loss: 0.553 - 168s 10ms/step - loss: 0.5530 - val_loss: 0.5512\n",
      "Epoch 20/75\n",
      "17621/17621 [==============================] - ETA: 2:34 - loss: 0.532 - ETA: 2:30 - loss: 0.515 - ETA: 2:25 - loss: 0.504 - ETA: 2:24 - loss: 0.520 - ETA: 2:19 - loss: 0.512 - ETA: 2:14 - loss: 0.513 - ETA: 2:09 - loss: 0.514 - ETA: 2:04 - loss: 0.517 - ETA: 1:59 - loss: 0.518 - ETA: 1:54 - loss: 0.525 - ETA: 1:49 - loss: 0.523 - ETA: 1:45 - loss: 0.525 - ETA: 1:40 - loss: 0.530 - ETA: 1:35 - loss: 0.525 - ETA: 1:30 - loss: 0.527 - ETA: 1:26 - loss: 0.531 - ETA: 1:22 - loss: 0.534 - ETA: 1:17 - loss: 0.531 - ETA: 1:12 - loss: 0.530 - ETA: 1:08 - loss: 0.532 - ETA: 1:03 - loss: 0.532 - ETA: 58s - loss: 0.535 - ETA: 54s - loss: 0.53 - ETA: 49s - loss: 0.53 - ETA: 44s - loss: 0.53 - ETA: 39s - loss: 0.53 - ETA: 35s - loss: 0.53 - ETA: 30s - loss: 0.53 - ETA: 25s - loss: 0.53 - ETA: 20s - loss: 0.53 - ETA: 16s - loss: 0.53 - ETA: 11s - loss: 0.53 - ETA: 6s - loss: 0.5385 - ETA: 1s - loss: 0.538 - 170s 10ms/step - loss: 0.5375 - val_loss: 0.5315\n",
      "Epoch 21/75\n",
      "17621/17621 [==============================] - ETA: 2:41 - loss: 0.471 - ETA: 2:34 - loss: 0.492 - ETA: 2:28 - loss: 0.500 - ETA: 2:22 - loss: 0.501 - ETA: 2:19 - loss: 0.511 - ETA: 2:14 - loss: 0.517 - ETA: 2:11 - loss: 0.520 - ETA: 2:06 - loss: 0.528 - ETA: 2:01 - loss: 0.529 - ETA: 1:56 - loss: 0.536 - ETA: 1:51 - loss: 0.536 - ETA: 1:46 - loss: 0.532 - ETA: 1:41 - loss: 0.533 - ETA: 1:36 - loss: 0.534 - ETA: 1:32 - loss: 0.532 - ETA: 1:27 - loss: 0.532 - ETA: 1:22 - loss: 0.532 - ETA: 1:17 - loss: 0.530 - ETA: 1:12 - loss: 0.529 - ETA: 1:08 - loss: 0.529 - ETA: 1:03 - loss: 0.527 - ETA: 58s - loss: 0.526 - ETA: 54s - loss: 0.52 - ETA: 49s - loss: 0.52 - ETA: 44s - loss: 0.52 - ETA: 39s - loss: 0.52 - ETA: 35s - loss: 0.52 - ETA: 30s - loss: 0.52 - ETA: 25s - loss: 0.52 - ETA: 20s - loss: 0.52 - ETA: 16s - loss: 0.52 - ETA: 11s - loss: 0.52 - ETA: 6s - loss: 0.5263 - ETA: 1s - loss: 0.525 - 169s 10ms/step - loss: 0.5257 - val_loss: 0.5240\n",
      "Epoch 22/75\n",
      "17621/17621 [==============================] - ETA: 2:35 - loss: 0.526 - ETA: 2:30 - loss: 0.509 - ETA: 2:25 - loss: 0.513 - ETA: 2:21 - loss: 0.510 - ETA: 2:16 - loss: 0.511 - ETA: 2:11 - loss: 0.509 - ETA: 2:07 - loss: 0.512 - ETA: 2:03 - loss: 0.512 - ETA: 1:58 - loss: 0.511 - ETA: 1:53 - loss: 0.514 - ETA: 1:51 - loss: 0.512 - ETA: 1:46 - loss: 0.511 - ETA: 1:42 - loss: 0.514 - ETA: 1:38 - loss: 0.516 - ETA: 1:33 - loss: 0.515 - ETA: 1:28 - loss: 0.515 - ETA: 1:23 - loss: 0.516 - ETA: 1:18 - loss: 0.518 - ETA: 1:13 - loss: 0.516 - ETA: 1:08 - loss: 0.517 - ETA: 1:03 - loss: 0.516 - ETA: 59s - loss: 0.520 - ETA: 54s - loss: 0.51 - ETA: 49s - loss: 0.51 - ETA: 44s - loss: 0.51 - ETA: 40s - loss: 0.51 - ETA: 35s - loss: 0.51 - ETA: 30s - loss: 0.51 - ETA: 25s - loss: 0.51 - ETA: 20s - loss: 0.51 - ETA: 16s - loss: 0.51 - ETA: 11s - loss: 0.51 - ETA: 6s - loss: 0.5159 - ETA: 1s - loss: 0.516 - 169s 10ms/step - loss: 0.5165 - val_loss: 0.5266\n",
      "Epoch 23/75\n",
      "17621/17621 [==============================] - ETA: 2:42 - loss: 0.553 - ETA: 2:40 - loss: 0.547 - ETA: 2:32 - loss: 0.514 - ETA: 2:25 - loss: 0.518 - ETA: 2:19 - loss: 0.508 - ETA: 2:14 - loss: 0.512 - ETA: 2:09 - loss: 0.495 - ETA: 2:04 - loss: 0.488 - ETA: 1:59 - loss: 0.484 - ETA: 1:54 - loss: 0.488 - ETA: 1:49 - loss: 0.486 - ETA: 1:45 - loss: 0.485 - ETA: 1:40 - loss: 0.490 - ETA: 1:35 - loss: 0.491 - ETA: 1:31 - loss: 0.493 - ETA: 1:26 - loss: 0.492 - ETA: 1:21 - loss: 0.493 - ETA: 1:17 - loss: 0.496 - ETA: 1:12 - loss: 0.496 - ETA: 1:07 - loss: 0.494 - ETA: 1:03 - loss: 0.494 - ETA: 58s - loss: 0.496 - ETA: 53s - loss: 0.49 - ETA: 49s - loss: 0.49 - ETA: 44s - loss: 0.49 - ETA: 39s - loss: 0.49 - ETA: 35s - loss: 0.49 - ETA: 30s - loss: 0.49 - ETA: 25s - loss: 0.49 - ETA: 20s - loss: 0.49 - ETA: 16s - loss: 0.49 - ETA: 11s - loss: 0.49 - ETA: 6s - loss: 0.4954 - ETA: 1s - loss: 0.496 - 170s 10ms/step - loss: 0.4970 - val_loss: 0.5180\n",
      "Epoch 24/75\n",
      "17621/17621 [==============================] - ETA: 2:40 - loss: 0.520 - ETA: 2:34 - loss: 0.503 - ETA: 2:30 - loss: 0.488 - ETA: 2:25 - loss: 0.485 - ETA: 2:23 - loss: 0.479 - ETA: 2:17 - loss: 0.469 - ETA: 2:11 - loss: 0.469 - ETA: 2:06 - loss: 0.478 - ETA: 2:01 - loss: 0.475 - ETA: 1:56 - loss: 0.477 - ETA: 1:51 - loss: 0.476 - ETA: 1:46 - loss: 0.479 - ETA: 1:41 - loss: 0.485 - ETA: 1:36 - loss: 0.484 - ETA: 1:31 - loss: 0.486 - ETA: 1:27 - loss: 0.487 - ETA: 1:24 - loss: 0.485 - ETA: 1:20 - loss: 0.483 - ETA: 1:15 - loss: 0.481 - ETA: 1:10 - loss: 0.482 - ETA: 1:05 - loss: 0.482 - ETA: 1:00 - loss: 0.482 - ETA: 55s - loss: 0.482 - ETA: 50s - loss: 0.48 - ETA: 45s - loss: 0.48 - ETA: 40s - loss: 0.48 - ETA: 35s - loss: 0.48 - ETA: 31s - loss: 0.48 - ETA: 26s - loss: 0.48 - ETA: 21s - loss: 0.48 - ETA: 16s - loss: 0.48 - ETA: 11s - loss: 0.48 - ETA: 6s - loss: 0.4864 - ETA: 2s - loss: 0.486 - 176s 10ms/step - loss: 0.4872 - val_loss: 0.5083\n",
      "Epoch 25/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17621/17621 [==============================] - ETA: 2:36 - loss: 0.481 - ETA: 2:31 - loss: 0.466 - ETA: 2:26 - loss: 0.475 - ETA: 2:21 - loss: 0.465 - ETA: 2:17 - loss: 0.464 - ETA: 2:12 - loss: 0.469 - ETA: 2:08 - loss: 0.470 - ETA: 2:05 - loss: 0.471 - ETA: 2:00 - loss: 0.470 - ETA: 1:55 - loss: 0.469 - ETA: 1:50 - loss: 0.470 - ETA: 1:45 - loss: 0.471 - ETA: 1:41 - loss: 0.477 - ETA: 1:36 - loss: 0.475 - ETA: 1:31 - loss: 0.473 - ETA: 1:27 - loss: 0.471 - ETA: 1:22 - loss: 0.475 - ETA: 1:17 - loss: 0.472 - ETA: 1:12 - loss: 0.473 - ETA: 1:07 - loss: 0.471 - ETA: 1:03 - loss: 0.469 - ETA: 58s - loss: 0.468 - ETA: 53s - loss: 0.46 - ETA: 49s - loss: 0.47 - ETA: 44s - loss: 0.47 - ETA: 39s - loss: 0.47 - ETA: 34s - loss: 0.47 - ETA: 30s - loss: 0.47 - ETA: 25s - loss: 0.47 - ETA: 20s - loss: 0.47 - ETA: 16s - loss: 0.47 - ETA: 11s - loss: 0.47 - ETA: 6s - loss: 0.4754 - ETA: 1s - loss: 0.475 - 170s 10ms/step - loss: 0.4745 - val_loss: 0.5050\n",
      "Epoch 26/75\n",
      "17621/17621 [==============================] - ETA: 2:36 - loss: 0.400 - ETA: 2:34 - loss: 0.424 - ETA: 2:31 - loss: 0.426 - ETA: 2:25 - loss: 0.432 - ETA: 2:19 - loss: 0.439 - ETA: 2:14 - loss: 0.435 - ETA: 2:10 - loss: 0.442 - ETA: 2:05 - loss: 0.443 - ETA: 2:00 - loss: 0.442 - ETA: 1:55 - loss: 0.443 - ETA: 1:51 - loss: 0.439 - ETA: 1:46 - loss: 0.442 - ETA: 1:41 - loss: 0.440 - ETA: 1:37 - loss: 0.439 - ETA: 1:32 - loss: 0.437 - ETA: 1:27 - loss: 0.435 - ETA: 1:22 - loss: 0.437 - ETA: 1:17 - loss: 0.439 - ETA: 1:12 - loss: 0.438 - ETA: 1:08 - loss: 0.439 - ETA: 1:03 - loss: 0.440 - ETA: 58s - loss: 0.440 - ETA: 53s - loss: 0.44 - ETA: 49s - loss: 0.44 - ETA: 44s - loss: 0.44 - ETA: 39s - loss: 0.44 - ETA: 35s - loss: 0.44 - ETA: 30s - loss: 0.44 - ETA: 25s - loss: 0.44 - ETA: 20s - loss: 0.44 - ETA: 16s - loss: 0.44 - ETA: 11s - loss: 0.44 - ETA: 6s - loss: 0.4450 - ETA: 1s - loss: 0.443 - 168s 10ms/step - loss: 0.4439 - val_loss: 0.5111\n",
      "Epoch 27/75\n",
      "17621/17621 [==============================] - ETA: 2:35 - loss: 0.446 - ETA: 2:38 - loss: 0.445 - ETA: 2:33 - loss: 0.452 - ETA: 2:26 - loss: 0.450 - ETA: 2:20 - loss: 0.445 - ETA: 2:15 - loss: 0.438 - ETA: 2:09 - loss: 0.430 - ETA: 2:04 - loss: 0.433 - ETA: 1:59 - loss: 0.436 - ETA: 1:54 - loss: 0.439 - ETA: 1:49 - loss: 0.442 - ETA: 1:45 - loss: 0.441 - ETA: 1:40 - loss: 0.443 - ETA: 1:36 - loss: 0.443 - ETA: 1:31 - loss: 0.444 - ETA: 1:27 - loss: 0.441 - ETA: 1:22 - loss: 0.442 - ETA: 1:17 - loss: 0.442 - ETA: 1:13 - loss: 0.438 - ETA: 1:08 - loss: 0.437 - ETA: 1:03 - loss: 0.437 - ETA: 58s - loss: 0.437 - ETA: 54s - loss: 0.43 - ETA: 49s - loss: 0.43 - ETA: 44s - loss: 0.43 - ETA: 39s - loss: 0.43 - ETA: 35s - loss: 0.43 - ETA: 30s - loss: 0.43 - ETA: 25s - loss: 0.43 - ETA: 20s - loss: 0.43 - ETA: 16s - loss: 0.43 - ETA: 11s - loss: 0.43 - ETA: 6s - loss: 0.4385 - ETA: 1s - loss: 0.439 - 169s 10ms/step - loss: 0.4409 - val_loss: 0.5150\n",
      "Epoch 28/75\n",
      "17621/17621 [==============================] - ETA: 2:37 - loss: 0.441 - ETA: 2:34 - loss: 0.433 - ETA: 2:32 - loss: 0.435 - ETA: 2:25 - loss: 0.435 - ETA: 2:20 - loss: 0.427 - ETA: 2:17 - loss: 0.429 - ETA: 2:12 - loss: 0.433 - ETA: 2:06 - loss: 0.436 - ETA: 2:01 - loss: 0.430 - ETA: 1:56 - loss: 0.428 - ETA: 1:51 - loss: 0.428 - ETA: 1:46 - loss: 0.430 - ETA: 1:41 - loss: 0.430 - ETA: 1:37 - loss: 0.429 - ETA: 1:32 - loss: 0.430 - ETA: 1:27 - loss: 0.431 - ETA: 1:22 - loss: 0.428 - ETA: 1:17 - loss: 0.428 - ETA: 1:13 - loss: 0.428 - ETA: 1:08 - loss: 0.425 - ETA: 1:03 - loss: 0.423 - ETA: 59s - loss: 0.422 - ETA: 54s - loss: 0.42 - ETA: 49s - loss: 0.42 - ETA: 45s - loss: 0.42 - ETA: 40s - loss: 0.42 - ETA: 35s - loss: 0.42 - ETA: 30s - loss: 0.42 - ETA: 25s - loss: 0.42 - ETA: 21s - loss: 0.42 - ETA: 16s - loss: 0.42 - ETA: 11s - loss: 0.42 - ETA: 6s - loss: 0.4291 - ETA: 1s - loss: 0.429 - 171s 10ms/step - loss: 0.4299 - val_loss: 0.5038\n",
      "Epoch 29/75\n",
      "17621/17621 [==============================] - ETA: 2:40 - loss: 0.366 - ETA: 2:33 - loss: 0.395 - ETA: 2:27 - loss: 0.392 - ETA: 2:21 - loss: 0.392 - ETA: 2:17 - loss: 0.407 - ETA: 2:12 - loss: 0.409 - ETA: 2:07 - loss: 0.399 - ETA: 2:03 - loss: 0.396 - ETA: 1:58 - loss: 0.396 - ETA: 1:55 - loss: 0.401 - ETA: 1:50 - loss: 0.400 - ETA: 1:45 - loss: 0.400 - ETA: 1:40 - loss: 0.401 - ETA: 1:35 - loss: 0.401 - ETA: 1:31 - loss: 0.398 - ETA: 1:26 - loss: 0.399 - ETA: 1:21 - loss: 0.401 - ETA: 1:17 - loss: 0.401 - ETA: 1:13 - loss: 0.404 - ETA: 1:08 - loss: 0.402 - ETA: 1:04 - loss: 0.404 - ETA: 59s - loss: 0.408 - ETA: 54s - loss: 0.41 - ETA: 49s - loss: 0.40 - ETA: 45s - loss: 0.41 - ETA: 40s - loss: 0.41 - ETA: 35s - loss: 0.41 - ETA: 30s - loss: 0.41 - ETA: 25s - loss: 0.41 - ETA: 21s - loss: 0.41 - ETA: 16s - loss: 0.41 - ETA: 11s - loss: 0.41 - ETA: 6s - loss: 0.4115 - ETA: 1s - loss: 0.413 - 171s 10ms/step - loss: 0.4129 - val_loss: 0.5223\n",
      "Epoch 30/75\n",
      "17621/17621 [==============================] - ETA: 2:35 - loss: 0.420 - ETA: 2:30 - loss: 0.433 - ETA: 2:25 - loss: 0.412 - ETA: 2:20 - loss: 0.412 - ETA: 2:15 - loss: 0.405 - ETA: 2:11 - loss: 0.398 - ETA: 2:06 - loss: 0.390 - ETA: 2:01 - loss: 0.391 - ETA: 1:57 - loss: 0.390 - ETA: 1:52 - loss: 0.394 - ETA: 1:48 - loss: 0.390 - ETA: 1:43 - loss: 0.390 - ETA: 1:39 - loss: 0.386 - ETA: 1:35 - loss: 0.384 - ETA: 1:30 - loss: 0.380 - ETA: 1:25 - loss: 0.378 - ETA: 1:21 - loss: 0.379 - ETA: 1:16 - loss: 0.383 - ETA: 1:11 - loss: 0.384 - ETA: 1:07 - loss: 0.387 - ETA: 1:02 - loss: 0.391 - ETA: 57s - loss: 0.395 - ETA: 53s - loss: 0.39 - ETA: 48s - loss: 0.39 - ETA: 43s - loss: 0.39 - ETA: 39s - loss: 0.39 - ETA: 34s - loss: 0.39 - ETA: 29s - loss: 0.39 - ETA: 25s - loss: 0.39 - ETA: 20s - loss: 0.39 - ETA: 15s - loss: 0.40 - ETA: 11s - loss: 0.40 - ETA: 6s - loss: 0.4011 - ETA: 1s - loss: 0.401 - 168s 10ms/step - loss: 0.4011 - val_loss: 0.4929\n",
      "Epoch 31/75\n",
      "17621/17621 [==============================] - ETA: 2:49 - loss: 0.405 - ETA: 2:48 - loss: 0.394 - ETA: 2:42 - loss: 0.396 - ETA: 2:41 - loss: 0.385 - ETA: 2:35 - loss: 0.382 - ETA: 2:29 - loss: 0.374 - ETA: 2:23 - loss: 0.371 - ETA: 2:17 - loss: 0.373 - ETA: 2:12 - loss: 0.374 - ETA: 2:06 - loss: 0.371 - ETA: 2:01 - loss: 0.372 - ETA: 1:55 - loss: 0.371 - ETA: 1:50 - loss: 0.371 - ETA: 1:45 - loss: 0.369 - ETA: 1:40 - loss: 0.369 - ETA: 1:35 - loss: 0.371 - ETA: 1:30 - loss: 0.369 - ETA: 1:24 - loss: 0.370 - ETA: 1:19 - loss: 0.373 - ETA: 1:14 - loss: 0.373 - ETA: 1:09 - loss: 0.372 - ETA: 1:04 - loss: 0.373 - ETA: 59s - loss: 0.377 - ETA: 53s - loss: 0.37 - ETA: 48s - loss: 0.37 - ETA: 43s - loss: 0.37 - ETA: 38s - loss: 0.37 - ETA: 33s - loss: 0.37 - ETA: 27s - loss: 0.37 - ETA: 22s - loss: 0.37 - ETA: 17s - loss: 0.37 - ETA: 12s - loss: 0.37 - ETA: 7s - loss: 0.3783 - ETA: 2s - loss: 0.380 - 184s 10ms/step - loss: 0.3804 - val_loss: 0.5122\n",
      "Epoch 32/75\n",
      "17621/17621 [==============================] - ETA: 2:50 - loss: 0.335 - ETA: 2:45 - loss: 0.331 - ETA: 2:39 - loss: 0.349 - ETA: 2:35 - loss: 0.359 - ETA: 2:35 - loss: 0.355 - ETA: 2:29 - loss: 0.359 - ETA: 2:23 - loss: 0.359 - ETA: 2:17 - loss: 0.366 - ETA: 2:11 - loss: 0.361 - ETA: 2:06 - loss: 0.361 - ETA: 2:01 - loss: 0.364 - ETA: 1:55 - loss: 0.369 - ETA: 1:50 - loss: 0.368 - ETA: 1:45 - loss: 0.366 - ETA: 1:39 - loss: 0.365 - ETA: 1:35 - loss: 0.365 - ETA: 1:30 - loss: 0.367 - ETA: 1:25 - loss: 0.368 - ETA: 1:20 - loss: 0.368 - ETA: 1:14 - loss: 0.369 - ETA: 1:09 - loss: 0.370 - ETA: 1:04 - loss: 0.369 - ETA: 58s - loss: 0.371 - ETA: 53s - loss: 0.37 - ETA: 48s - loss: 0.37 - ETA: 43s - loss: 0.37 - ETA: 38s - loss: 0.37 - ETA: 33s - loss: 0.37 - ETA: 27s - loss: 0.37 - ETA: 22s - loss: 0.37 - ETA: 17s - loss: 0.37 - ETA: 12s - loss: 0.37 - ETA: 7s - loss: 0.3770 - ETA: 2s - loss: 0.378 - 185s 10ms/step - loss: 0.3784 - val_loss: 0.5038\n",
      "Epoch 33/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17621/17621 [==============================] - ETA: 2:49 - loss: 0.399 - ETA: 2:43 - loss: 0.377 - ETA: 2:38 - loss: 0.368 - ETA: 2:33 - loss: 0.357 - ETA: 2:29 - loss: 0.353 - ETA: 2:26 - loss: 0.363 - ETA: 2:21 - loss: 0.362 - ETA: 2:15 - loss: 0.365 - ETA: 2:10 - loss: 0.366 - ETA: 2:04 - loss: 0.366 - ETA: 1:59 - loss: 0.365 - ETA: 1:54 - loss: 0.364 - ETA: 1:48 - loss: 0.367 - ETA: 1:43 - loss: 0.362 - ETA: 1:38 - loss: 0.360 - ETA: 1:33 - loss: 0.359 - ETA: 1:27 - loss: 0.358 - ETA: 1:22 - loss: 0.355 - ETA: 1:18 - loss: 0.355 - ETA: 1:13 - loss: 0.356 - ETA: 1:08 - loss: 0.356 - ETA: 1:03 - loss: 0.356 - ETA: 58s - loss: 0.356 - ETA: 53s - loss: 0.35 - ETA: 47s - loss: 0.35 - ETA: 42s - loss: 0.35 - ETA: 37s - loss: 0.35 - ETA: 32s - loss: 0.35 - ETA: 27s - loss: 0.35 - ETA: 22s - loss: 0.35 - ETA: 17s - loss: 0.35 - ETA: 12s - loss: 0.35 - ETA: 7s - loss: 0.3584 - ETA: 2s - loss: 0.357 - 182s 10ms/step - loss: 0.3574 - val_loss: 0.5210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xe8c42ac8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(300, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=75, \n",
    "          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
