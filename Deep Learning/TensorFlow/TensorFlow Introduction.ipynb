{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conda prompt, write:\n",
    "    \n",
    "    activate tensorflow\n",
    "\n",
    "to activate the conda environment (to deactivate just write conda deactivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:/Users/DQ815GM/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting C:/Users/DQ815GM/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting C:/Users/DQ815GM/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting C:/Users/DQ815GM/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"C:/Users/DQ815GM/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set our hyper parameters.\n",
    "The first one is the learning rate which defines how fast we want to update our weights, if the learning rate is too big our model might skip the optimal solution, if it's too small we might need too many iterations to converge on the best results so we'll set it to 0.01 because it's a known decent learning rate for this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set hyper parameters\n",
    "learning_rate = 0.01\n",
    "training_iteration = 30\n",
    "batch_size = 100\n",
    "display_step = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to create our model in tensorflow a model is represented as a data flow graph the graph contains a set of nodes called operations.\n",
    "\n",
    "![tensorflow_graph.jpg](images/tensorflow_graph.jpg)\n",
    "\n",
    "These are units of computation they can be as simple as addition or multiplication and can be complicated at some multivariate equation. Each operation takes in as input a tensor and outputs a tensor as well. A tensor is how data is represented in tensor flow they are multi-dimensional arrays of numbers and they flow between operations hence the name tensor flow.\n",
    "\n",
    "We start by building our model by creating two operations both our placeholder operations a placeholder is just a variable that we will assign data to at a later date it's never initialized and contains no data well define the type and shape of our data as the parameters.\n",
    "\n",
    "![placeholder_tensorflow.png](images/placeholder_tensorflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF graph input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) #mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10]) #0-9 digits recognition => 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our weights W and biases b for our model. The weights are the probabilities that affect how data flows in the graph and they will be updated continuously during training, so that our results get closer and closer to the right solution. The bias lets a shift our regression line to better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a named scope. Scopes help us organize nodes in the graph visualizer, called tensor board, which will view at the end. We will create three scopes: in the first scope we'll implement our model which is a logistic regression, by matrix multiplying the input images X by the weight matrix W and adding the bias B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Wx_b') as scope:\n",
    "    #construct a linear model\n",
    "    model = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create summary operations to help us later visualize the distribution of our weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add summury ops to collect data\n",
    "w_h = tf.summary.histogram('weights', W)\n",
    "b_h = tf.summary.histogram('biases', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second scope will create our cost function. The cost function helps us minimize our error during training and we'll use the popular cross-entropy function as it. \n",
    "Then we'll create a scalar summary to monitor it during training, so we can visualize it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('cost_function') as scope:\n",
    "    #Minimize error using cross entropy\n",
    "    #Cross entropy\n",
    "    cost_function = -tf.reduce_sum(y*tf.log(model))\n",
    "    #Create a summary to monitor the cost function\n",
    "    tf.summary.scalar('cost_function', cost_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our last scope is called Train and it will create our optimization function that makes our model improve during training. \n",
    "We use the popular gradient descent algorithm which takes our learning rate as a parameter for pacing and our cost function as a parameter to help minimize the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('train') as scope:\n",
    "    #Gradient descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our graph built, we will initialize all of our variables then we'll merge all of our summaries into a single operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DQ815GM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "#Merge all summaries into a single operator\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to launch our graph by initializing a session which lets us execute our data flow graph. Then we set our summary write or folder location which will later load data to visualize in tensor board.\n",
    "We got to the training time, let's set our for loop for our specified number of iterations and initialize our average cost which will print out every so often to make sure our model is improving during training. We compute our batch size and start training over each example in our training data. Next we fit our model using the batch data in the gradient descent algorithm for back propagation. we compute the average loss and write logs for each iteration via the summary writer for each display step we'll display error logs to terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0001 cost= 30.051488478\n",
      "Iteration: 0003 cost= 21.183532762\n",
      "Iteration: 0005 cost= 20.084198595\n",
      "Iteration: 0007 cost= 19.705146739\n",
      "Iteration: 0009 cost= 19.365873906\n",
      "Iteration: 0011 cost= 19.095534765\n",
      "Iteration: 0013 cost= 18.947475856\n",
      "Iteration: 0015 cost= 18.772883054\n",
      "Iteration: 0017 cost= 18.635726760\n",
      "Iteration: 0019 cost= 18.549086073\n",
      "Iteration: 0021 cost= 18.513515310\n",
      "Iteration: 0023 cost= 18.353174478\n",
      "Iteration: 0025 cost= 18.223478372\n",
      "Iteration: 0027 cost= 18.222337539\n",
      "Iteration: 0029 cost= 18.039090610\n",
      "Tuning completed!\n",
      "Accuracy: 0.9222\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Change this to a location on your computer\n",
    "    summary_writer = tf.summary.FileWriter('C:/Users/DQ815GM/MNIST_data/logs', sess.graph)\n",
    "\n",
    "    # Training cycle\n",
    "    for iteration in range(training_iteration):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Fit training using batch data\n",
    "            sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\n",
    "            # Compute the average loss\n",
    "            avg_cost += sess.run(cost_function, feed_dict={x: batch_xs, y: batch_ys})/total_batch\n",
    "            # Write logs for each iteration\n",
    "            summary_str = sess.run(merged_summary_op, feed_dict={x: batch_xs, y: batch_ys})\n",
    "            summary_writer.add_summary(summary_str, iteration*total_batch + i)\n",
    "        # Display logs per iteration step\n",
    "        if iteration % display_step == 0:\n",
    "            print(\"Iteration:\", '%04d' % (iteration + 1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    \n",
    "    \n",
    "    print(\"Tuning completed!\")\n",
    "    \n",
    "    # Test the model\n",
    "    predictions = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(predictions, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then visualize our graph in tensor board. In our browser we'll be able to view the output of our cost function over time under the events tab, under histograms we'll be able to see the variance in our biases and weights over time, under graphs we can view the actual graph we created as well as the variables for weights and bias. We can see the flow of tensors in the form of edges connecting our nodes or operations. We can see each of the three scopes we named in our code earlier and by double clicking on each we can see a more detailed view of how tensors are flowing through each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running in the conda prompt:\n",
    "    \n",
    "    tensorboard --logdir=C:\\Users\\DQ815GM\\MNIST_data\\logs\n",
    "\n",
    "the prompt will give you a URL where you can see the summaries.\n",
    "Once you have check everything type CTRL+C in the conda prompt to close that URL.\n",
    "Then, to end:\n",
    "    \n",
    "    conda deactivate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
