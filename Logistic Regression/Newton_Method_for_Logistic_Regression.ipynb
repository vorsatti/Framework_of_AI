{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton's Method for Logistic Regression\n",
    "I am going to use Newton's Method as optimization technique to perform Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our task\n",
    "We're going to compute the probability that someone has Diabetes given their height, weight, and blood pressure. \n",
    "We'll generate this data ourselves (toy data), plot it, learn a logistic regression curve using Newton's Method \n",
    "for Optimization, then use that curve to predict the probability someone new with these 3 features has diabetes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use Newton's Method for optimizing?\n",
    "- Newton’s method usually converges faster than gradient descent when maximizing logistic regression log likelihood.\n",
    "- Each iteration is more expensive than gradient descent because of calculating inverse of Hessian\n",
    "- As long as data points are not very large, Newton’s methods are preferred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Logistic%20Regression.png](images/Logistic%20Regression.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#matrix math\n",
    "import numpy as np\n",
    "#data manipulation\n",
    "import pandas as pd\n",
    "#matrix data structure\n",
    "from patsy import dmatrices\n",
    "#for error logging\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#outputs probability between 0 and 1\n",
    "def sigmoid(x):\n",
    "    '''Sigmoid function of x.'''\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Regularization%20-%20method%20to%20prevent%20oferfitting.png](images/Regularization%20-%20method%20to%20prevent%20oferfitting.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#makes the random numbers predictable\n",
    "#(pseudo-)random numbers work by starting with a number (the seed), \n",
    "#multiplying it by a large number, then taking modulo of that product. \n",
    "#The resulting number is then used as the seed to generate the next \"random\" number. \n",
    "#When you set the seed (every time), it does the same thing every time, giving you the same numbers.\n",
    "#good for reproducing results for debugging\n",
    "\n",
    "\n",
    "np.random.seed(0) # set the seed\n",
    "\n",
    "##Step 1 - Define model parameters (hyperparameters)\n",
    "\n",
    "## algorithm settings\n",
    "#the minimum threshold for the difference between the predicted output and the actual output\n",
    "#this tells our model when to stop learning, when our prediction capability is good enough\n",
    "tol=1e-8 # convergence tolerance\n",
    "\n",
    "lam = None # l2-regularization\n",
    "#how long to train for?\n",
    "max_iter = 20 # maximum allowed iterations\n",
    "\n",
    "## data creation settings\n",
    "#Covariance measures how two variables move together. \n",
    "#It measures whether the two move in the same direction (a positive covariance) \n",
    "#or in opposite directions (a negative covariance). \n",
    "r = 0.95 # covariance between x and z\n",
    "n = 1000 # number of observations (size of dataset to generate) \n",
    "sigma = 1 # variance of noise - how spread out is the data?\n",
    "\n",
    "## model settings\n",
    "beta_x, beta_z, beta_v = -4, .9, 1 # true beta coefficients\n",
    "var_x, var_z, var_v = 1, 1, 4 # variances of inputs\n",
    "\n",
    "## the model specification you want to fit\n",
    "formula = 'y ~ x + z + v + np.exp(x) + I(v**2 + z)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>x</th>\n",
       "      <th>z</th>\n",
       "      <th>v</th>\n",
       "      <th>np.exp(x)</th>\n",
       "      <th>I(v ** 2 + z)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.611418</td>\n",
       "      <td>-1.566192</td>\n",
       "      <td>15.613483</td>\n",
       "      <td>0.199604</td>\n",
       "      <td>242.214667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.080909</td>\n",
       "      <td>0.085959</td>\n",
       "      <td>42.720111</td>\n",
       "      <td>0.922278</td>\n",
       "      <td>1825.093814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.297531</td>\n",
       "      <td>0.142110</td>\n",
       "      <td>3.530885</td>\n",
       "      <td>1.346531</td>\n",
       "      <td>12.609259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.412771</td>\n",
       "      <td>1.734809</td>\n",
       "      <td>-57.235945</td>\n",
       "      <td>4.107323</td>\n",
       "      <td>3277.688187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.204214</td>\n",
       "      <td>-0.302335</td>\n",
       "      <td>-0.074792</td>\n",
       "      <td>1.226561</td>\n",
       "      <td>-0.296741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.515413</td>\n",
       "      <td>2.075660</td>\n",
       "      <td>-74.190590</td>\n",
       "      <td>4.551301</td>\n",
       "      <td>5506.319245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.122694</td>\n",
       "      <td>-1.001452</td>\n",
       "      <td>-115.449743</td>\n",
       "      <td>0.325402</td>\n",
       "      <td>13327.641761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.490410</td>\n",
       "      <td>1.406678</td>\n",
       "      <td>203.717128</td>\n",
       "      <td>4.438913</td>\n",
       "      <td>41502.075078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.076719</td>\n",
       "      <td>0.961474</td>\n",
       "      <td>18.996406</td>\n",
       "      <td>2.935033</td>\n",
       "      <td>361.824934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.328823</td>\n",
       "      <td>0.099469</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>1.389332</td>\n",
       "      <td>0.099470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Intercept         x         z           v  np.exp(x)  I(v ** 2 + z)\n",
       "0        1.0 -1.611418 -1.566192   15.613483   0.199604     242.214667\n",
       "1        1.0 -0.080909  0.085959   42.720111   0.922278    1825.093814\n",
       "2        1.0  0.297531  0.142110    3.530885   1.346531      12.609259\n",
       "3        1.0  1.412771  1.734809  -57.235945   4.107323    3277.688187\n",
       "4        1.0  0.204214 -0.302335   -0.074792   1.226561      -0.296741\n",
       "5        1.0  1.515413  2.075660  -74.190590   4.551301    5506.319245\n",
       "6        1.0 -1.122694 -1.001452 -115.449743   0.325402   13327.641761\n",
       "7        1.0  1.490410  1.406678  203.717128   4.438913   41502.075078\n",
       "8        1.0  1.076719  0.961474   18.996406   2.935033     361.824934\n",
       "9        1.0  0.328823  0.099469    0.001091   1.389332       0.099470"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 2 - Generate and organize our data\n",
    "\n",
    "#The multivariate normal, multinormal or Gaussian distribution is a generalization of the one-dimensional normal \n",
    "#distribution to higher dimensions. Such a distribution is specified by its mean and covariance matrix.\n",
    "#so we generate values input values - (x, v, z) using normal distributions\n",
    "\n",
    "#A probability distribution is a function that provides us the probabilities of all \n",
    "#possible outcomes of a stochastic process. \n",
    "\n",
    "#lets keep x and z closely related (height and weight)\n",
    "x, z = np.random.multivariate_normal([0,0], [[var_x,r],[r,var_z]], n).T\n",
    "#blood presure\n",
    "v = np.random.normal(0,var_v,n)**3\n",
    "\n",
    "#create a pandas dataframe (easily parseable object for manipulation)\n",
    "A = pd.DataFrame({'x' : x, 'z' : z, 'v' : v})\n",
    "#compute the log odds for our 3 independent variables\n",
    "#using the sigmoid function \n",
    "A['log_odds'] = sigmoid(A[['x','z','v']].dot([beta_x,beta_z,beta_v]) + sigma*np.random.normal(0,1,n))\n",
    "\n",
    "\n",
    "\n",
    "#compute the probability sample from binomial distribution\n",
    "#A binomial random variable is the number of successes x has in n repeated trials of a binomial experiment. \n",
    "#The probability distribution of a binomial random variable is called a binomial distribution. \n",
    "A['y'] = [np.random.binomial(1,p) for p in A.log_odds]\n",
    "\n",
    "#create a dataframe that encompasses our input data, model formula, and outputs\n",
    "y, X = dmatrices(formula, A, return_type='dataframe')\n",
    "\n",
    "#print it\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Algorithm Setup\n",
    "We begin with a quick function for catching singular matrix errors that we will use to decorate our Newton steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#like dividing by zero (Wtff omgggggg universe collapses)\n",
    "def catch_singularity(f):\n",
    "    '''Silences LinAlg Errors and throws a warning instead.'''\n",
    "    \n",
    "    def silencer(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except np.linalg.LinAlgError:\n",
    "            warnings.warn('Algorithm terminated - singular Hessian!')\n",
    "            return args[0]\n",
    "    return silencer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of a Single Newton Step\n",
    "Recall that Newton's method for maximizing / minimizing a given function $f(\\beta)$ iteratively computes the following estimate:\n",
    "\n",
    "$$\n",
    "\\beta^+ = \\beta - Hf(\\beta)^{-1}\\nabla f(\\beta)\n",
    "$$\n",
    "The Hessian of the log-likelihood for logistic regression is given by:\n",
    "\n",
    "hessian of our function = negative tranpose of (N times (p+1) times (N x N diagional matrix of weights, each is p*(1-p) times X again\n",
    "\n",
    "$$\n",
    "Hf(\\beta) = -X^TWX\n",
    "$$\n",
    "and the gradient is:\n",
    "\n",
    "gradient of our function = tranpose of X times (column vector - N vector of probabilities)\n",
    "\n",
    "$$\n",
    "\\nabla f(\\beta) = X^T(y-p)\n",
    "$$\n",
    "where $$\n",
    "W := \\text{diag}\\left(p(1-p)\\right)\n",
    "$$ and $p$ are the predicted probabilites computed at the current value of $\\beta$.\n",
    "\n",
    "#### Connection to Iteratively Reweighted Least Squares (IRLS)\n",
    "For logistic regression, this step is actually equivalent to computing a weighted least squares estimator at each iteration! The method of least squares is about estimating parameters by minimizing the squared discrepancies between observed data, on the one hand, and their expected values on the other\n",
    "\n",
    "I.e., $$\n",
    "\\beta^+ = \\arg\\min_\\beta (z-X\\beta)^TW(z-X\\beta)\n",
    "$$ with $W$ as before and the adjusted response $z$ is given by $$\n",
    "z := X\\beta + W^{-1}(y-p)\n",
    "$$\n",
    "\n",
    "Takeaway: This is fun, but in fact it can be leveraged to derive asymptotics and inferential statistics about the computed MLE $\\beta^*$!\n",
    "\n",
    "##### Our implementations\n",
    "Below we implement a single step of Newton's method, and we compute $Hf(\\beta)^{-1}\\nabla f(\\beta)$ using np.linalg.lstsq(A,b) to solve the equation $Ax = b$. Note that this does not require us to compute the actual inverse of the Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@catch_singularity\n",
    "def newton_step(curr, X, lam=None):\n",
    "    '''One naive step of Newton's Method'''\n",
    "    \n",
    "    #how to compute inverse? http://www.mathwarehouse.com/algebra/matrix/images/square-matrix/inverse-matrix.gif\n",
    "    \n",
    "    ## compute necessary objects\n",
    "    #create probability matrix, miniminum 2 dimensions, tranpose (flip it)\n",
    "    p = np.array(sigmoid(X.dot(curr[:,0])), ndmin=2).T\n",
    "    #create weight matrix from it\n",
    "    W = np.diag((p*(1-p))[:,0])\n",
    "    #derive the hessian \n",
    "    hessian = X.T.dot(W).dot(X)\n",
    "    #derive the gradient\n",
    "    grad = X.T.dot(y-p)\n",
    "    \n",
    "    ## regularization step (avoiding overfitting)\n",
    "    if lam:\n",
    "        # Return the least-squares solution to a linear matrix equation\n",
    "        step, *_ = np.linalg.lstsq(hessian + lam*np.eye(curr.shape[0]), grad)\n",
    "    else:\n",
    "        step, *_ = np.linalg.lstsq(hessian, grad)\n",
    "        \n",
    "    ## update our \n",
    "    beta = curr + step\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement Newton's method in a slightly different way; this time, at each iteration, we actually compute the full inverse of the Hessian using np.linalg.inv()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@catch_singularity\n",
    "def alt_newton_step(curr, X, lam=None):\n",
    "    '''One naive step of Newton's Method'''\n",
    "    \n",
    "    ## compute necessary objects\n",
    "    p = np.array(sigmoid(X.dot(curr[:,0])), ndmin=2).T\n",
    "    W = np.diag((p*(1-p))[:,0])\n",
    "    hessian = X.T.dot(W).dot(X)\n",
    "    grad = X.T.dot(y-p)\n",
    "    \n",
    "    ## regularization\n",
    "    if lam:\n",
    "        #Compute the inverse of a matrix.\n",
    "        step = np.dot(np.linalg.inv(hessian + lam*np.eye(curr.shape[0])), grad)\n",
    "    else:\n",
    "        step = np.dot(np.linalg.inv(hessian), grad)\n",
    "        \n",
    "    ## update our weights\n",
    "    beta = curr + step\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convergence Setup\n",
    "First we implement coefficient convergence; this stopping criterion results in convergence whenever $$\n",
    "\\|\\beta^+ - \\beta\\|_\\infty &lt; \\epsilon\n",
    "$$ where $\\epsilon$ is a given tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_coefs_convergence(beta_old, beta_new, tol, iters):\n",
    "    '''Checks whether the coefficients have converged in the l-infinity norm.\n",
    "    Returns True if they have converged, False otherwise.'''\n",
    "    #calculate the change in the coefficients\n",
    "    coef_change = np.abs(beta_old - beta_new)\n",
    "    \n",
    "    #if change hasn't reached the threshold and we have more iterations to go, keep training\n",
    "    return not (np.any(coef_change>tol) & (iters < max_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Examples\n",
    "#### Standard Newton with Coefficient Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations : 19\n",
      "Beta : [[-5.52054923e+29]\n",
      " [ 3.99522325e+29]\n",
      " [-5.64687199e+29]\n",
      " [ 7.84204654e+29]\n",
      " [-1.16296996e+28]\n",
      " [ 8.05959278e+29]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:22: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "## initial conditions\n",
    "#initial coefficients (weight values), 2 copies, we'll update one\n",
    "beta_old, beta = np.ones((len(X.columns),1)), np.zeros((len(X.columns),1))\n",
    "\n",
    "#num iterations we've done so far\n",
    "iter_count = 0\n",
    "#have we reached convergence?\n",
    "coefs_converged = False\n",
    "\n",
    "#if we haven't reached convergence... (training step)\n",
    "while not coefs_converged:\n",
    "    \n",
    "    #set the old coefficients to our current\n",
    "    beta_old = beta\n",
    "    #perform a single step of newton's optimization on our data, set our updated beta values\n",
    "    beta = newton_step(beta, X, lam=lam)\n",
    "    #increment the number of iterations\n",
    "    iter_count += 1\n",
    "    \n",
    "    #check for convergence between our old and new beta values\n",
    "    coefs_converged = check_coefs_convergence(beta_old, beta, tol, iter_count)\n",
    "    \n",
    "print('Iterations : {}'.format(iter_count))\n",
    "print('Beta : {}'.format(beta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
